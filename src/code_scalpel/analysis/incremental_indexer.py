"""
Incremental Indexing - Cache file analysis for scale.

[20251226_FEATURE] File hash tracking with SQLite cache for incremental analysis.

This module provides a caching layer for project crawl analysis, enabling
efficient re-analysis of massive codebases by only parsing changed files.

Architecture:
- SQLite database storing file hashes and analysis results
- File hash comparison (MD5/SHA256) for change detection
- Timestamp-based invalidation
- Automatic cleanup of stale entries
- Thread-safe access with optional Redis backing

Usage:
    from code_scalpel.analysis.incremental_indexer import IncrementalIndexer

    indexer = IncrementalIndexer("/path/to/project")

    # First run - indexes everything
    result = await indexer.analyze_changed_files()

    # Second run - only re-analyzes changed files
    result = await indexer.analyze_changed_files()
    print(f"Re-analyzed {result.files_changed} files")

Performance:
- First run: O(n) where n = number of files
- Subsequent runs: O(m) where m = number of changed files
- Memory: O(1) per file (uses streaming)
- Disk: ~1KB per cached file result
"""

from __future__ import annotations

import hashlib
import json
import logging
import sqlite3
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Optional

logger = logging.getLogger(__name__)


@dataclass
class FileHash:
    """File hash entry."""

    file_path: str
    file_hash: str  # SHA256
    mtime: float
    file_size: int


@dataclass
class CachedAnalysis:
    """Cached analysis result."""

    file_path: str
    file_hash: str
    analysis_data: Dict[str, Any]
    cached_at: float
    ttl_seconds: int = 86400  # 24 hours default


@dataclass
class IncrementalIndexResult:
    """Result of incremental indexing."""

    files_total: int
    files_changed: int
    files_analyzed: int
    cache_hits: int
    cache_misses: int
    analysis_time_seconds: float
    cache_data: Optional[Dict[str, Any]] = None


class IncrementalIndexer:
    """Incremental analysis with file-level caching."""

    def __init__(
        self,
        project_root: str | Path,
        cache_dir: Optional[str | Path] = None,
        ttl_seconds: int = 86400,
        use_redis: bool = False,
    ):
        """
        Initialize incremental indexer.

        Args:
            project_root: Project root directory
            cache_dir: Directory for SQLite cache (default: .code-scalpel/cache)
            ttl_seconds: Cache TTL in seconds (default: 24 hours)
            use_redis: Use Redis for distributed caching (Enterprise only)
        """
        self.root = Path(project_root)
        self.ttl_seconds = ttl_seconds
        self.use_redis = use_redis

        # Setup cache directory
        if cache_dir is None:
            cache_dir = self.root / ".code-scalpel" / "cache"
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # SQLite database path
        self.db_path = self.cache_dir / "index.db"

        # Initialize database
        self._init_database()

        # Redis client (if enabled)
        self.redis_client = None
        if use_redis:
            self._init_redis()

    def _init_database(self) -> None:
        """Initialize SQLite cache database."""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS file_hashes (
                    file_path TEXT PRIMARY KEY,
                    file_hash TEXT NOT NULL,
                    mtime REAL NOT NULL,
                    file_size INTEGER NOT NULL,
                    indexed_at REAL NOT NULL
                )
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS analyses (
                    file_path TEXT PRIMARY KEY,
                    file_hash TEXT NOT NULL,
                    analysis_json TEXT NOT NULL,
                    cached_at REAL NOT NULL,
                    ttl_seconds INTEGER NOT NULL,
                    FOREIGN KEY(file_path) REFERENCES file_hashes(file_path)
                )
            """)
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_file_hash ON file_hashes(file_hash)
            """)
            conn.commit()

    def _init_redis(self) -> None:
        """Initialize Redis connection (Enterprise)."""
        try:
            import redis

            self.redis_client = redis.Redis(
                host="localhost",
                port=6379,
                db=0,
                decode_responses=True,
                socket_connect_timeout=5,
            )
            self.redis_client.ping()
            logger.info("Redis cache enabled")
        except Exception as e:
            logger.warning(f"Redis unavailable, falling back to SQLite: {e}")
            self.redis_client = None

    def compute_file_hash(self, file_path: Path) -> str:
        """Compute SHA256 hash of a file."""
        sha256 = hashlib.sha256()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    sha256.update(chunk)
            return sha256.hexdigest()
        except OSError:
            return ""

    def get_file_status(self, file_path: Path) -> tuple[bool, bool]:
        """
        Get file status.

        Returns: (file_exists_in_cache, file_changed)
        """
        if not file_path.exists():
            return False, True

        current_hash = self.compute_file_hash(file_path)
        file_path.stat().st_mtime
        file_path.stat().st_size

        with sqlite3.connect(self.db_path) as conn:
            row = conn.execute(
                "SELECT file_hash, mtime, file_size FROM file_hashes WHERE file_path = ?",
                (str(file_path),),
            ).fetchone()

        if row is None:
            return False, True

        cached_hash, cached_mtime, cached_size = row

        # File changed if hash differs
        file_changed = current_hash != cached_hash
        return True, file_changed

    def cache_analysis(self, file_path: Path, analysis: Dict[str, Any]) -> None:
        """Cache analysis result for a file."""
        file_hash = self.compute_file_hash(file_path)
        mtime = file_path.stat().st_mtime
        file_size = file_path.stat().st_size
        now = time.time()

        with sqlite3.connect(self.db_path) as conn:
            # Update or insert file hash
            conn.execute(
                """
                INSERT OR REPLACE INTO file_hashes
                (file_path, file_hash, mtime, file_size, indexed_at)
                VALUES (?, ?, ?, ?, ?)
                """,
                (str(file_path), file_hash, mtime, file_size, now),
            )

            # Cache analysis result
            conn.execute(
                """
                INSERT OR REPLACE INTO analyses
                (file_path, file_hash, analysis_json, cached_at, ttl_seconds)
                VALUES (?, ?, ?, ?, ?)
                """,
                (
                    str(file_path),
                    file_hash,
                    json.dumps(analysis),
                    now,
                    self.ttl_seconds,
                ),
            )

            # Optional: Redis backup
            if self.redis_client:
                try:
                    cache_key = f"scalpel:analysis:{file_hash}"
                    self.redis_client.setex(
                        cache_key,
                        self.ttl_seconds,
                        json.dumps(analysis),
                    )
                except Exception as e:
                    logger.warning(f"Redis cache write failed: {e}")

            conn.commit()

    def get_cached_analysis(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """Get cached analysis for a file (if not expired)."""
        file_hash = self.compute_file_hash(file_path)

        # Try Redis first (faster)
        if self.redis_client:
            try:
                cache_key = f"scalpel:analysis:{file_hash}"
                cached = self.redis_client.get(cache_key)
                if cached:
                    # Redis may return bytes when decode_responses=False; normalize to str for json.loads
                    if isinstance(cached, (bytes, bytearray)):
                        cached = cached.decode()
                    return json.loads(str(cached))
            except Exception:
                pass

        # Fall back to SQLite
        with sqlite3.connect(self.db_path) as conn:
            row = conn.execute(
                """
                SELECT analysis_json, cached_at, ttl_seconds
                FROM analyses
                WHERE file_path = ? AND file_hash = ?
                """,
                (str(file_path), file_hash),
            ).fetchone()

        if row is None:
            return None

        analysis_json, cached_at, ttl = row
        now = time.time()

        # Check if expired
        if now - cached_at > ttl:
            self._invalidate_analysis(file_path)
            return None

        return json.loads(analysis_json)

    def invalidate_cache(self, file_path: Optional[Path] = None) -> None:
        """Invalidate cache for a specific file or entire cache."""
        with sqlite3.connect(self.db_path) as conn:
            if file_path is None:
                # Clear entire cache
                conn.execute("DELETE FROM analyses")
                conn.execute("DELETE FROM file_hashes")
            else:
                conn.execute(
                    "DELETE FROM analyses WHERE file_path = ?", (str(file_path),)
                )
                conn.execute(
                    "DELETE FROM file_hashes WHERE file_path = ?", (str(file_path),)
                )
            conn.commit()

    def _invalidate_analysis(self, file_path: Path) -> None:
        """Invalidate expired analysis."""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("DELETE FROM analyses WHERE file_path = ?", (str(file_path),))
            conn.commit()

    def get_cache_stats(self) -> Dict[str, Any]:
        """Get cache statistics."""
        with sqlite3.connect(self.db_path) as conn:
            cached_files = conn.execute("SELECT COUNT(*) FROM file_hashes").fetchone()[
                0
            ]
            cached_analyses = conn.execute("SELECT COUNT(*) FROM analyses").fetchone()[
                0
            ]
            db_size = self.db_path.stat().st_size if self.db_path.exists() else 0

        return {
            "cached_files": cached_files,
            "cached_analyses": cached_analyses,
            "db_size_bytes": db_size,
            "cache_dir": str(self.cache_dir),
        }

    def cleanup_expired(self) -> int:
        """Remove expired cache entries. Returns number of entries removed."""
        now = time.time()
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute(
                """
                DELETE FROM analyses
                WHERE (cached_at + ttl_seconds) < ?
                """,
                (now,),
            )
            removed = cursor.rowcount
            conn.commit()
        return removed


def create_incremental_indexer(
    project_root: str | Path,
    use_redis: bool = False,
) -> IncrementalIndexer:
    """Factory function for creating an incremental indexer."""
    return IncrementalIndexer(project_root, use_redis=use_redis)
