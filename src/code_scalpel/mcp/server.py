"""
Code Scalpel MCP Server - Real MCP Protocol Implementation.

This server implements the Model Context Protocol (MCP) specification using
the official Python SDK. It exposes Code Scalpel's analysis tools to any
MCP-compliant client (Claude Desktop, Cursor, etc.).

Transports:
- stdio: Default. Client spawns server as subprocess. Best for local use.
- streamable-http: Network deployment. Requires explicit --transport flag.

Usage:
    # stdio (default)
    python -m code_scalpel.mcp.server

    # HTTP transport for network access
    python -m code_scalpel.mcp.server --transport streamable-http --port 8080
"""

# TODO [COMMUNITY/CORE] Implement all 20 core MCP tools (analyze_code, extract_code, etc.)
# TODO [COMMUNITY/CORE] Add stdin/stdout MCP transport implementation
# TODO [COMMUNITY/CORE] Implement FastMCP server with tool registration
# TODO [COMMUNITY/CORE] Add request/response envelope wrapping
# TODO [COMMUNITY/CORE] Implement error handling with error codes
# TODO [COMMUNITY/CORE] Add tool timeout enforcement
# TODO [COMMUNITY/CORE] Implement maximum payload size limits
# TODO [COMMUNITY/CORE] Create tool initialization and health checks
# TODO [COMMUNITY/CORE] Add comprehensive server logging
# TODO [COMMUNITY/CORE] Document server setup and deployment
# TODO [PRO/ADVANCED] Implement HTTP transport with TLS
# TODO [PRO/ADVANCED] Add client authentication (API key, JWT)
# TODO [PRO/ADVANCED] Implement tier-aware tool filtering
# TODO [PRO/ADVANCED] Add tool-level performance profiling
# TODO [PRO/ADVANCED] Implement response compression for large outputs
# TODO [PRO/ADVANCED] Add request queuing and concurrency limits
# TODO [PRO/ADVANCED] Implement custom tool metrics collection
# TODO [PRO/ADVANCED] Add batch tool invocation support
# TODO [PRO/ADVANCED] Create advanced analytics and monitoring
# TODO [PRO/ADVANCED] Implement tool versioning and backward compatibility
# TODO [ENTERPRISE/SCALABILITY] Implement distributed server with load balancing
# TODO [ENTERPRISE/SCALABILITY] Add multi-protocol MCP (gRPC, WebSocket)
# TODO [ENTERPRISE/SCALABILITY] Implement federated MCP across servers
# TODO [ENTERPRISE/SCALABILITY] Add OpenTelemetry instrumentation
# TODO [ENTERPRISE/SECURITY] Implement RBAC and fine-grained permissions
# TODO [ENTERPRISE/SECURITY] Add audit logging for compliance
# TODO [ENTERPRISE/SECURITY] Implement request signing and verification
# TODO [ENTERPRISE/SECURITY] Add health checks and failover
# TODO [ENTERPRISE/SECURITY] Support custom authentication providers
# TODO [ENTERPRISE/OPTIMIZATION] Implement AI-powered request optimization

from __future__ import annotations

import ast
import asyncio
import hashlib
import json
import logging
import math
import os
import re
import sys
import time
from collections.abc import Callable
from importlib import import_module
from pathlib import Path
from typing import TYPE_CHECKING, Any, Optional, Set, cast

# [20260102_REFACTOR] Deferred heavy imports below for startup performance and type-checking clarity.
# ruff: noqa: E402
if TYPE_CHECKING:
    from code_scalpel import SurgicalExtractor
    from code_scalpel.graph_engine.graph import UniversalGraph
    from code_scalpel.mcp.contract import ToolError

from typing import TypedDict

from mcp.server.fastmcp import Context
from pydantic import BaseModel, Field


class TreeNodeDict(TypedDict, total=False):
    """Represents a file or directory node in the project tree."""

    name: str
    type: str  # "file" or "directory"
    children: list[TreeNodeDict]  # Only present for directories


# [20251226_BUGFIX] Import version from package for result models
from code_scalpel import __version__
from code_scalpel.licensing.features import get_tool_capabilities, has_capability
from code_scalpel.licensing.jwt_validator import JWTLicenseValidator

# [20251225_BUGFIX] Import tier helpers from concrete modules for type checkers.
from code_scalpel.licensing.jwt_validator import (
    get_current_tier as get_current_tier_from_license,
)
from code_scalpel.mcp.contract import ToolResponseEnvelope
from code_scalpel.mcp.response_config import filter_tool_response, get_response_config

# [20251226_BUGFIX] Repaired corrupted unified sink import stub.
from code_scalpel.security.analyzers import UnifiedSinkDetector
from code_scalpel.security.analyzers.policy_engine import PolicyEngine

# Central MCP instance (moved to protocol module)
mcp = import_module("code_scalpel.mcp.protocol").mcp

# Register prompt handlers (side effects)
from code_scalpel.mcp import prompts as _prompts  # noqa: F401

# Register resource handlers (side effects)
from code_scalpel.mcp import resources as _resources  # noqa: F401

# Current tier for response envelope metadata.
# Initialized to "community" (free tier) by default.
# Can be overridden via CODE_SCALPEL_TIER environment variable.
CURRENT_TIER = "community"

# [20251228_FEATURE] Runtime license grace for long-lived server processes.
# If a license expires mid-session, keep the last known valid tier for 24h.
# This does NOT change startup behavior: expired licenses remain invalid at startup.
_LAST_VALID_LICENSE_TIER: str | None = None
_LAST_VALID_LICENSE_AT: float | None = None
_MID_SESSION_EXPIRY_GRACE_SECONDS = 24 * 60 * 60

# [20250101_FEATURE] Session tracking for update_symbol rate limiting (roadmap v1.0)
# Tracks updates per tool per session for Community tier limits
_SESSION_UPDATE_COUNTS: dict[str, int] = {}
_SESSION_AUDIT_TRAIL: list[dict[str, Any]] = []  # Enterprise: audit trail


# [20260101_BUGFIX] Pyright: resolve_file_path utility for path resolution
def resolve_file_path(path: str, check_exists: bool = False) -> Path:
    """Resolve file path to absolute Path object.

    Args:
        path: File path string to resolve
        check_exists: If True, raise FileNotFoundError if path doesn't exist

    Returns:
        Resolved absolute Path object

    Raises:
        FileNotFoundError: If check_exists=True and path doesn't exist
    """
    resolved = Path(path).resolve()
    if check_exists and not resolved.exists():
        raise FileNotFoundError(f"File not found: {path}")
    return resolved


def _get_session_update_count(tool_name: str) -> int:
    """Get the number of updates performed by a tool in this session."""
    return _SESSION_UPDATE_COUNTS.get(tool_name, 0)


def _increment_session_update_count(tool_name: str) -> int:
    """Increment and return the update count for a tool."""
    current = _SESSION_UPDATE_COUNTS.get(tool_name, 0)
    _SESSION_UPDATE_COUNTS[tool_name] = current + 1
    return current + 1


def _add_audit_entry(
    tool_name: str,
    file_path: str,
    target_name: str,
    operation: str,
    success: bool,
    tier: str,
    metadata: dict[str, Any] | None = None,
) -> None:
    """Add an entry to the audit trail (Enterprise tier)."""
    from datetime import datetime, timezone

    entry = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "tool": tool_name,
        "file_path": file_path,
        "target_name": target_name,
        "operation": operation,
        "success": success,
        "tier": tier,
        "metadata": metadata or {},
    }
    _SESSION_AUDIT_TRAIL.append(entry)


def _get_audit_trail() -> list[dict[str, Any]]:
    """Get the session audit trail."""
    return _SESSION_AUDIT_TRAIL.copy()


def _normalize_tier(value: str | None) -> str:
    if not value:
        return "community"
    v = value.strip().lower()
    if v == "free":
        return "community"
    if v == "all":
        return "enterprise"
    return v


def _requested_tier_from_env() -> str | None:
    requested = os.environ.get("CODE_SCALPEL_TIER") or os.environ.get("SCALPEL_TIER")
    if requested is None:
        return None
    requested = _normalize_tier(requested)
    if requested not in {"community", "pro", "enterprise"}:
        return None
    return requested


def _get_current_tier() -> str:
    """Get the current tier from environment or global.

    Checks in order:
    1. CODE_SCALPEL_TIER environment variable
    2. SCALPEL_TIER environment variable (legacy)
    3. CURRENT_TIER global (set by main() or defaults to 'community')

    Returns:
        str: One of 'community', 'pro', or 'enterprise'
    """
    # [20251228_FEATURE] Compute effective tier from live license state.
    # This is evaluated per tool invocation and provides mid-session downgrade.

    global _LAST_VALID_LICENSE_AT, _LAST_VALID_LICENSE_TIER

    requested = _requested_tier_from_env()
    disable_license_discovery = (
        os.environ.get("CODE_SCALPEL_DISABLE_LICENSE_DISCOVERY") == "1"
    )
    force_tier_override = os.environ.get("CODE_SCALPEL_TEST_FORCE_TIER") == "1"

    # [TESTING/OFFLINE] If license discovery is disabled and explicit test override
    # is set, honor the requested tier (used for tier-gated contract tests only).
    if disable_license_discovery and force_tier_override and requested:
        return requested

    # When discovery is disabled and no explicit license path is provided, clamp to
    # Community to avoid silently elevating tier just via env vars. Explicit license
    # paths can still be validated even with discovery off.
    if disable_license_discovery and not force_tier_override:
        if not os.environ.get("CODE_SCALPEL_LICENSE_PATH"):
            return "community"

    validator = JWTLicenseValidator()
    license_data = validator.validate()
    licensed = "community"

    if license_data.is_valid:
        licensed = _normalize_tier(license_data.tier)
        _LAST_VALID_LICENSE_TIER = licensed
        _LAST_VALID_LICENSE_AT = __import__("time").time()
    else:
        # Revocation is immediate: no grace.
        err = (license_data.error_message or "").lower()
        if "revoked" in err:
            licensed = "community"
        # Expiration mid-session: allow 24h grace based on last known valid tier.
        elif getattr(license_data, "is_expired", False) and _LAST_VALID_LICENSE_AT:
            now = __import__("time").time()
            if now - _LAST_VALID_LICENSE_AT <= _MID_SESSION_EXPIRY_GRACE_SECONDS:
                if _LAST_VALID_LICENSE_TIER in {"pro", "enterprise"}:
                    licensed = _LAST_VALID_LICENSE_TIER

    if requested is None:
        return licensed

    rank = {"community": 0, "pro": 1, "enterprise": 2}
    return requested if rank[requested] <= rank[licensed] else licensed


# [20251231_FEATURE] Tier-aware invisible governance at MCP boundary.
# Tiers are controlled by Authentication/Licensing; governance must use the
# license-derived tier from `_get_current_tier()` and apply enforcement posture.
_GOVERNANCE_FEATURE_POLICY_INTEGRITY = "policy_integrity"
_GOVERNANCE_FEATURE_RESPONSE_CONFIG = "response_config"
_GOVERNANCE_FEATURE_LIMITS = "limits"
_GOVERNANCE_FEATURE_BUDGET = "budget"
_GOVERNANCE_FEATURE_DEV_GOVERNANCE = "dev_governance"
_GOVERNANCE_FEATURE_PROJECT_STRUCTURE = "project_structure"
_GOVERNANCE_FEATURE_POLICY_EVALUATION = "policy_evaluation"


def _emit_governance_audit_event(policy_dir: Path, event: dict[str, Any]) -> None:
    """Emit a structured audit event.

    Events are appended to `.code-scalpel/audit.jsonl`.

    [20251231_FEATURE] Audit logging for governance preflight decisions.
    """
    # Default: on when governance is active; can be disabled.
    if not _parse_bool_env("SCALPEL_GOVERNANCE_AUDIT", default=True):
        return

    try:
        audit_path = policy_dir / "audit.jsonl"
        payload = dict(event)
        payload.setdefault("ts", time.time())
        payload.setdefault(
            "iso_utc", time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
        )
        with audit_path.open("a", encoding="utf-8") as f:
            f.write(json.dumps(payload, ensure_ascii=False, sort_keys=True))
            f.write("\n")
    except Exception:
        # Best-effort only: governance enforcement must not fail due to audit IO.
        return


def _parse_bool_env(name: str, default: bool) -> bool:
    val = os.environ.get(name)
    if val is None:
        return default
    v = val.strip().lower()
    if v in {"1", "true", "yes", "on"}:
        return True
    if v in {"0", "false", "no", "off"}:
        return False
    return default


def _get_governance_enforcement(tier: str) -> str:
    """Resolve enforcement posture.

    Supports:
    - SCALPEL_GOVERNANCE_ENFORCEMENT=off|warn|block
    - Legacy SCALPEL_GOVERNANCE_MODE=enforce|warn|off

    Note: For Pro/Enterprise, WARN requires explicit break-glass.
    """

    # Prefer the explicit new name.
    raw = os.environ.get("SCALPEL_GOVERNANCE_ENFORCEMENT")
    if raw is None:
        legacy = os.environ.get("SCALPEL_GOVERNANCE_MODE")
        if legacy is not None:
            legacy_norm = legacy.strip().lower()
            if legacy_norm in {"enforce", "block"}:
                raw = "block"
            elif legacy_norm == "warn":
                raw = "warn"
            else:
                raw = "off"

    enforcement = (raw or "off").strip().lower()
    if enforcement not in {"off", "warn", "block"}:
        enforcement = "off"

    # [20251231_FEATURE] Pro/Enterprise must remain fail-closed unless
    # break-glass is explicitly enabled.
    if enforcement == "warn" and tier in {"pro", "enterprise"}:
        if not _parse_bool_env("SCALPEL_GOVERNANCE_BREAK_GLASS", default=False):
            return "block"

    return enforcement


def _default_governance_features_for_tier(tier: str) -> set[str]:
    if tier == "community":
        return {_GOVERNANCE_FEATURE_RESPONSE_CONFIG, _GOVERNANCE_FEATURE_LIMITS}
    return {
        _GOVERNANCE_FEATURE_POLICY_INTEGRITY,
        _GOVERNANCE_FEATURE_RESPONSE_CONFIG,
        _GOVERNANCE_FEATURE_LIMITS,
        # [20251231_FEATURE] Pro/Enterprise: enforce write budgets when enabled.
        _GOVERNANCE_FEATURE_BUDGET,
    }


def _is_budgeted_write_tool(tool_id: str) -> bool:
    # [20251231_FEATURE] Budget enforcement is applied only to tools that can
    # mutate the filesystem.
    return tool_id in {"update_symbol", "rename_symbol"}


def _diff_added_removed_lines(
    old_code: str, new_code: str
) -> tuple[list[str], list[str]]:
    """Compute added/removed lines for budget accounting.

    [20251231_FEATURE] Used by governance budget preflight.
    """
    import difflib

    added: list[str] = []
    removed: list[str] = []
    for line in difflib.ndiff(old_code.splitlines(), new_code.splitlines()):
        if line.startswith("+ "):
            added.append(line[2:])
        elif line.startswith("- "):
            removed.append(line[2:])
    return added, removed


def _evaluate_change_budget_for_write_tool(
    *,
    tool_id: str,
    tier: str,
    arguments: dict[str, Any],
    policy_dir: Path,
) -> "tuple[bool, dict[str, Any] | None]":
    """Evaluate change budgets for write tools without mutating the filesystem.

    Returns:
        (allowed, details)
    """
    try:
        from code_scalpel.governance.change_budget import (
            ChangeBudget,
            FileChange,
            Operation,
            load_budget_config,
        )
        from code_scalpel.mcp.path_resolver import resolve_path
        from code_scalpel.surgery.surgical_patcher import UnifiedPatcher
    except Exception as e:
        return True, {"skipped": True, "reason": f"Budget preflight unavailable: {e}"}

    def _simulate_cross_file_rename(
        *,
        project_root: Path,
        target_file: Path,
        target_type: str,
        target_name: str,
        new_name: str,
        max_files_searched: int | None,
        max_files_updated: int | None,
    ) -> list[tuple[Path, str, str]]:
        """Simulate cross-file rename edits without writing.

        [20251231_FEATURE] Budget preflight accounts for cross-file edits.
        """
        try:
            from code_scalpel.surgery.rename_symbol_refactor import (
                _apply_token_replacements,
                _collect_reference_edits,
                _read_text,
                _tokenize,
                iter_python_files,
                module_name_for_file,
            )
        except Exception:
            return []

        target_module = module_name_for_file(project_root, target_file)
        if not target_module:
            return []

        updates: list[tuple[Path, str, str]] = []
        updated = 0
        for py_file in iter_python_files(project_root, max_files=max_files_searched):
            if py_file.resolve() == target_file.resolve():
                continue

            try:
                code = _read_text(py_file)
            except Exception:
                continue

            edits = _collect_reference_edits(
                code,
                target_module=target_module,
                target_type=target_type,
                target_name=target_name,
                new_name=new_name,
            )
            if not getattr(edits, "token_replacements", None):
                continue

            new_code = _apply_token_replacements(
                _tokenize(code), getattr(edits, "token_replacements")
            )
            if new_code == code:
                continue

            if max_files_updated is not None and updated >= max_files_updated:
                break

            updates.append((py_file, code, new_code))
            updated += 1

        return updates

    raw_file_path = str(arguments.get("file_path") or "")
    if not raw_file_path:
        # Tool will fail input validation anyway.
        return True, {"skipped": True, "reason": "Missing file_path"}

    try:
        resolved = resolve_path(raw_file_path, str(PROJECT_ROOT))
        resolved_path = Path(resolved)
    except Exception as e:
        # Tool will likely fail later; do not block here.
        return True, {"skipped": True, "reason": f"Path resolve failed: {e}"}

    budget_config = load_budget_config(str(policy_dir / "budget.yaml"))
    default_budget = (
        budget_config.get("default", {}) if isinstance(budget_config, dict) else {}
    )
    budget = ChangeBudget(default_budget)

    try:
        # [20260103_BUGFIX] Use UnifiedPatcher for automatic language detection
        patcher = UnifiedPatcher.from_file(str(resolved_path))
    except Exception as e:
        return True, {"skipped": True, "reason": f"Cannot load file for budget: {e}"}

    original_source = patcher.original_code

    changes: list[FileChange] = []

    try:
        if tool_id == "rename_symbol":
            target_type = str(arguments.get("target_type") or "")
            target_name = str(arguments.get("target_name") or "")
            new_name = str(arguments.get("new_name") or "")
            if not (target_type and target_name and new_name):
                return True, {"skipped": True, "reason": "Missing rename arguments"}
            patch_result = patcher.rename_symbol(target_type, target_name, new_name)

        elif tool_id == "update_symbol":
            target_type = str(arguments.get("target_type") or "")
            target_name = str(arguments.get("target_name") or "")
            op = str(arguments.get("operation") or "replace").strip().lower()
            if op == "rename":
                new_name = str(arguments.get("new_name") or "")
                if not (target_type and target_name and new_name):
                    return True, {"skipped": True, "reason": "Missing rename arguments"}
                patch_result = patcher.rename_symbol(target_type, target_name, new_name)
            else:
                new_code = arguments.get("new_code")
                if not (target_type and target_name and new_code):
                    return True, {
                        "skipped": True,
                        "reason": "Missing replace arguments",
                    }

                if target_type == "function":
                    patch_result = patcher.update_function(target_name, str(new_code))
                    if (
                        not patch_result.success
                        and "not found" in (patch_result.error or "").lower()
                    ):
                        insert_fn = getattr(patcher, "insert_function", None)
                        if callable(insert_fn):
                            patch_result = insert_fn(str(new_code))
                elif target_type == "class":
                    patch_result = patcher.update_class(target_name, str(new_code))
                    if (
                        not patch_result.success
                        and "not found" in (patch_result.error or "").lower()
                    ):
                        insert_cls = getattr(patcher, "insert_class", None)
                        if callable(insert_cls):
                            patch_result = insert_cls(str(new_code))
                elif target_type == "method":
                    if "." not in target_name:
                        return True, {
                            "skipped": True,
                            "reason": "Invalid method target_name",
                        }
                    class_name, method_name = target_name.rsplit(".", 1)
                    patch_result = patcher.update_method(
                        class_name, method_name, str(new_code)
                    )
                    if (
                        not patch_result.success
                        and "not found" in (patch_result.error or "").lower()
                        and "class" not in (patch_result.error or "").lower()
                    ):
                        insert_method = getattr(patcher, "insert_method", None)
                        if callable(insert_method):
                            patch_result = insert_method(class_name, str(new_code))
                else:
                    return True, {
                        "skipped": True,
                        "reason": f"Unknown target_type: {target_type}",
                    }
        else:
            return True, {"skipped": True, "reason": "Not a budgeted write tool"}
    except Exception as e:
        return True, {"skipped": True, "reason": f"Patch simulation failed: {e}"}

    if not getattr(patch_result, "success", False):
        # Don't block on budget when the tool itself would fail.
        return True, {"skipped": True, "reason": "Patch simulation did not succeed"}

    new_source = patcher.current_code
    added, removed = _diff_added_removed_lines(original_source, new_source)
    changes.append(
        FileChange(
            file_path=str(resolved_path),
            added_lines=added,
            removed_lines=removed,
            original_code=original_source,
            modified_code=new_source,
        )
    )

    # If this is a rename operation at Pro/Enterprise, include the cross-file
    # rename edits that the tool would apply (bounded by its configured limits).
    is_rename_op = tool_id == "rename_symbol" or (
        tool_id == "update_symbol"
        and str(arguments.get("operation") or "").strip().lower() == "rename"
    )
    if is_rename_op and tier in {"pro", "enterprise"}:
        try:
            from code_scalpel.licensing.config_loader import (
                get_cached_limits,
                get_tool_limits,
                merge_limits,
            )
            from code_scalpel.licensing.features import get_tool_capabilities

            caps = get_tool_capabilities("rename_symbol", tier)
            default_limits = (caps or {}).get("limits", {})
            config = get_cached_limits()
            overrides = get_tool_limits("rename_symbol", tier, config=config)
            limits = merge_limits(default_limits, overrides)

            max_files_searched = limits.get("max_files_searched")
            max_files_updated = limits.get("max_files_updated")

            if not ((max_files_searched == 0) and (max_files_updated == 0)):
                project_root = Path(PROJECT_ROOT)
                target_type = str(arguments.get("target_type") or "")
                target_name = str(arguments.get("target_name") or "")
                new_name = str(arguments.get("new_name") or "")
                for p, old, new in _simulate_cross_file_rename(
                    project_root=project_root,
                    target_file=Path(resolved_path),
                    target_type=target_type,
                    target_name=target_name,
                    new_name=new_name,
                    max_files_searched=max_files_searched,
                    max_files_updated=max_files_updated,
                ):
                    a2, r2 = _diff_added_removed_lines(old, new)
                    changes.append(
                        FileChange(
                            file_path=str(p),
                            added_lines=a2,
                            removed_lines=r2,
                            original_code=old,
                            modified_code=new,
                        )
                    )
        except Exception:
            # Best-effort only; if we can't simulate cross-file updates, budget
            # enforcement still applies to the primary file.
            pass

    op_desc = f"{tool_id}:{arguments.get('target_type')}:{arguments.get('target_name')}"
    operation_obj = Operation(changes=changes, description=op_desc)
    decision = budget.validate_operation(operation_obj)

    if getattr(decision, "allowed", True):
        return True, {"allowed": True, "reason": getattr(decision, "reason", "")}

    violations = []
    for v in getattr(decision, "violations", []) or []:
        violations.append(
            {
                "rule": getattr(v, "rule", None),
                "severity": getattr(v, "severity", None),
                "message": getattr(v, "message", None),
                "limit": getattr(v, "limit", None),
                "actual": getattr(v, "actual", None),
                "file": getattr(v, "file", None),
            }
        )

    return False, {
        "allowed": False,
        "reason": getattr(decision, "reason", "Budget constraints violated"),
        "violations": violations,
    }


def _supported_governance_features_for_tier(tier: str) -> set[str]:
    if tier == "community":
        return {_GOVERNANCE_FEATURE_RESPONSE_CONFIG, _GOVERNANCE_FEATURE_LIMITS}
    # Pro/Enterprise: feature set is broader, but some items may remain
    # tool-dependent until fully wired.
    return {
        _GOVERNANCE_FEATURE_POLICY_INTEGRITY,
        _GOVERNANCE_FEATURE_RESPONSE_CONFIG,
        _GOVERNANCE_FEATURE_LIMITS,
        _GOVERNANCE_FEATURE_BUDGET,
        _GOVERNANCE_FEATURE_DEV_GOVERNANCE,
        _GOVERNANCE_FEATURE_PROJECT_STRUCTURE,
        _GOVERNANCE_FEATURE_POLICY_EVALUATION,
    }


def _parse_governance_features_env() -> set[str] | None:
    raw = os.environ.get("SCALPEL_GOVERNANCE_FEATURES")
    if raw is None:
        return None
    parts = [p.strip().lower() for p in raw.split(",")]
    features = {p for p in parts if p}
    return features


def _compute_effective_governance_features(tier: str) -> tuple[set[str], list[str]]:
    requested = _parse_governance_features_env()
    effective = (
        requested
        if requested is not None
        else _default_governance_features_for_tier(tier)
    )

    supported = _supported_governance_features_for_tier(tier)
    unsupported = sorted(effective - supported)
    effective = set(effective & supported)

    enforcement = _get_governance_enforcement(tier)
    warnings: list[str] = []
    if unsupported and enforcement == "warn":
        for feat in unsupported:
            warnings.append(
                f"Governance WARN: feature '{feat}' is unavailable at tier '{tier}' (ignored)."
            )

    return effective, warnings


def _maybe_enforce_governance_before_tool(
    *,
    tool_id: str,
    tier: str,
    arguments: dict[str, Any] | None = None,
) -> tuple[Optional["ToolError"], list[str]]:
    """Apply tier-aware governance preflight.

    This is invoked by the envelope wrapper before executing any tool.

    Returns:
        (error, warnings)
    """

    from code_scalpel.mcp.contract import ToolError

    # The integrity tool must remain callable for debugging.
    if tool_id == "verify_policy_integrity":
        return None, []

    enforcement = _get_governance_enforcement(tier)
    if enforcement == "off":
        return None, []

    effective_features, warnings = _compute_effective_governance_features(tier)

    # [20251231_FEATURE] Optional optimization: apply governance preflight only
    # to write-capable tools (keeps Community/Pro read tools low overhead).
    write_tools_only = _parse_bool_env(
        "SCALPEL_GOVERNANCE_WRITE_TOOLS_ONLY", default=False
    )

    # Policy directory resolution is CWD-aware and supports SCALPEL_POLICY_DIR.
    policy_dir = _resolve_policy_dir()
    if policy_dir is None:
        # If org mandates governance but no policies present, fail closed.
        required = (os.environ.get("SCALPEL_GOVERNANCE_REQUIRED") or "").strip().lower()
        if required in {"1", "true", "yes", "on"}:
            return (
                ToolError(
                    error="Governance required but no .code-scalpel directory found",
                    error_code="forbidden",
                    error_details={"expected": ".code-scalpel", "tier": tier},
                ),
                warnings,
            )
        return None, warnings

    is_write_tool = _is_budgeted_write_tool(tool_id)
    if write_tools_only and not is_write_tool:
        return None, warnings

    base_audit: dict[str, Any] = {
        # [20251231_FEATURE] Common fields for governance audit events.
        "tool_id": tool_id,
        "tier": tier,
        "enforcement": enforcement,
        "features": sorted(effective_features),
    }

    # [20251231_FEATURE] Highest-level governance: policy integrity preflight.
    if _GOVERNANCE_FEATURE_POLICY_INTEGRITY in effective_features and tier in {
        "pro",
        "enterprise",
    }:
        manifest_source = (
            (os.environ.get("SCALPEL_POLICY_MANIFEST_SOURCE") or "file").strip().lower()
        )
        if manifest_source not in {"file", "git", "env"}:
            manifest_source = "file"

        fingerprint = _policy_state_fingerprint(policy_dir)
        cache_key = (
            f"tier={tier};dir={policy_dir};source={manifest_source};{fingerprint}"
        )
        cached = _GOVERNANCE_VERIFY_CACHE.get(cache_key)
        if cached is not None:
            verified = cached
        else:
            verified = _verify_policy_integrity_sync(
                policy_dir=str(policy_dir),
                manifest_source=manifest_source,
                tier=tier,
            )
            if len(_GOVERNANCE_VERIFY_CACHE) > 25:
                _GOVERNANCE_VERIFY_CACHE.clear()
            _GOVERNANCE_VERIFY_CACHE[cache_key] = verified

        _emit_governance_audit_event(
            policy_dir,
            {
                **base_audit,
                "check": "policy_integrity",
                "manifest_source": manifest_source,
                "cached": cached is not None,
                "verified": bool(getattr(verified, "success", False)),
            },
        )

        if not getattr(verified, "success", False):
            err = (
                getattr(verified, "error", None)
                or "Policy integrity verification failed"
            )
            if enforcement == "warn":
                warnings.append(
                    "Governance WARN: policy integrity check failed; proceeding due to break-glass."
                )
                _emit_governance_audit_event(
                    policy_dir,
                    {
                        **base_audit,
                        "check": "policy_integrity",
                        "decision": "allow_warn",
                        "reason": str(err),
                    },
                )
                return None, warnings

            _emit_governance_audit_event(
                policy_dir,
                {
                    **base_audit,
                    "check": "policy_integrity",
                    "decision": "deny",
                    "reason": str(err),
                },
            )

            return (
                ToolError(
                    error=f"Governance policy integrity check failed: {err}",
                    error_code="forbidden",
                    error_details={
                        "policy_dir": str(policy_dir),
                        "manifest_source": manifest_source,
                        "tier": tier,
                    },
                ),
                warnings,
            )

    # [20251231_FEATURE] Change budget enforcement for write tools.
    if (
        _GOVERNANCE_FEATURE_BUDGET in effective_features
        and tier in {"pro", "enterprise"}
        and arguments is not None
        and _is_budgeted_write_tool(tool_id)
    ):
        allowed, details = _evaluate_change_budget_for_write_tool(
            tool_id=tool_id,
            tier=tier,
            arguments=arguments,
            policy_dir=policy_dir,
        )
        if not allowed:
            if enforcement == "warn":
                warnings.append(
                    "Governance WARN: change budget exceeded; proceeding due to break-glass."
                )
                _emit_governance_audit_event(
                    policy_dir,
                    {
                        **base_audit,
                        "check": "budget",
                        "decision": "allow_warn",
                        "details": details or {},
                    },
                )
                return None, warnings

            _emit_governance_audit_event(
                policy_dir,
                {
                    **base_audit,
                    "check": "budget",
                    "decision": "deny",
                    "details": details or {},
                },
            )
            return (
                ToolError(
                    error="Governance blocked tool execution (change budget exceeded).",
                    error_code="forbidden",
                    error_details=details or {"tier": tier, "tool_id": tool_id},
                ),
                warnings,
            )

        _emit_governance_audit_event(
            policy_dir,
            {
                **base_audit,
                "check": "budget",
                "decision": "allow",
                "details": details or {},
            },
        )

    # [20251231_FEATURE] Policy evaluation preflight for write tools.
    if (
        _GOVERNANCE_FEATURE_POLICY_EVALUATION in effective_features
        and tier in {"pro", "enterprise"}
        and arguments is not None
        and _is_budgeted_write_tool(tool_id)
    ):
        from pathlib import Path

        policy_path = policy_dir / "policy.yaml"
        if not policy_path.exists():
            if enforcement == "warn":
                warnings.append(
                    "Governance WARN: policy.yaml missing for policy evaluation; proceeding due to break-glass."
                )
                _emit_governance_audit_event(
                    policy_dir,
                    {
                        **base_audit,
                        "check": "policy_evaluation",
                        "decision": "allow_warn",
                        "reason": f"missing policy.yaml at {policy_path}",
                    },
                )
                return None, warnings

            _emit_governance_audit_event(
                policy_dir,
                {
                    **base_audit,
                    "check": "policy_evaluation",
                    "decision": "deny",
                    "reason": f"missing policy.yaml at {policy_path}",
                },
            )
            return (
                ToolError(
                    error="Governance blocked tool execution (policy.yaml missing for policy evaluation).",
                    error_code="forbidden",
                    error_details={
                        "policy_path": str(policy_path),
                        "tier": tier,
                        "tool_id": tool_id,
                    },
                ),
                warnings,
            )

        from code_scalpel.policy_engine import PolicyEngine as OpaPolicyEngine
        from code_scalpel.policy_engine.policy_engine import (
            Operation as PolicyOperation,
        )
        from code_scalpel.policy_engine.policy_engine import (
            PolicyError as OpaPolicyError,
        )

        try:

            file_path = str(arguments.get("file_path") or "")
            # Provide the most relevant code view we have pre-mutation.
            code_blob = ""
            if tool_id == "update_symbol":
                new_code = arguments.get("new_code")
                if isinstance(new_code, str):
                    code_blob = new_code

            language = Path(file_path).suffix.lstrip(".") if file_path else ""
            affected_files = [Path(file_path)] if file_path else []

            engine = OpaPolicyEngine(str(policy_path))
            op = PolicyOperation(
                type="code_edit",
                code=code_blob,
                language=language,
                file_path=file_path,
                affected_files=affected_files,
                metadata={
                    "tool": tool_id,
                    "target_type": arguments.get("target_type"),
                    "target_name": arguments.get("target_name"),
                    "new_name": arguments.get("new_name"),
                },
            )
            decision = engine.evaluate(op)
        except OpaPolicyError as exc:
            _emit_governance_audit_event(
                policy_dir,
                {
                    **base_audit,
                    "check": "policy_evaluation",
                    "decision": "deny",
                    "reason": f"PolicyError: {exc}",
                },
            )
            return (
                ToolError(
                    error="Governance blocked tool execution (policy evaluation failed).",
                    error_code="forbidden",
                    error_details={
                        "tier": tier,
                        "tool_id": tool_id,
                        "reason": str(exc),
                    },
                ),
                warnings,
            )
        except Exception as exc:
            _emit_governance_audit_event(
                policy_dir,
                {
                    **base_audit,
                    "check": "policy_evaluation",
                    "decision": "deny",
                    "reason": f"error: {type(exc).__name__}",
                },
            )
            return (
                ToolError(
                    error="Governance blocked tool execution (policy evaluation failed).",
                    error_code="forbidden",
                    error_details={
                        "tier": tier,
                        "tool_id": tool_id,
                        "reason": f"{type(exc).__name__}",
                    },
                ),
                warnings,
            )

        if not getattr(decision, "allowed", False):
            if enforcement == "warn":
                warnings.append(
                    "Governance WARN: policy evaluation denied; proceeding due to break-glass."
                )
                _emit_governance_audit_event(
                    policy_dir,
                    {
                        **base_audit,
                        "check": "policy_evaluation",
                        "decision": "allow_warn",
                        "reason": getattr(decision, "reason", ""),
                        "violated_policies": getattr(decision, "violated_policies", []),
                    },
                )
                return None, warnings

            _emit_governance_audit_event(
                policy_dir,
                {
                    **base_audit,
                    "check": "policy_evaluation",
                    "decision": "deny",
                    "reason": getattr(decision, "reason", ""),
                    "violated_policies": getattr(decision, "violated_policies", []),
                },
            )
            return (
                ToolError(
                    error="Governance blocked tool execution (policy evaluation denied).",
                    error_code="forbidden",
                    error_details={
                        "tier": tier,
                        "tool_id": tool_id,
                        "reason": getattr(decision, "reason", None),
                        "violated_policies": getattr(decision, "violated_policies", []),
                    },
                ),
                warnings,
            )

        _emit_governance_audit_event(
            policy_dir,
            {
                **base_audit,
                "check": "policy_evaluation",
                "decision": "allow",
                "reason": getattr(decision, "reason", ""),
            },
        )

    return None, warnings


# [20251215_BUGFIX] Configure logging to stderr only to prevent stdio transport corruption
# When using stdio transport, stdout must contain ONLY valid JSON-RPC messages.
# Any logging to stdout will corrupt the protocol stream.
def _configure_logging(transport: str = "stdio"):
    """Configure logging based on transport type."""
    root_logger = logging.getLogger()

    # Remove any existing handlers
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)

    # Always log to stderr to avoid corrupting stdio transport
    handler = logging.StreamHandler(sys.stderr)
    handler.setFormatter(
        logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    )

    # [20251229_CONFIG] Use SCALPEL_MCP_INFO with string levels (DEBUG, INFO, ALERT)
    env_level = os.environ.get("SCALPEL_MCP_INFO", "WARNING").upper()
    if env_level == "DEBUG":
        level = logging.DEBUG
    elif env_level == "INFO":
        level = logging.INFO
    elif env_level == "ALERT":
        level = logging.CRITICAL
    else:
        level = logging.WARNING

    handler.setLevel(level)
    root_logger.setLevel(level)
    root_logger.addHandler(handler)


# Setup logging (default to stderr)
logger = logging.getLogger(__name__)

# Maximum code size to prevent resource exhaustion
MAX_CODE_SIZE = 100_000

# [20251220_FEATURE] v3.0.5 - Consistent confidence thresholds across security tools
# Default minimum confidence for sink detection across all tools
DEFAULT_MIN_CONFIDENCE = (
    0.7  # Balanced: catches most issues without too many false positives
)

# [20251220_TODO] Add configurable confidence thresholds:
#     - Support per-tool threshold configuration
#     - Allow environment variable overrides (SCALPEL_MIN_CONFIDENCE_*)
#     - Implement adaptive thresholds based on false positive rates
#     - Add user feedback loop to tune thresholds per project

# [20251220_TODO] Add streaming/incremental response support:
#     - Implement Tool Use with streaming for large result sets
#     - Add pagination for crawl_project and cross_file_security_scan
#     - Support partial results with continuation tokens
#     - Allow clients to limit result size

# [20251220_TODO] Add request timeout and cancellation:
#     - Implement per-tool timeout configuration
#     - Support client-initiated cancellation via MCP protocol
#     - Add graceful shutdown for long-running operations
#     - Track timeout-prone tools for monitoring

# [20251220_TODO] Add result deduplication:
#     - Deduplicate vulnerabilities across multiple scan runs
#     - Track vulnerability lineage (new/fixed/regressed)
#     - Implement baseline comparisons for security scans
#     - Support incremental scan mode

# Project root for resources (default to current directory)
PROJECT_ROOT = Path.cwd()

# [20251215_FEATURE] v2.0.0 - Roots capability for file system boundaries
# Client-specified allowed directories. If empty, PROJECT_ROOT is used.
ALLOWED_ROOTS: list[Path] = []


# [20251230_FEATURE] Invisible governance enforcement.
# Once an organization defines `.code-scalpel` rulesets, the MCP server should
# enforce them automatically without requiring users to remember to run a tool.
_GOVERNANCE_VERIFY_CACHE: dict[str, Any] = {}


def _scalpel_home_dir() -> Path:
    """Return the user-level Code Scalpel home directory.

    [20251230_FEATURE] Support non-IDE MCP deployments (Claude Desktop, ChatGPT)
    where there may be no project checkout / no dedicated working directory.

    Precedence:
    1) SCALPEL_HOME (explicit)
    2) $XDG_CONFIG_HOME/code-scalpel
    3) ~/.config/code-scalpel
    """
    env_home = os.environ.get("SCALPEL_HOME")
    if env_home:
        return Path(env_home).expanduser()

    xdg = os.environ.get("XDG_CONFIG_HOME")
    if xdg:
        return Path(xdg).expanduser() / "code-scalpel"

    return Path.home() / ".config" / "code-scalpel"


def _resolve_policy_dir() -> Path | None:
    """Resolve the active policy directory.

    Precedence:
    1) SCALPEL_POLICY_DIR (explicit override)
    2) .code-scalpel under current working directory

    Note: Use Path.cwd() at call-time so tests and long-lived servers can
    change CWD intentionally without requiring module reload.
    """
    env_dir = os.environ.get("SCALPEL_POLICY_DIR")
    if env_dir:
        return Path(env_dir)

    # CWD-first: tests and headless MCP clients often change working directory
    # without re-importing this module. PROJECT_ROOT is best-effort context, not
    # authoritative for governance.
    candidate_cwd = Path.cwd() / ".code-scalpel"
    if candidate_cwd.exists():
        return candidate_cwd

    # Fall back to PROJECT_ROOT if it differs from CWD.
    if isinstance(PROJECT_ROOT, Path):
        candidate = PROJECT_ROOT / ".code-scalpel"
        if candidate.exists():
            return candidate

    # Finally, fall back to user-level config home.
    user_candidate = _scalpel_home_dir() / ".code-scalpel"
    return user_candidate if user_candidate.exists() else None


def _env_truthy(value: str | None) -> bool:
    return (value or "").strip().lower() in {"1", "true", "yes", "on"}


def _maybe_auto_init_config_dir(
    *,
    project_root: Path,
    tier: str,
    enabled: bool | None = None,
    mode: str | None = None,
    target: str | None = None,
) -> dict[str, Any] | None:
    """Optionally create `.code-scalpel/` at startup.

    [20251230_FEATURE] Support "invisible" onboarding: MCP startup can generate
    the `.code-scalpel/` directory so users do not need to run `code-scalpel init`.

    This is intentionally opt-in because it writes files.

    Environment:
    - SCALPEL_AUTO_INIT=1 enables auto-init
    - SCALPEL_AUTO_INIT_MODE=full|templates_only selects init behavior
    - SCALPEL_AUTO_INIT_TARGET=project|user selects where to create `.code-scalpel/`
    """
    if enabled is None:
        enabled = _env_truthy(os.environ.get("SCALPEL_AUTO_INIT"))
    if not enabled:
        return None

    selected_target = (
        (target or os.environ.get("SCALPEL_AUTO_INIT_TARGET") or "project")
        .strip()
        .lower()
    )
    if selected_target not in {"project", "user"}:
        selected_target = "project"

    init_root = project_root if selected_target == "project" else _scalpel_home_dir()

    config_dir = init_root / ".code-scalpel"
    if config_dir.exists():
        return {
            "created": False,
            "skipped": True,
            "path": str(config_dir),
            "target": selected_target,
        }

    selected_mode = (
        (mode or os.environ.get("SCALPEL_AUTO_INIT_MODE") or "").strip().lower()
    )
    if not selected_mode:
        # Default: don't create secrets unless Pro/Enterprise.
        selected_mode = "full" if tier in {"pro", "enterprise"} else "templates_only"
    if selected_mode not in {"full", "templates_only"}:
        selected_mode = "templates_only"

    from code_scalpel.config.init_config import init_config_dir

    result = init_config_dir(str(init_root), mode=selected_mode)
    return {
        "created": bool(result.get("success")),
        "skipped": False,
        "mode": selected_mode,
        "path": str(config_dir),
        "target": selected_target,
        "message": result.get("message"),
    }


def _policy_state_fingerprint(policy_dir: Path) -> str:
    """Create a cheap fingerprint for policy inputs to support caching."""
    try:
        # Only include files relevant to integrity verification.
        candidates: list[Path] = []
        manifest = policy_dir / "policy.manifest.json"
        if manifest.exists():
            candidates.append(manifest)

        for ext in ("*.yaml", "*.yml", "*.json"):
            candidates.extend(policy_dir.glob(ext))

        max_mtime = 0.0
        total_size = 0
        for p in candidates:
            st = p.stat()
            max_mtime = max(max_mtime, st.st_mtime)
            total_size += st.st_size

        return f"files={len(candidates)};mtime={max_mtime:.6f};size={total_size}"
    except Exception:
        # If we cannot stat, disable caching by returning a changing key.
        return f"nocache:{time.time()}"


def _governance_enforcement_for_tier(tier: str) -> str:
    """Deprecated shim: use _get_governance_enforcement().

    [20251231_REFACTOR] Consolidate enforcement resolution.
    """
    return _get_governance_enforcement(tier)


# Caching enabled by default
CACHE_ENABLED = (
    os.environ.get("SCALPEL_CACHE_ENABLED", "1") != "0"
    and os.environ.get("SCALPEL_NO_CACHE", "0") != "1"
)

# [20251220_PERF] v3.0.5 - AST Cache for parsed Python files
# Stores parsed ASTs keyed by (file_path, mtime) to avoid re-parsing unchanged files
# Format: {(file_path_str, mtime): ast.Module}
_AST_CACHE: dict[tuple[str, float], "ast.Module"] = {}
_AST_CACHE_MAX_SIZE = 500  # Limit memory usage - keep last 500 files

# [20251220_TODO] Add persistent AST caching:
#     - Serialize parsed ASTs to disk for session persistence
#     - Use SQLite or pickle-based cache backend
#     - Implement cache invalidation on Python version change
#     - Add cache statistics endpoint for monitoring

# [20251220_TODO] Add cache eviction strategies:
#     - Implement LRU eviction instead of FIFO
#     - Track cache hit/miss rates for monitoring
#     - Add adaptive cache sizing based on memory pressure
#     - Support cache preloading for frequently accessed files


def _get_cached_ast(file_path: Path) -> "ast.Module | None":
    """Get cached AST for a file if it hasn't changed."""
    try:
        mtime = file_path.stat().st_mtime
        key = (str(file_path.resolve()), mtime)
        return _AST_CACHE.get(key)
    except OSError:
        return None


def _cache_ast(file_path: Path, tree: "ast.Module") -> None:
    """Cache a parsed AST for a file."""
    try:
        mtime = file_path.stat().st_mtime
        key = (str(file_path.resolve()), mtime)

        # Evict old entries if cache is too large
        if len(_AST_CACHE) >= _AST_CACHE_MAX_SIZE:
            # Remove oldest 20% of entries
            entries_to_remove = _AST_CACHE_MAX_SIZE // 5
            keys_to_remove = list(_AST_CACHE.keys())[:entries_to_remove]
            for k in keys_to_remove:
                del _AST_CACHE[k]

        _AST_CACHE[key] = tree
    except OSError:
        pass


def _parse_file_cached(file_path: Path) -> "ast.Module | None":
    """Parse a Python file with caching."""
    # Check cache first
    cached = _get_cached_ast(file_path)
    if cached is not None:
        return cached

    try:
        code = file_path.read_text(encoding="utf-8")
        tree = ast.parse(code)
        _cache_ast(file_path, tree)
        return tree
    except (OSError, SyntaxError):
        return None


# [20251220_PERF] v3.0.5 - Singleton UnifiedSinkDetector to avoid rebuilding patterns
_SINK_DETECTOR: "UnifiedSinkDetector | None" = None


def _get_sink_detector() -> "UnifiedSinkDetector":  # type: ignore[return-value]
    """Get or create singleton UnifiedSinkDetector."""
    global _SINK_DETECTOR
    if _SINK_DETECTOR is None:
        from code_scalpel.security.analyzers import UnifiedSinkDetector

        _SINK_DETECTOR = UnifiedSinkDetector()
    return _SINK_DETECTOR  # type: ignore[return-value]


# [20251220_TODO] Add detector warm-up and preloading:
#     - Precompile patterns on server startup
#     - Detect regex performance issues
#     - Support custom pattern injection from config
#     - Add pattern versioning and updates

# [20251220_TODO] Add detector metrics and monitoring:
#     - Track pattern match times
#     - Monitor false positive rates per pattern
#     - Detect performance regressions
#     - Support pattern profiling and optimization hints

# [20251219_FEATURE] v3.0.4 - Call graph cache for get_graph_neighborhood
# Stores UniversalGraph objects keyed by project root path (+ variant)
# Format: {cache_key: (UniversalGraph, timestamp)}
# [20251220_PERF] v3.0.5 - Increased cache TTL from 60s to 300s for large codebases
_GRAPH_CACHE: dict[str, tuple[UniversalGraph, float]] = {}  # type: ignore[name-defined]
_GRAPH_CACHE_TTL = 300.0  # seconds (5 minutes for stable codebases)

# [20251220_TODO] Add graph cache invalidation on file changes:
#     - Watch for file system changes (using watchdog or similar)
#     - Invalidate only affected portions of graph on incremental changes
#     - Support manual cache invalidation via MCP
#     - Track invalidation frequency for debugging

# [20251220_TODO] Add graph cache compression:
#     - Serialize graphs to compressed format for memory efficiency
#     - Implement graph delta encoding for incremental updates
#     - Support distributed cache (Redis) for multi-process deployments
#     - Add cache statistics and memory usage monitoring


def _get_cached_graph(
    project_root: Path, cache_variant: str = "default"
) -> UniversalGraph | None:  # type: ignore[name-defined]
    """Get cached UniversalGraph for project if still valid."""
    import time

    # [20251225_BUGFIX] Avoid mixing graph variants (e.g., Pro advanced resolution)
    # in a single cache entry.
    key = f"{project_root.resolve()}::{cache_variant}"
    if key in _GRAPH_CACHE:
        graph, timestamp = _GRAPH_CACHE[key]
        if time.time() - timestamp < _GRAPH_CACHE_TTL:
            logger.debug(f"Using cached graph for {key}")
            return graph
        else:
            # Cache expired
            del _GRAPH_CACHE[key]
            logger.debug(f"Graph cache expired for {key}")
    return None


def _cache_graph(
    project_root: Path, graph: UniversalGraph, cache_variant: str = "default"
) -> None:  # type: ignore[name-defined]
    """Cache a UniversalGraph for a project."""
    import time

    key = f"{project_root.resolve()}::{cache_variant}"
    _GRAPH_CACHE[key] = (graph, time.time())
    logger.debug(f"Cached graph for {key}")


def _invalidate_graph_cache(project_root: Path | None = None) -> None:
    """Invalidate graph cache for a project or all projects."""
    if project_root:
        # [20251226_BUGFIX] Invalidate all variants for this root.
        prefix = f"{project_root.resolve()}::"
        keys_to_delete = [k for k in _GRAPH_CACHE.keys() if k.startswith(prefix)]
        for key in keys_to_delete:
            del _GRAPH_CACHE[key]
        logger.debug(
            f"Invalidated graph cache for {project_root.resolve()} ({len(keys_to_delete)} variants)"
        )
    else:
        _GRAPH_CACHE.clear()
        logger.debug("Invalidated all graph caches")


def _is_path_allowed(path: Path) -> bool:
    """
    Check if a path is within allowed roots.

    [20251215_FEATURE] v2.0.0 - Security boundary enforcement

    Args:
        path: Path to validate

    Returns:
        True if path is within allowed roots, False otherwise

    [20251220_TODO] Add path traversal detection:
        - Detect symlink escape attempts
        - Validate path components don't contain suspicious patterns
        - Support denied path patterns (blacklist certain directories)
        - Log security-relevant path access attempts
    """
    resolved = path.resolve()

    # [20260104_BUGFIX] Respect SCALPEL_ROOT env override and allow overrides
    # even when ALLOWED_ROOTS is already populated from a previous request.
    global ALLOWED_ROOTS
    env_roots = os.environ.get("SCALPEL_ROOT") or os.environ.get("CODE_SCALPEL_ROOT")
    if env_roots:
        parsed_env_roots = [
            Path(r).expanduser().resolve() for r in env_roots.split(os.pathsep) if r
        ]
        if ALLOWED_ROOTS != parsed_env_roots:
            ALLOWED_ROOTS = parsed_env_roots
    elif not ALLOWED_ROOTS:
        # No explicit env override and cache empty: fall back to project root
        ALLOWED_ROOTS = []

    # If no roots specified, use PROJECT_ROOT
    roots_to_check = ALLOWED_ROOTS if ALLOWED_ROOTS else [PROJECT_ROOT]

    for root in roots_to_check:
        try:
            resolved.relative_to(root.resolve())
            return True
        except ValueError:
            continue

    return False


def _validate_path_security(path: Path) -> Path:
    """
    Validate path is within allowed roots and return resolved path.

    [20251215_FEATURE] v2.0.0 - Security validation with helpful errors

    Args:
        path: Path to validate

    Returns:
        Resolved path if valid

    Raises:
        PermissionError: If path is outside allowed roots

    [20251220_TODO] Add audit logging for path access:
        - Log all path validations with caller identity
        - Track denied access attempts
        - Generate security alerts for suspicious patterns
        - Support per-client access control lists
    """
    resolved = path.resolve()

    if not _is_path_allowed(resolved):
        roots_str = ", ".join(str(r) for r in (ALLOWED_ROOTS or [PROJECT_ROOT]))
        raise PermissionError(
            f"Access denied: {path} is outside allowed roots.\n"
            f"Allowed roots: {roots_str}\n"
            f"Set roots via the roots/list capability or SCALPEL_ROOT environment variable."
        )

    return resolved


async def _fetch_and_cache_roots(ctx: Context | None) -> list[Path]:
    """
    Fetch roots from client via MCP context and cache in ALLOWED_ROOTS.

    [20251215_FEATURE] v2.0.0 - Dynamic roots capability support

    This function requests the list of allowed filesystem roots from the
    MCP client. Roots define the boundaries where the server can operate.

    Args:
        ctx: MCP Context object (from tool execution)

    Returns:
        List of allowed root paths

    Note:
        If ctx is None or client doesn't support roots, returns PROJECT_ROOT.
        Roots are cached in ALLOWED_ROOTS global for subsequent calls.
    """
    global ALLOWED_ROOTS

    if ctx is None:
        return [PROJECT_ROOT]

    try:
        # Request roots from client via MCP protocol
        # Note: list_roots may not be available on all Context implementations
        list_roots_fn = getattr(ctx, "list_roots", None)
        if list_roots_fn is None:
            return [PROJECT_ROOT]
        roots = await list_roots_fn()

        if roots:
            # Convert file:// URIs to Path objects
            ALLOWED_ROOTS = []
            for root in roots:
                uri = str(root.uri)
                if uri.startswith("file://"):
                    # Handle file:// URIs (e.g., file:///home/user/project)
                    # Remove 'file://' prefix and handle Windows paths
                    path_str = uri[7:]  # Remove 'file://'
                    # Windows paths may have extra slash: file:///C:/path
                    if len(path_str) >= 3 and path_str[0] == "/" and path_str[2] == ":":
                        path_str = path_str[1:]  # Remove leading /
                    ALLOWED_ROOTS.append(Path(path_str))
                else:
                    # Non-file URIs - log warning but try as path
                    logger.warning(f"Non-file root URI: {uri}")
                    ALLOWED_ROOTS.append(Path(uri))

            logger.debug(f"Updated ALLOWED_ROOTS from client: {ALLOWED_ROOTS}")
            return ALLOWED_ROOTS
        else:
            return [PROJECT_ROOT]

    except Exception as e:
        # Client may not support roots capability
        logger.debug(f"Could not fetch roots from client: {e}")
        return [PROJECT_ROOT]


# ============================================================================
# CACHING
# ============================================================================


def _get_cache():
    """Get the analysis cache (lazy initialization)."""
    if not CACHE_ENABLED:
        return None
    try:
        # [20251223_CONSOLIDATION] Import from unified cache
        from code_scalpel.cache import get_cache

        return get_cache()
    except ImportError:
        logger.warning("Cache module not available")
        return None


# ============================================================================
# STRUCTURED OUTPUT MODELS
# ============================================================================


class FunctionInfo(BaseModel):
    """Information about a function."""

    name: str = Field(description="Function name")
    lineno: int = Field(description="Line number where function starts")
    end_lineno: int | None = Field(
        default=None, description="Line number where function ends"
    )
    is_async: bool = Field(default=False, description="Whether function is async")


class ClassInfo(BaseModel):
    """Information about a class."""

    name: str = Field(description="Class name")
    lineno: int = Field(description="Line number where class starts")
    end_lineno: int | None = Field(
        default=None, description="Line number where class ends"
    )
    methods: list[str] = Field(
        default_factory=list, description="Method names in class"
    )


class AnalysisResult(BaseModel):
    """Result of code analysis.

    [20251229_FEATURE] v3.3.0 - Added cognitive_complexity and code_smells for tier-based features.
    [20251226_FEATURE] Token efficiency - Removed redundant fields, success only included when False.
    """

    # [20251228_BUGFIX] Restore explicit success flag for stable test/tool contract.
    success: bool = Field(
        default=True,
        description="Whether analysis succeeded.",
    )
    functions: list[str] = Field(description="List of function names found")
    classes: list[str] = Field(description="List of class names found")
    imports: list[str] = Field(description="List of import statements")
    complexity: int = Field(description="Cyclomatic complexity estimate")
    lines_of_code: int = Field(description="Total lines of code")
    issues: list[str] = Field(default_factory=list, description="Issues found")
    error: str | None = Field(default=None, description="Error message if failed")
    # v1.3.0: Detailed info with line numbers
    function_details: list[FunctionInfo] = Field(
        default_factory=list, description="Detailed function info with line numbers"
    )
    class_details: list[ClassInfo] = Field(
        default_factory=list, description="Detailed class info with line numbers"
    )
    # [20251229_FEATURE] v3.3.0: Tier-gated advanced metrics
    cognitive_complexity: int = Field(
        default=0,
        description="Cognitive complexity score (PRO/ENTERPRISE tier, 0 if COMMUNITY)",
    )
    code_smells: list[str] = Field(
        default_factory=list,
        description="Detected code smells (PRO/ENTERPRISE tier, empty if COMMUNITY)",
    )
    # NOTE: The advanced fields below are intentionally declared once.
    # [20251225_FEATURE] Tier-gated advanced analysis outputs
    halstead_metrics: dict[str, float] | None = Field(
        default=None,
        description="Halstead metrics (PRO/ENTERPRISE; None if not computed)",
    )
    duplicate_code_blocks: list[dict[str, Any]] = Field(
        default_factory=list,
        description="Detected duplicate code blocks (PRO/ENTERPRISE)",
    )
    dependency_graph: dict[str, list[str]] = Field(
        default_factory=dict,
        description="Function call adjacency map (PRO/ENTERPRISE)",
    )
    naming_issues: list[str] = Field(
        default_factory=list,
        description="Naming convention issues (ENTERPRISE)",
    )
    compliance_issues: list[str] = Field(
        default_factory=list,
        description="Compliance findings (ENTERPRISE)",
    )
    custom_rule_violations: list[dict[str, Any]] = Field(
        default_factory=list,
        description="Custom rule matches (ENTERPRISE)",
    )
    organization_patterns: list[str] = Field(
        default_factory=list,
        description="Detected architectural patterns (ENTERPRISE)",
    )

    # [20251231_FEATURE] v3.3.x: Additional tier-gated enrichments (best-effort)
    frameworks: list[str] = Field(
        default_factory=list,
        description="Detected application/framework context (PRO/ENTERPRISE; empty if not computed)",
    )
    dead_code_hints: list[str] = Field(
        default_factory=list,
        description="Dead code candidates/unreachable code hints (PRO/ENTERPRISE; empty if not computed)",
    )
    decorator_summary: dict[str, Any] | None = Field(
        default=None,
        description="Decorator/annotation summary (PRO/ENTERPRISE; None if not computed)",
    )
    type_summary: dict[str, Any] | None = Field(
        default=None,
        description="Type annotation / generic usage summary (PRO/ENTERPRISE; None if not computed)",
    )
    architecture_patterns: list[str] = Field(
        default_factory=list,
        description="Architecture/service-pattern hints (ENTERPRISE; empty if not computed)",
    )
    technical_debt: dict[str, Any] | None = Field(
        default=None,
        description="Technical debt scoring summary (ENTERPRISE; None if not computed)",
    )
    api_surface: dict[str, Any] | None = Field(
        default=None,
        description="API surface summary (ENTERPRISE; None if not computed)",
    )
    prioritized: bool = Field(
        default=False,
        description="Whether outputs were priority-ordered (ENTERPRISE)",
    )
    complexity_trends: dict[str, Any] | None = Field(
        default=None,
        description="Historical complexity trend summary keyed by file_path (ENTERPRISE; None if unavailable)",
    )

    # [20260110_FEATURE] v1.0 - Output metadata for transparency
    language_detected: str | None = Field(
        default=None,
        description="Language that was actually analyzed (python/javascript/typescript/java)",
    )
    tier_applied: str | None = Field(
        default=None,
        description="Tier applied for feature gating (community/pro/enterprise)",
    )

    # [20251228_BUGFIX] Backward-compatible convenience counts used by tests.
    @property
    def function_count(self) -> int:
        return len(self.functions)

    @property
    def class_count(self) -> int:
        return len(self.classes)

    @property
    def import_count(self) -> int:
        return len(self.imports)


class VulnerabilityInfo(BaseModel):
    """Information about a detected vulnerability."""

    type: str = Field(description="Vulnerability type (e.g., SQL Injection)")
    cwe: str = Field(description="CWE identifier")
    severity: str = Field(description="Severity level")
    line: int | None = Field(default=None, description="Line number if known")
    description: str = Field(description="Description of the vulnerability")


class SecurityResult(BaseModel):
    """Result of security analysis."""

    success: bool = Field(description="Whether analysis succeeded")
    server_version: str = Field(default=__version__, description="Code Scalpel version")
    has_vulnerabilities: bool = Field(description="Whether vulnerabilities were found")
    vulnerability_count: int = Field(description="Number of vulnerabilities")
    risk_level: str = Field(description="Overall risk level")
    vulnerabilities: list[VulnerabilityInfo] = Field(
        default_factory=list, description="List of vulnerabilities"
    )
    taint_sources: list[str] = Field(
        default_factory=list, description="Identified taint sources"
    )
    # [20251226_FEATURE] Tier-aware optional security outputs for Pro/Enterprise
    sanitizer_paths: list[str] | None = Field(
        default=None, description="Detected sanitizer usages (Pro/Enterprise)"
    )
    confidence_scores: dict[str, float] | None = Field(
        default=None, description="Heuristic confidence scores per finding"
    )
    false_positive_analysis: dict[str, Any] | None = Field(
        default=None, description="False-positive reduction metadata"
    )
    # [20260118_FEATURE] v1.0 - Pro tier remediation suggestions
    remediation_suggestions: list[str] | None = Field(
        default=None,
        description="Remediation suggestions per vulnerability (Pro/Enterprise)",
    )
    policy_violations: list[dict[str, Any]] | None = Field(
        default=None, description="Custom policy violations (Enterprise)"
    )
    compliance_mappings: dict[str, list[str]] | None = Field(
        default=None, description="Compliance framework mappings (Enterprise)"
    )
    custom_rule_results: list[dict[str, Any]] | None = Field(
        default=None, description="Custom rule matches (Enterprise)"
    )
    # [20251230_FEATURE] v1.0 roadmap Enterprise tier fields
    priority_ordered_findings: list[dict[str, Any]] | None = Field(
        default=None, description="Findings sorted by priority (Enterprise)"
    )
    reachability_analysis: dict[str, Any] | None = Field(
        default=None, description="Vulnerability reachability analysis (Enterprise)"
    )
    false_positive_tuning: dict[str, Any] | None = Field(
        default=None, description="False positive tuning results (Enterprise)"
    )
    error: str | None = Field(default=None, description="Error message if failed")


# [20251216_FEATURE] Unified sink detection result model
class UnifiedDetectedSink(BaseModel):
    """Detected sink with confidence and OWASP mapping."""

    # [20260110_FEATURE] v1.0 - Stable identifier for correlation across runs
    sink_id: str = Field(
        description="Stable sink identifier (for correlation across runs)"
    )

    pattern: str = Field(description="Sink pattern matched")
    sink_type: str = Field(description="Sink type classification")
    confidence: float = Field(description="Confidence score (0.0-1.0)")
    line: int = Field(default=0, description="Line number of sink occurrence")
    column: int = Field(default=0, description="Column offset of sink occurrence")
    code_snippet: str = Field(default="", description="Snippet around the sink")
    # [20260110_FEATURE] v1.0 - Snippet truncation observability
    code_snippet_truncated: bool = Field(
        default=False, description="Whether code_snippet was truncated"
    )
    code_snippet_original_len: int | None = Field(
        default=None, description="Original snippet length before truncation"
    )
    vulnerability_type: str | None = Field(
        default=None, description="Vulnerability category key"
    )
    owasp_category: str | None = Field(
        default=None, description="Mapped OWASP Top 10 category"
    )
    # [20251231_FEATURE] v1.0 - Added CWE mapping
    cwe_id: str | None = Field(
        default=None, description="CWE identifier (e.g., CWE-89)"
    )


class UnifiedSinkResult(BaseModel):
    """Result of unified polyglot sink detection."""

    success: bool = Field(description="Whether detection succeeded")
    # [20260110_FEATURE] v1.0 - Machine-readable failures
    error_code: str | None = Field(
        default=None, description="Machine-readable error code"
    )
    server_version: str = Field(default=__version__, description="Code Scalpel version")
    language: str = Field(description="Language analyzed")
    sink_count: int = Field(description="Number of sinks detected")
    sinks: list[UnifiedDetectedSink] = Field(
        default_factory=list, description="Detected sinks meeting threshold"
    )
    coverage_summary: dict[str, Any] = Field(
        default_factory=dict, description="Summary of sink pattern coverage"
    )
    # [20251225_FEATURE] Tier-specific outputs for Pro/Enterprise
    logic_sinks: list[dict[str, Any]] = Field(
        default_factory=list,
        description="Logic sinks (S3/email/payment) detected at Pro tier",
    )
    extended_language_sinks: dict[str, list[dict[str, Any]]] = Field(
        default_factory=dict,
        description="Additional language sink details for extended support",
    )
    confidence_scores: dict[str, float] = Field(
        default_factory=dict, description="Per-sink confidence scores (Pro/Enterprise)"
    )
    sink_categories: dict[str, list[dict[str, Any]]] = Field(
        default_factory=dict, description="Enterprise sink categorization by risk level"
    )
    risk_assessments: list[dict[str, Any]] = Field(
        default_factory=list, description="Enterprise risk assessments with clearance"
    )
    custom_sink_matches: list[dict[str, Any]] = Field(
        default_factory=list, description="Enterprise custom sink pattern matches"
    )
    # [20251231_FEATURE] v1.0 - New fields for roadmap compliance
    context_analysis: dict[str, Any] | None = Field(
        default=None, description="[Pro] Context-aware detection results"
    )
    framework_sinks: list[dict[str, Any]] = Field(
        default_factory=list, description="[Pro] Framework-specific sink detections"
    )
    compliance_mapping: dict[str, Any] | None = Field(
        default=None,
        description="[Enterprise] Compliance standard mappings (SOC2, HIPAA)",
    )
    historical_comparison: dict[str, Any] | None = Field(
        default=None, description="[Enterprise] Historical sink tracking comparison"
    )
    remediation_suggestions: list[dict[str, Any]] = Field(
        default_factory=list,
        description="[Enterprise] Automated remediation suggestions",
    )

    # [20260110_FEATURE] v1.0 - Limit observability (populated when applicable)
    truncated: bool | None = Field(
        default=None, description="Whether results were truncated"
    )
    sinks_detected: int | None = Field(
        default=None, description="Total sinks detected before truncation"
    )
    max_sinks_applied: int | None = Field(
        default=None, description="max_sinks limit applied to this response"
    )
    error: str | None = Field(default=None, description="Error message if failed")


class PathCondition(BaseModel):
    """A condition along an execution path."""

    condition: str = Field(description="The condition expression")
    is_satisfiable: bool = Field(description="Whether condition is satisfiable")


class ExecutionPath(BaseModel):
    """An execution path discovered by symbolic execution."""

    path_id: int = Field(description="Unique path identifier")
    conditions: list[str] = Field(description="Conditions along the path")
    final_state: dict[str, Any] = Field(description="Variable values at path end")
    reproduction_input: dict[str, Any] | None = Field(
        default=None, description="Input values that trigger this path"
    )
    is_reachable: bool = Field(description="Whether path is reachable")


class SymbolicResult(BaseModel):
    """Result of symbolic execution."""

    success: bool = Field(description="Whether analysis succeeded")
    server_version: str = Field(default=__version__, description="Code Scalpel version")
    paths_explored: int = Field(description="Number of execution paths explored")
    paths: list[ExecutionPath] = Field(
        default_factory=list,
        description="Discovered execution paths (may be limited by configuration)",
    )
    symbolic_variables: list[str] = Field(
        default_factory=list, description="Variables treated symbolically"
    )
    constraints: list[str] = Field(
        default_factory=list, description="Discovered constraints"
    )
    total_paths: int | None = Field(
        default=None,
        description="Total paths discovered before limiting (if known)",
    )
    truncated: bool = Field(
        default=False,
        description="Whether paths were limited by configuration",
    )
    truncation_warning: str | None = Field(
        default=None,
        description="Neutral warning when results are limited by configuration",
    )
    # [20251230_FEATURE] v1.0 roadmap Pro/Enterprise tier fields
    path_prioritization: dict[str, Any] | None = Field(
        default=None, description="Path prioritization metadata (Pro/Enterprise)"
    )
    concolic_results: dict[str, Any] | None = Field(
        default=None, description="Concolic execution results (Pro/Enterprise)"
    )
    state_space_analysis: dict[str, Any] | None = Field(
        default=None, description="State space reduction analysis (Enterprise)"
    )
    memory_model: dict[str, Any] | None = Field(
        default=None, description="Memory modeling results (Enterprise)"
    )
    error: str | None = Field(default=None, description="Error message if failed")


class GeneratedTestCase(BaseModel):
    """A generated test case."""

    path_id: int = Field(description="Path ID this test covers")
    function_name: str = Field(description="Function being tested")
    inputs: dict[str, Any] = Field(description="Input values for this test")
    description: str = Field(description="Human-readable description")
    path_conditions: list[str] = Field(
        default_factory=list, description="Conditions that define this path"
    )


class TestGenerationResult(BaseModel):
    """Result of test generation."""

    success: bool = Field(description="Whether generation succeeded")
    server_version: str = Field(default=__version__, description="Code Scalpel version")
    function_name: str = Field(description="Function tests were generated for")
    test_count: int = Field(description="Number of test cases generated")
    test_cases: list[GeneratedTestCase] = Field(
        default_factory=list, description="Generated test cases"
    )
    # [20251225_FEATURE] Tier-aware truncation metadata (neutral messaging).
    total_test_cases: int = Field(
        default=0, description="Total test cases before truncation"
    )
    truncated: bool = Field(default=False, description="Whether results were truncated")
    truncation_warning: str | None = Field(
        default=None, description="Neutral warning when truncation occurs"
    )
    pytest_code: str = Field(default="", description="Generated pytest code")
    unittest_code: str = Field(default="", description="Generated unittest code")
    error: str | None = Field(default=None, description="Error message if failed")

    # [20260111_FEATURE] Output metadata for transparency
    tier_applied: str = Field(
        default="community",
        description="Tier used for this generation (community/pro/enterprise)",
    )
    framework_used: str = Field(
        default="pytest",
        description="Test framework used for generation",
    )
    max_test_cases_limit: int | None = Field(
        default=None,
        description="Max test cases limit applied (None=unlimited)",
    )
    data_driven_enabled: bool = Field(
        default=False,
        description="Whether data-driven/parametrized tests were generated",
    )
    bug_reproduction_enabled: bool = Field(
        default=False,
        description="Whether bug reproduction mode was used",
    )


class RefactorSecurityIssue(BaseModel):
    """A security issue found in refactored code."""

    type: str = Field(description="Vulnerability type")
    severity: str = Field(description="Severity level")
    line: int | None = Field(default=None, description="Line number")
    description: str = Field(description="Issue description")
    cwe: str | None = Field(default=None, description="CWE identifier")


class RefactorSimulationResult(BaseModel):
    """Result of refactor simulation."""

    success: bool = Field(description="Whether simulation succeeded")
    server_version: str = Field(default=__version__, description="Code Scalpel version")
    is_safe: bool = Field(description="Whether the refactor is safe to apply")
    status: str = Field(description="Status: safe, unsafe, warning, or error")
    reason: str | None = Field(default=None, description="Reason if not safe")
    security_issues: list[RefactorSecurityIssue] = Field(
        default_factory=list, description="Security issues found"
    )
    structural_changes: dict[str, Any] = Field(
        default_factory=dict, description="Functions/classes added/removed"
    )
    warnings: list[str] = Field(default_factory=list, description="Warnings")
    error: str | None = Field(default=None, description="Error message if failed")


class CrawlFunctionInfo(BaseModel):
    """Information about a function from project crawl."""

    name: str = Field(description="Function name (qualified if method)")
    lineno: int = Field(description="Line number")
    complexity: int = Field(description="Cyclomatic complexity")


class CrawlClassInfo(BaseModel):
    """Information about a class from project crawl."""

    name: str = Field(description="Class name")
    lineno: int = Field(description="Line number")
    methods: list[CrawlFunctionInfo] = Field(
        default_factory=list, description="Methods in the class"
    )
    bases: list[str] = Field(default_factory=list, description="Base classes")


class CrawlFileResult(BaseModel):
    """Result of analyzing a single file during crawl."""

    path: str = Field(description="Relative path to the file")
    status: str = Field(description="success or error")
    lines_of_code: int = Field(default=0, description="Lines of code")
    functions: list[CrawlFunctionInfo] = Field(
        default_factory=list, description="Top-level functions"
    )
    classes: list[CrawlClassInfo] = Field(
        default_factory=list, description="Classes found"
    )
    imports: list[str] = Field(default_factory=list, description="Import statements")
    complexity_warnings: list[CrawlFunctionInfo] = Field(
        default_factory=list, description="High-complexity functions"
    )
    error: str | None = Field(default=None, description="Error if failed")


class CrawlSummary(BaseModel):
    """Summary statistics from project crawl."""

    total_files: int = Field(description="Total files scanned")
    successful_files: int = Field(description="Files analyzed successfully")
    failed_files: int = Field(description="Files that failed analysis")
    total_lines_of_code: int = Field(description="Total lines of code")
    total_functions: int = Field(description="Total functions found")
    total_classes: int = Field(description="Total classes found")
    complexity_warnings: int = Field(description="Number of high-complexity functions")


class ProjectCrawlResult(BaseModel):
    """Result of crawling an entire project."""

    # Allow tier-gated feature fields without breaking older clients.
    try:
        from pydantic import ConfigDict as _ConfigDict  # type: ignore

        model_config = _ConfigDict(extra="allow")
    except Exception:

        class Config:  # type: ignore
            extra = "allow"

    success: bool = Field(description="Whether crawl succeeded")
    server_version: str = Field(default=__version__, description="Code Scalpel version")
    root_path: str = Field(description="Project root path")
    timestamp: str = Field(description="When the crawl was performed")
    summary: CrawlSummary = Field(description="Summary statistics")
    files: list[CrawlFileResult] = Field(
        default_factory=list, description="Analyzed files"
    )
    errors: list[CrawlFileResult] = Field(
        default_factory=list, description="Files with errors"
    )
    markdown_report: str = Field(default="", description="Markdown report")
    error: str | None = Field(default=None, description="Error if failed")
    # [20260106_FEATURE] v1.0 pre-release - Output transparency metadata
    tier_applied: str | None = Field(
        default=None,
        description="Which tier's rules were applied (community/pro/enterprise)",
    )
    crawl_mode: str | None = Field(
        default=None,
        description="Crawl mode used: 'discovery' (Community) or 'deep' (Pro/Enterprise)",
    )
    files_limit_applied: int | None = Field(
        default=None, description="Max files limit that was applied (None = unlimited)"
    )
    # Tier-gated fields (best-effort, optional)
    language_breakdown: dict[str, int] | None = Field(
        default=None, description="Counts of files per detected language"
    )
    cache_hits: int | None = Field(
        default=None,
        description="Number of files reused from cache (Pro/Enterprise incremental)",
    )
    compliance_summary: dict[str, Any] | None = Field(
        default=None, description="Enterprise compliance scanning summary"
    )
    framework_hints: list[str] | None = Field(
        default=None, description="Detected frameworks/entrypoints in discovery mode"
    )
    entrypoints: list[str] | None = Field(
        default=None, description="Detected entrypoint file paths"
    )


class SurgicalExtractionResult(BaseModel):
    """Result of surgical code extraction."""

    success: bool = Field(description="Whether extraction succeeded")
    server_version: str = Field(default=__version__, description="Code Scalpel version")
    name: str = Field(description="Name of extracted element")
    code: str = Field(description="Extracted source code")
    node_type: str = Field(description="Type: function, class, or method")
    line_start: int = Field(default=0, description="Starting line number")
    line_end: int = Field(default=0, description="Ending line number")
    dependencies: list[str] = Field(
        default_factory=list, description="Names of dependencies"
    )
    imports_needed: list[str] = Field(
        default_factory=list, description="Required import statements"
    )
    token_estimate: int = Field(default=0, description="Estimated token count")
    error: str | None = Field(default=None, description="Error if failed")


class ContextualExtractionResult(BaseModel):
    """Result of extraction with dependencies included."""

    # Allow tier-gated feature fields without breaking older clients.
    try:
        from pydantic import ConfigDict as _ConfigDict  # type: ignore

        model_config = _ConfigDict(extra="allow")
    except Exception:

        class Config:  # type: ignore
            extra = "allow"

    success: bool = Field(description="Whether extraction succeeded")
    server_version: str = Field(default=__version__, description="Code Scalpel version")
    target_name: str = Field(description="Name of target element")
    target_code: str = Field(description="Target element source code")
    context_code: str = Field(description="Combined dependency source code")
    full_code: str = Field(description="Complete code block for LLM consumption")
    context_items: list[str] = Field(
        default_factory=list, description="Names of included dependencies"
    )
    total_lines: int = Field(default=0, description="Total lines in extraction")
    # v1.3.0: Line number information
    line_start: int = Field(default=0, description="Starting line number of target")
    line_end: int = Field(default=0, description="Ending line number of target")
    token_estimate: int = Field(default=0, description="Estimated token count")
    error: str | None = Field(default=None, description="Error if failed")

    # [20260111_FEATURE] Output metadata for transparency
    tier_applied: str = Field(
        default="community",
        description="Tier used for this extraction (community/pro/enterprise)",
    )
    language_detected: str | None = Field(
        default=None,
        description="Language detected/used for extraction (python/javascript/typescript/java)",
    )
    cross_file_deps_enabled: bool = Field(
        default=False,
        description="Whether cross-file dependency resolution was enabled",
    )
    max_depth_applied: int | None = Field(
        default=None,
        description="Max depth limit applied for context/dependencies (None=unlimited)",
    )

    # [20251216_FEATURE] v2.0.2 - JSX/TSX extraction metadata
    jsx_normalized: bool = Field(
        default=False, description="Whether JSX syntax was normalized"
    )
    is_server_component: bool = Field(
        default=False, description="Next.js Server Component (async)"
    )
    is_server_action: bool = Field(
        default=False, description="Next.js Server Action ('use server')"
    )
    component_type: str | None = Field(
        default=None, description="React component type: 'functional', 'class', or None"
    )
    # [20260103_FEATURE] v3.3.1 - Warnings field for tier-aware behavior messaging
    warnings: list[str] = Field(
        default_factory=list,
        description="Non-fatal warnings (e.g., tier-aware context depth clamping)",
    )
    advanced: dict[str, Any] = Field(
        default_factory=dict, description="Tier-specific or experimental metadata"
    )


class PatchResultModel(BaseModel):
    """Result of a surgical code modification."""

    success: bool = Field(description="Whether the patch was applied successfully")
    server_version: str = Field(default=__version__, description="Code Scalpel version")
    file_path: str = Field(description="Path to the modified file")
    target_name: str = Field(description="Name of the modified symbol")
    target_type: str = Field(description="Type: function, class, or method")
    lines_before: int = Field(default=0, description="Lines in original code")
    lines_after: int = Field(default=0, description="Lines in replacement code")
    lines_delta: int = Field(default=0, description="Change in line count")
    backup_path: str | None = Field(default=None, description="Path to backup file")
    # [20260110_FEATURE] Machine-readable failure signaling for minimal output profiles.
    error_code: str | None = Field(
        default=None, description="Machine-readable error code if failed"
    )
    hint: str | None = Field(
        default=None, description="Actionable hint to remediate failure"
    )
    # [20260110_FEATURE] Session limit observability for update_symbol.
    max_updates_per_session: int | None = Field(
        default=None, description="Session max updates applied for this tool"
    )
    updates_used: int | None = Field(
        default=None, description="Updates used in this session for this tool"
    )
    updates_remaining: int | None = Field(
        default=None, description="Remaining updates in this session for this tool"
    )
    # [20251225_FEATURE] Optional warnings for tier-aware behavior (neutral messaging).
    warnings: list[str] = Field(
        default_factory=list,
        description="Non-fatal warnings (neutral; no upgrade messaging)",
    )
    error: str | None = Field(default=None, description="Error message if failed")


# [20251212_FEATURE] v1.4.0 - New MCP tool models for enhanced AI context


class FileContextResult(BaseModel):
    """Result of get_file_context - file overview without full content."""

    success: bool = Field(description="Whether analysis succeeded")
    server_version: str = Field(default=__version__, description="Code Scalpel version")

    # [20260111_FEATURE] v1.0 - Output metadata fields for transparency
    tier_applied: str = Field(
        default="community",
        description="The tier used for analysis (community, pro, enterprise)",
    )
    max_context_lines_applied: int | None = Field(
        default=None,
        description="The max context lines limit applied (None = unlimited for Enterprise)",
    )
    pro_features_enabled: bool = Field(
        default=False,
        description="Whether Pro tier features were enabled (code smells, doc coverage, maintainability)",
    )
    enterprise_features_enabled: bool = Field(
        default=False,
        description="Whether Enterprise tier features were enabled (metadata, compliance, owners)",
    )

    file_path: str = Field(description="Path to the analyzed file")
    language: str = Field(default="python", description="Detected language")
    line_count: int = Field(description="Total lines in file")
    functions: list[FunctionInfo | str] = Field(
        default_factory=list, description="Function names or detailed info"
    )
    classes: list[ClassInfo | str] = Field(
        default_factory=list, description="Class names or detailed info"
    )
    imports: list[str] = Field(
        default_factory=list, description="Import statements (max 20)"
    )
    exports: list[str] = Field(
        default_factory=list, description="Exported symbols (__all__)"
    )
    complexity_score: int = Field(
        default=0, description="Overall cyclomatic complexity"
    )
    has_security_issues: bool = Field(
        default=False, description="Whether file has security issues"
    )
    summary: str = Field(default="", description="Brief description of file purpose")
    # [20251220_FEATURE] v3.0.5 - Truncation communication
    imports_truncated: bool = Field(
        default=False, description="Whether imports list was truncated"
    )
    total_imports: int = Field(default=0, description="Total imports before truncation")
    # [20251225_FEATURE] v3.3.0 - Tiered enrichments and safeguards
    semantic_summary: str | None = Field(
        default=None, description="AI-lite semantic summary when enabled"
    )
    intent_tags: list[str] = Field(
        default_factory=list,
        description="Extracted intents/topics from docstrings/names",
    )
    related_imports: list[str] = Field(
        default_factory=list,
        description="Resolved related imports for context expansion",
    )
    expanded_context: str | None = Field(
        default=None, description="Smartly expanded context when allowed by tier limits"
    )
    pii_redacted: bool = Field(
        default=False, description="Whether PII content was redacted"
    )
    secrets_masked: bool = Field(
        default=False, description="Whether secrets/API keys were masked"
    )
    redaction_summary: list[str] = Field(
        default_factory=list, description="What redactions/masking actions were taken"
    )
    access_controlled: bool = Field(
        default=False, description="Whether RBAC/file access controls were applied"
    )
    # [20251231_FEATURE] v3.3.1 - Pro tier: code quality metrics
    code_smells: list[dict[str, Any]] = Field(
        default_factory=list,
        description="Detected code smells (Pro+): [{type, line, message, severity}]",
    )
    doc_coverage: float | None = Field(
        default=None,
        description="Documentation coverage percentage (Pro+): 0.0-100.0",
    )
    maintainability_index: float | None = Field(
        default=None,
        description="Maintainability index (Pro+): 0-100 scale (higher is better)",
    )
    # [20251231_FEATURE] v3.3.1 - Enterprise tier: organizational metadata
    custom_metadata: dict[str, Any] = Field(
        default_factory=dict,
        description="Custom metadata from .code-scalpel/metadata.yaml (Enterprise)",
    )
    compliance_flags: list[str] = Field(
        default_factory=list,
        description="Compliance flags detected (Enterprise): HIPAA, PCI, SOC2, etc.",
    )
    technical_debt_score: float | None = Field(
        default=None,
        description="Technical debt score (Enterprise): estimated hours to fix issues",
    )
    owners: list[str] = Field(
        default_factory=list,
        description="Code owners from CODEOWNERS file (Enterprise)",
    )
    historical_metrics: dict[str, Any] | None = Field(
        default=None,
        description="Historical metrics from git (Enterprise): churn, age, contributors",
    )
    error: str | None = Field(default=None, description="Error message if failed")


class SymbolReference(BaseModel):
    """A single reference to a symbol."""

    file: str = Field(description="File path containing the reference")
    line: int = Field(description="Line number of the reference")
    column: int = Field(default=0, description="Column number")
    context: str = Field(description="Code snippet showing usage context")
    is_definition: bool = Field(
        default=False, description="Whether this is the definition"
    )
    # [20251225_FEATURE] Tiered symbol references: optional metadata
    reference_type: str | None = Field(
        default=None,
        description="Reference category (definition|import|call|read|write|reference)",
    )
    is_test_file: bool = Field(
        default=False,
        description="Whether the reference is in a test file",
    )
    owners: list[str] | None = Field(
        default=None,
        description="CODEOWNERS owners for the file (Enterprise)",
    )


class SymbolReferencesResult(BaseModel):
    """Result of get_symbol_references - all usages of a symbol."""

    success: bool = Field(description="Whether search succeeded")
    server_version: str = Field(default=__version__, description="Code Scalpel version")

    # [20250112_FEATURE] Output metadata fields for tier transparency
    tier_applied: str = Field(
        default="community",
        description="The tier that was applied to this request (community, pro, enterprise)",
    )
    max_files_applied: int | None = Field(
        default=None,
        description="The max_files_searched limit applied",
    )
    max_references_applied: int | None = Field(
        default=None,
        description="The max_references limit applied",
    )
    pro_features_enabled: list[str] | None = Field(
        default=None,
        description="List of Pro tier features enabled (None if community tier)",
    )
    enterprise_features_enabled: list[str] | None = Field(
        default=None,
        description="List of Enterprise tier features enabled (None if not enterprise)",
    )

    symbol_name: str = Field(description="Name of the searched symbol")
    definition_file: str | None = Field(
        default=None, description="File where symbol is defined"
    )
    definition_line: int | None = Field(
        default=None, description="Line where symbol is defined"
    )
    references: list[SymbolReference] = Field(
        default_factory=list, description="References found (max 100)"
    )
    total_references: int = Field(
        default=0, description="Total reference count before truncation"
    )
    # [20251225_FEATURE] Tiered symbol references: scan metadata
    files_scanned: int = Field(default=0, description="Number of files scanned")
    total_files: int = Field(
        default=0, description="Total candidate files before filtering"
    )
    files_truncated: bool = Field(
        default=False, description="Whether file scanning was truncated"
    )
    file_truncation_warning: str | None = Field(
        default=None, description="Warning if file scan was truncated"
    )
    category_counts: dict[str, int] | None = Field(
        default=None, description="Counts by reference category (Pro+)"
    )
    owner_counts: dict[str, int] | None = Field(
        default=None, description="Counts by CODEOWNERS owner (Enterprise)"
    )
    change_risk: str | None = Field(
        default=None, description="Heuristic change risk (Enterprise)"
    )
    # [20251220_FEATURE] v3.0.5 - Truncation communication
    references_truncated: bool = Field(
        default=False, description="Whether references list was truncated"
    )
    truncation_warning: str | None = Field(
        default=None, description="Warning if results truncated"
    )
    # [20251226_FEATURE] Enterprise tier: Impact analysis fields
    risk_score: int | None = Field(
        default=None, description="Weighted risk score 0-100 (Enterprise)"
    )
    risk_factors: list[str] | None = Field(
        default=None, description="List of factors contributing to risk (Enterprise)"
    )
    blast_radius: int | None = Field(
        default=None, description="Number of unique files affected (Enterprise)"
    )
    test_coverage_ratio: float | None = Field(
        default=None, description="Ratio of references in test files (Enterprise)"
    )
    complexity_hotspots: list[str] | None = Field(
        default=None,
        description="High-complexity files containing references (Enterprise)",
    )
    impact_mermaid: str | None = Field(
        default=None,
        description="Mermaid diagram of reference distribution (Enterprise)",
    )
    codeowners_coverage: float | None = Field(
        default=None,
        description="Ratio of references with CODEOWNERS attribution (Enterprise)",
    )
    error: str | None = Field(default=None, description="Error message if failed")


# Wrap tool registration to make ALL tools return ToolResponseEnvelope in their signature.
_original_add_tool = mcp._tool_manager.add_tool


def _add_tool_with_envelope_output(
    fn: Callable[..., Any], **add_tool_kwargs: Any
) -> Any:
    """Register a tool normally, then wrap its run() method to return ToolResponseEnvelope."""
    # Force structured_output=False so FastMCP doesn't validate output against schema
    # We'll return ToolResponseEnvelope as unstructured JSON
    add_tool_kwargs["structured_output"] = False

    # Register the tool - let FastMCP build input schema from the original function
    tool = _original_add_tool(fn, **add_tool_kwargs)

    # Save the original run method
    original_run = tool.run

    async def _enveloped_run(
        arguments: dict[str, Any], context: Any = None, convert_result: bool = False
    ) -> dict[str, Any]:
        """Wrapped run() that returns ToolResponseEnvelope as a dict."""
        import time as time_module
        import uuid as uuid_module

        started = time_module.perf_counter()
        request_id = uuid_module.uuid4().hex
        tier = _get_current_tier()

        # [20251230_FEATURE] Invisible governance enforcement.
        # Verify `.code-scalpel` integrity before executing any tool when enabled.
        gov_error, gov_warnings = _maybe_enforce_governance_before_tool(
            tool_id=tool.name, tier=tier, arguments=arguments
        )
        if gov_error is not None:
            duration_ms = int((time_module.perf_counter() - started) * 1000)
            return ToolResponseEnvelope(
                tier=tier,
                tool_version=__version__,
                tool_id=tool.name,
                request_id=request_id,
                capabilities=["envelope-v1"],
                duration_ms=duration_ms,
                error=gov_error,
                upgrade_hints=[],
                warnings=gov_warnings,
                data=None,
            ).model_dump(mode="json", exclude_none=False)

        try:
            # Call the original run method with convert_result=False to get raw tool output
            # We'll wrap it in an envelope, so FastMCP's conversion happens at envelope level
            result = await original_run(
                arguments, context=context, convert_result=False
            )

            duration_ms = int((time_module.perf_counter() - started) * 1000)

            # Check for failure indicators in the result
            success = None
            err_msg = None
            if isinstance(result, BaseModel):
                success = getattr(result, "success", None)
                err_msg = getattr(result, "error", None)
            elif isinstance(result, dict):
                success = result.get("success")
                err_msg = result.get("error")

            error_obj = None
            upgrade_hints = []
            if success is False and err_msg:
                from code_scalpel.mcp.contract import (
                    ToolError,
                    _classify_failure_message,
                )

                code = _classify_failure_message(err_msg) or "internal_error"
                error_details = None

                if code == "upgrade_required":
                    # [20260112_REFACTOR] Removed inline upgrade URL parsing - hints belong in docs only.
                    # Error details kept minimal for token efficiency.
                    error_details = None

                    from code_scalpel.mcp.contract import UpgradeHint

                    upgrade_hints = [
                        UpgradeHint(
                            feature="tier_limited_feature",
                            tier="pro",
                            reason="Unavailable at current tier",
                        )
                    ]

                error_obj = ToolError(
                    error=err_msg,
                    error_code=code,
                    error_details=error_details,
                )

            # [20251228_BUGFIX] Contract-stable envelope: always include required fields.
            # Token efficiency is applied to the tool-specific `data` payload, not by
            # omitting top-level envelope keys. This keeps the wire schema stable for
            # MCP clients and tests.
            get_response_config()

            # Filter tool response data
            if isinstance(result, BaseModel):
                result_dict = result.model_dump(mode="json", exclude_none=True)
            elif isinstance(result, dict):
                result_dict = result
            else:
                result_dict = result

            # [20251228_BUGFIX] Contract payload consistency: ensure `success` is present.
            # Some tools return domain payloads without a `success` field (notably
            # `analyze_code`), but our MCP contract tests and clients expect it.
            inferred_success: bool
            if error_obj is not None:
                inferred_success = False
            elif success is True:
                inferred_success = True
            elif success is False:
                inferred_success = False
            else:
                inferred_success = True

            if isinstance(result_dict, dict):
                result_dict.setdefault("success", inferred_success)

            filtered_data = (
                filter_tool_response(result_dict, tool.name, tier)
                if isinstance(result_dict, dict)
                else result_dict
            )

            # [20251231_FIX] Add tool-specific capabilities to envelope
            envelope_caps = ["envelope-v1"]
            tool_caps_dict = get_tool_capabilities(tool.name, tier)
            if tool_caps_dict:
                tool_caps_raw = tool_caps_dict.get("capabilities", [])
                # Convert set to list if needed (TOOL_CAPABILITIES uses sets)
                tool_caps_list = (
                    list(tool_caps_raw)
                    if isinstance(tool_caps_raw, set)
                    else tool_caps_raw
                )
                envelope_caps.extend(tool_caps_list)

            # [20260112_FEATURE] Filter envelope fields based on response_config profile
            # This allows users to control metadata verbosity for token efficiency
            response_cfg = get_response_config()
            envelope_fields = response_cfg.get_envelope_fields(tool.name)

            # Build envelope with conditional field inclusion
            envelope = ToolResponseEnvelope(
                tier=tier if "tier" in envelope_fields else None,
                tool_version=__version__ if "tool_version" in envelope_fields else None,
                tool_id=tool.name if "tool_id" in envelope_fields else None,
                request_id=request_id if "request_id" in envelope_fields else None,
                capabilities=(
                    envelope_caps if "capabilities" in envelope_fields else None
                ),
                duration_ms=duration_ms if "duration_ms" in envelope_fields else None,
                error=(
                    error_obj if "error" in envelope_fields or error_obj else error_obj
                ),  # Always include errors
                upgrade_hints=(
                    upgrade_hints if "upgrade_hints" in envelope_fields else None
                ),
                warnings=(
                    gov_warnings if gov_warnings else []
                ),  # Always include governance warnings
                data=filtered_data,
            )
            return envelope.model_dump(mode="json", exclude_none=True)

        except BaseException as exc:
            duration_ms = int((time_module.perf_counter() - started) * 1000)
            from code_scalpel.mcp.contract import (
                ToolError,
                UpgradeHint,
                UpgradeRequiredError,
                _classify_exception,
            )

            code = _classify_exception(exc)

            # [20260112_REFACTOR] Use response_config for envelope filtering in error path too
            response_cfg = get_response_config()
            envelope_fields = response_cfg.get_envelope_fields(tool.name)

            # [20251228_FEATURE] Structured upgrade-required errors.
            if isinstance(exc, UpgradeRequiredError):
                # Build minimal error details (no upgrade_url - hints belong in docs)
                error_details = {
                    "feature": exc.feature,
                    "required_tier": exc.required_tier,
                }

                envelope = ToolResponseEnvelope(
                    tier=tier if "tier" in envelope_fields else None,
                    tool_version=(
                        __version__ if "tool_version" in envelope_fields else None
                    ),
                    tool_id=tool.name if "tool_id" in envelope_fields else None,
                    request_id=request_id if "request_id" in envelope_fields else None,
                    capabilities=(
                        ["envelope-v1"] if "capabilities" in envelope_fields else None
                    ),
                    duration_ms=(
                        duration_ms if "duration_ms" in envelope_fields else None
                    ),
                    error=ToolError(
                        error=str(exc) or "Upgrade required",
                        error_code="upgrade_required",
                        error_details=error_details,
                    ),
                    upgrade_hints=(
                        [
                            UpgradeHint(
                                feature=exc.feature,
                                tier=exc.required_tier,
                                reason=f"Unavailable at {tier.title()} tier",
                            )
                        ]
                        if "upgrade_hints" in envelope_fields
                        else None
                    ),
                    warnings=gov_warnings if gov_warnings else [],
                    data=None,
                )
                return envelope.model_dump(mode="json", exclude_none=True)

            # [20251228_BUGFIX] Avoid leaking stack traces/details to clients.
            # Only include raw exception message for non-internal error codes.
            if code == "internal_error":
                # Default to a generic message; allow opt-in detail in debug mode.
                safe_message = "Tool error"
                if os.environ.get("SCALPEL_MCP_INFO", "").upper() == "DEBUG":
                    safe_message = str(exc) or safe_message
            else:
                safe_message = str(exc) or "Tool error"

            envelope = ToolResponseEnvelope(
                tier=tier if "tier" in envelope_fields else None,
                tool_version=__version__ if "tool_version" in envelope_fields else None,
                tool_id=tool.name if "tool_id" in envelope_fields else None,
                request_id=request_id if "request_id" in envelope_fields else None,
                capabilities=(
                    ["envelope-v1"] if "capabilities" in envelope_fields else None
                ),
                duration_ms=duration_ms if "duration_ms" in envelope_fields else None,
                error=ToolError(error=safe_message, error_code=code),
                upgrade_hints=None,  # No hints on generic errors
                warnings=gov_warnings if gov_warnings else [],
                data=None,
            )
            return envelope.model_dump(mode="json", exclude_none=True)

    # Replace the tool's run method (Tool is a Pydantic model, use object.__setattr__)
    object.__setattr__(tool, "run", _enveloped_run)

    return tool


mcp._tool_manager.add_tool = _add_tool_with_envelope_output  # type: ignore[method-assign]


def _validate_code(code: str) -> tuple[bool, str | None]:
    """Validate code before analysis."""
    if not code:
        return False, "Code cannot be empty"
    if not isinstance(code, str):
        return False, "Code must be a string"
    if len(code) > MAX_CODE_SIZE:
        return False, f"Code exceeds maximum size of {MAX_CODE_SIZE} characters"
    return True, None


def _count_complexity(tree: ast.AST) -> int:
    """Estimate cyclomatic complexity."""
    complexity = 1
    for node in ast.walk(tree):
        if isinstance(node, (ast.If, ast.While, ast.For, ast.ExceptHandler)):
            complexity += 1
        elif isinstance(node, ast.BoolOp) and isinstance(node.op, (ast.And, ast.Or)):
            complexity += len(node.values) - 1
    return complexity


def _calculate_cognitive_complexity_python(tree: ast.AST) -> int:
    """
    Calculate cognitive complexity for Python code.

    [20251229_FEATURE] v3.3.0 - Implements Sonar cognitive complexity metric.

    Cognitive complexity measures code understandability by penalizing:
    - Nested control structures (heavier weight)
    - Control flow breaks (continue, break, return)
    - Recursive calls

    Returns:
        int: Cognitive complexity score
    """
    complexity = 0

    def visit_node(node: ast.AST, parent_nesting: int) -> None:
        nonlocal complexity
        local_complexity = 0
        current_nesting = parent_nesting

        # Increment for control structures
        if isinstance(node, (ast.If, ast.While, ast.For, ast.ExceptHandler)):
            local_complexity = 1 + current_nesting
            current_nesting += 1
        elif isinstance(node, ast.BoolOp):
            # Logical operators add complexity
            local_complexity = len(node.values) - 1
        elif isinstance(node, (ast.Break, ast.Continue, ast.Return)):
            # Control flow breaks
            local_complexity = 1
        elif isinstance(node, ast.Lambda):
            # Lambdas increase nesting
            current_nesting += 1

        complexity += local_complexity

        # Recursively visit children
        for child in ast.iter_child_nodes(node):
            visit_node(child, current_nesting)

    visit_node(tree, 0)
    return complexity


def _detect_code_smells_python(tree: ast.AST, code: str) -> list[str]:
    """
    Detect code smells in Python code.

    [20251229_FEATURE] v3.3.0 - Code smell detection for PRO tier.

    Detects:
    - Long methods (>50 lines)
    - God classes (>10 methods)
    - Too many parameters (>5 parameters)
    - Deep nesting (>4 levels)

    Returns:
        list[str]: List of code smell descriptions
    """
    smells = []
    code.splitlines()

    def _max_nesting(n: ast.AST, depth: int = 0) -> int:
        depths = [depth]
        for child in ast.iter_child_nodes(n):
            if isinstance(
                child,
                (
                    ast.If,
                    ast.For,
                    ast.While,
                    ast.Try,
                    ast.With,
                    ast.AsyncWith,
                    ast.FunctionDef,
                    ast.AsyncFunctionDef,
                ),
            ):
                depths.append(_max_nesting(child, depth + 1))
            else:
                depths.append(_max_nesting(child, depth))
        return max(depths) if depths else depth

    for node in ast.walk(tree):
        # Long method detection
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            if hasattr(node, "end_lineno") and node.end_lineno and node.lineno:
                method_length = node.end_lineno - node.lineno
                if method_length > 50:
                    smells.append(
                        f"Long method '{node.name}' ({method_length} lines) "
                        f"at line {node.lineno}. Consider breaking into smaller functions."
                    )

            # Too many parameters
            if len(node.args.args) > 5:
                smells.append(
                    f"Method '{node.name}' has {len(node.args.args)} parameters "
                    f"at line {node.lineno}. Consider using parameter objects."
                )

            # Deep nesting and high complexity hints
            depth = _max_nesting(node, 0)
            if depth > 4:
                smells.append(
                    f"Method '{node.name}' has deep nesting (>{depth} levels) at line {node.lineno}."
                )
            try:
                func_tree = ast.Module(body=node.body, type_ignores=[])
                func_complexity = _count_complexity(func_tree)
                if func_complexity > 10:
                    smells.append(
                        f"Method '{node.name}' has high cyclomatic complexity ({func_complexity}) at line {node.lineno}."
                    )
            except Exception:
                pass

        # God class detection
        elif isinstance(node, ast.ClassDef):
            methods = [
                n
                for n in node.body
                if isinstance(n, (ast.FunctionDef, ast.AsyncFunctionDef))
            ]
            if len(methods) > 10:
                smells.append(
                    f"God class '{node.name}' with {len(methods)} methods "
                    f"at line {node.lineno}. Consider splitting responsibilities."
                )

    return smells


# [20251225_FEATURE] Tier-gated advanced metrics and detections for analyze_code
def _compute_halstead_metrics_python(tree: ast.AST) -> dict[str, float]:
    """Compute Halstead metrics for Python code using AST traversal."""
    operators: list[str] = []
    operands: list[str] = []

    def op_name(node: ast.AST) -> str:
        return type(node).__name__

    for node in ast.walk(tree):
        if isinstance(node, (ast.BinOp, ast.BoolOp, ast.UnaryOp, ast.Compare)):
            operators.append(op_name(node))
        elif isinstance(node, (ast.Assign, ast.AugAssign, ast.AnnAssign)):
            operators.append(op_name(node))
        elif isinstance(node, ast.Call):
            operators.append(op_name(node))
        elif isinstance(node, ast.IfExp):
            operators.append(op_name(node))

        if isinstance(node, ast.Name):
            operands.append(node.id)
        elif isinstance(node, ast.Constant):
            operands.append(repr(node.value))

    distinct_operators = len(set(operators))
    distinct_operands = len(set(operands))
    total_operators = len(operators)
    total_operands = len(operands)

    vocabulary = distinct_operators + distinct_operands
    length = total_operators + total_operands
    volume = length * math.log2(vocabulary) if vocabulary > 0 else 0.0
    difficulty = (
        (distinct_operators / 2) * (total_operands / distinct_operands)
        if distinct_operands > 0
        else 0.0
    )
    effort = difficulty * volume

    return {
        "n1": float(distinct_operators),
        "n2": float(distinct_operands),
        "N1": float(total_operators),
        "N2": float(total_operands),
        "vocabulary": float(vocabulary),
        "length": float(length),
        "volume": float(volume),
        "difficulty": float(difficulty),
        "effort": float(effort),
    }


def _detect_duplicate_code_blocks(
    code: str, min_lines: int = 5
) -> list[dict[str, Any]]:
    """Detect duplicate code blocks using line-hash sliding windows."""
    lines = [
        ln.strip()
        for ln in code.splitlines()
        if ln.strip() and not ln.strip().startswith("#")
    ]
    if len(lines) < min_lines:
        return []

    window_map: dict[str, list[int]] = {}
    for i in range(len(lines) - min_lines + 1):
        window = "\n".join(lines[i : i + min_lines])
        block_hash = hash(window)
        window_map.setdefault(str(block_hash), []).append(i + 1)

    duplicates: list[dict[str, Any]] = []
    for block_hash, occurrences in window_map.items():
        if len(occurrences) > 1:
            duplicates.append({"hash": block_hash, "occurrences": occurrences})
    return duplicates


def _build_dependency_graph_python(tree: ast.AST) -> dict[str, list[str]]:
    """Build a lightweight intra-module call graph for Python code."""
    graph: dict[str, list[str]] = {}

    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            caller = node.name
            callees: set[str] = set()
            for child in ast.walk(node):
                if isinstance(child, ast.Call):
                    func = child.func
                    if isinstance(func, ast.Name):
                        callees.add(func.id)
                    elif isinstance(func, ast.Attribute):
                        callees.add(func.attr)
            graph[caller] = sorted(callees)
    return graph


def _detect_naming_issues_python(tree: ast.AST) -> list[str]:
    """Detect simple naming convention issues (snake_case for functions, PascalCase for classes)."""
    issues: list[str] = []
    snake = re.compile(r"^[a-z_][a-z0-9_]*$")
    pascal = re.compile(r"^[A-Z][A-Za-z0-9]*$")

    for node in ast.walk(tree):
        if isinstance(
            node, (ast.FunctionDef, ast.AsyncFunctionDef)
        ) and not snake.match(node.name):
            issues.append(
                f"Function '{node.name}' should be snake_case at line {node.lineno}."
            )
        if isinstance(node, ast.ClassDef) and not pascal.match(node.name):
            issues.append(
                f"Class '{node.name}' should be PascalCase at line {node.lineno}."
            )
    return issues


def _apply_custom_rules_python(code: str) -> list[dict[str, Any]]:
    """Apply a small built-in custom rule set for Enterprise tier."""
    rules = {
        "CR-001": (re.compile(r"\beval\("), "Avoid eval for safety"),
        "CR-002": (re.compile(r"\bexec\("), "Avoid exec for safety"),
        "CR-003": (re.compile(r"\bprint\("), "Prefer structured logging over print"),
    }
    findings: list[dict[str, Any]] = []
    for idx, line in enumerate(code.splitlines(), 1):
        for rule, (pattern, message) in rules.items():
            if pattern.search(line):
                findings.append(
                    {
                        "rule": rule,
                        "line": idx,
                        "detail": line.strip(),
                        "message": message,
                    }
                )
    return findings


def _detect_compliance_issues_python(tree: ast.AST, code: str) -> list[str]:
    """Detect simple compliance-related patterns (bare except, missing logging)."""
    issues: list[str] = []
    for node in ast.walk(tree):
        if isinstance(node, ast.ExceptHandler) and node.type is None:
            issues.append(
                f"Bare except detected at line {node.lineno}. Specify exception types."
            )
    if "password" in code.lower() and "hashlib" not in code.lower():
        issues.append(
            "Potential plaintext password handling detected. Ensure hashing/encryption."
        )
    return issues


def _detect_organization_patterns_python(tree: ast.AST) -> list[str]:
    """Detect coarse architectural hints from class naming conventions."""
    patterns: list[str] = []
    for node in ast.walk(tree):
        if isinstance(node, ast.ClassDef):
            if node.name.endswith("Controller"):
                patterns.append(f"Controller pattern detected: {node.name}")
            if node.name.endswith("Service"):
                patterns.append(f"Service pattern detected: {node.name}")
            if node.name.endswith("Repository"):
                patterns.append(f"Repository pattern detected: {node.name}")
    return patterns


def _analyze_java_code(code: str) -> AnalysisResult:
    """Analyze Java code using tree-sitter."""
    try:
        from code_scalpel.code_parsers.java_parsers.java_parser_treesitter import (
            JavaParser,
        )

        parser = JavaParser()
        result = parser.parse(code)
        return AnalysisResult(
            success=True,
            functions=result["functions"],
            classes=result["classes"],
            imports=result["imports"],
            complexity=result["complexity"],
            lines_of_code=result["lines_of_code"],
            issues=result["issues"],
        )
    except ImportError:
        return AnalysisResult(
            success=False,
            functions=[],
            classes=[],
            imports=[],
            complexity=0,
            lines_of_code=0,
            error="Java support not available. Please install tree-sitter and tree-sitter-java.",
        )
    except Exception as e:
        return AnalysisResult(
            success=False,
            functions=[],
            classes=[],
            imports=[],
            complexity=0,
            lines_of_code=0,
            error=f"Java analysis failed: {str(e)}.",
        )


def _analyze_javascript_code(code: str, is_typescript: bool = False) -> AnalysisResult:
    """
    Analyze JavaScript/TypeScript code using tree-sitter.

    [20251220_FEATURE] v3.0.4 - Multi-language analyze_code support.
    [20251220_BUGFIX] v3.0.5 - Consolidated tree-sitter imports.
    """
    try:
        from tree_sitter import Language, Parser

        if is_typescript:
            import tree_sitter_typescript as ts_ts

            lang = Language(ts_ts.language_typescript())
        else:
            import tree_sitter_javascript as ts_js

            lang = Language(ts_js.language())

        parser = Parser(lang)
        tree = parser.parse(bytes(code, "utf-8"))

        functions = []
        function_details = []
        classes = []
        class_details = []
        imports = []

        def walk_tree(node, depth=0):
            """Walk tree-sitter tree to extract structure."""
            node_type = node.type

            # Functions (function declarations, arrow functions, methods)
            if node_type in (
                "function_declaration",
                "function",
                "generator_function_declaration",
            ):
                name_node = node.child_by_field_name("name")
                name = name_node.text.decode("utf-8") if name_node else "<anonymous>"
                functions.append(name)
                function_details.append(
                    FunctionInfo(
                        name=name,
                        lineno=node.start_point[0] + 1,
                        end_lineno=node.end_point[0] + 1,
                        is_async=any(c.type == "async" for c in node.children),
                    )
                )

            # Arrow functions with variable declaration
            elif (
                node_type == "lexical_declaration"
                or node_type == "variable_declaration"
            ):
                for child in node.children:
                    if child.type == "variable_declarator":
                        name_node = child.child_by_field_name("name")
                        value_node = child.child_by_field_name("value")
                        if value_node and value_node.type == "arrow_function":
                            name = (
                                name_node.text.decode("utf-8")
                                if name_node
                                else "<anonymous>"
                            )
                            functions.append(name)
                            function_details.append(
                                FunctionInfo(
                                    name=name,
                                    lineno=child.start_point[0] + 1,
                                    end_lineno=child.end_point[0] + 1,
                                    is_async=any(
                                        c.type == "async" for c in value_node.children
                                    ),
                                )
                            )

            # Classes
            elif node_type == "class_declaration":
                name_node = node.child_by_field_name("name")
                name = name_node.text.decode("utf-8") if name_node else "<anonymous>"

                # Extract methods
                methods = []
                body_node = node.child_by_field_name("body")
                if body_node:
                    for member in body_node.children:
                        if member.type == "method_definition":
                            method_name_node = member.child_by_field_name("name")
                            if method_name_node:
                                methods.append(method_name_node.text.decode("utf-8"))

                classes.append(name)
                class_details.append(
                    ClassInfo(
                        name=name,
                        lineno=node.start_point[0] + 1,
                        end_lineno=node.end_point[0] + 1,
                        methods=methods,
                    )
                )

            # Imports (ES6 import statements)
            elif node_type == "import_statement":
                source_node = node.child_by_field_name("source")
                if source_node:
                    module = source_node.text.decode("utf-8").strip("'\"")
                    imports.append(module)

            # CommonJS require
            elif node_type == "call_expression":
                func_node = node.child_by_field_name("function")
                if func_node and func_node.text == b"require":
                    args_node = node.child_by_field_name("arguments")
                    if args_node and args_node.children:
                        for arg in args_node.children:
                            if arg.type == "string":
                                imports.append(arg.text.decode("utf-8").strip("'\""))

            # Recurse into children
            for child in node.children:
                walk_tree(child, depth + 1)

        walk_tree(tree.root_node)

        # Estimate complexity (branches)
        complexity = 1
        for node in _walk_ts_tree(tree.root_node):
            if node.type in (
                "if_statement",
                "while_statement",
                "for_statement",
                "for_in_statement",
                "catch_clause",
                "ternary_expression",
                "switch_case",
            ):
                complexity += 1
            elif node.type == "binary_expression":
                op_node = node.child_by_field_name("operator")
                if op_node and op_node.text in (b"&&", b"||"):
                    complexity += 1

        lang_name = "TypeScript" if is_typescript else "JavaScript"
        return AnalysisResult(
            success=True,
            functions=functions,
            classes=classes,
            imports=imports,
            complexity=complexity,
            lines_of_code=len(code.splitlines()),
            issues=[],
            function_details=function_details,
            class_details=class_details,
        )
    except ImportError as e:
        lang_name = "TypeScript" if is_typescript else "JavaScript"
        return AnalysisResult(
            success=False,
            functions=[],
            classes=[],
            imports=[],
            complexity=0,
            lines_of_code=0,
            error=f"{lang_name} support not available. Please install tree-sitter packages: {str(e)}.",
        )
    except Exception as e:
        lang_name = "TypeScript" if is_typescript else "JavaScript"
        return AnalysisResult(
            success=False,
            functions=[],
            classes=[],
            imports=[],
            complexity=0,
            lines_of_code=0,
            error=f"{lang_name} analysis failed: {str(e)}.",
        )


def _walk_ts_tree(node):
    """Generator to walk all nodes in a tree-sitter tree."""
    yield node
    for child in node.children:
        yield from _walk_ts_tree(child)


# [20251231_FEATURE] v3.3.x - Best-effort enrichments for analyze_code
_ANALYZE_CODE_COMPLEXITY_HISTORY: dict[str, list[dict[str, Any]]] = {}


def _detect_frameworks_from_code(
    code: str,
    language: str,
    imports: list[str] | None = None,
) -> list[str]:
    """Heuristic framework detection for a single code blob.

    This intentionally remains lightweight and non-executing.
    """
    lang = (language or "").lower()
    imports_set = set(imports or [])
    code_lower = code.lower()
    frameworks: set[str] = set()

    # Python web frameworks
    if any(i.startswith("django") for i in imports_set) or "django" in code_lower:
        frameworks.add("django")
    if any(i.startswith("flask") for i in imports_set) or "flask" in code_lower:
        frameworks.add("flask")
    if any(i.startswith("fastapi") for i in imports_set) or "fastapi" in code_lower:
        frameworks.add("fastapi")

    # Java / Spring
    if (
        lang == "java"
        or "org.springframework" in code_lower
        or "@component" in code_lower
    ):
        if (
            "springframework" in code_lower
            or "@autowired" in code_lower
            or "@component" in code_lower
        ):
            frameworks.add("spring")

    # React / Next.js style patterns (JS/TS)
    if lang in {"javascript", "typescript"} or "tsx" in lang:
        if (
            "from 'react'" in code_lower
            or 'from "react"' in code_lower
            or "react" in code_lower
        ):
            if (
                "usestate(" in code_lower
                or "useeffect(" in code_lower
                or "usecontext(" in code_lower
            ):
                frameworks.add("react")
            # Even without hooks, a React import is a strong signal.
            elif "react" in code_lower:
                frameworks.add("react")

    return sorted(frameworks)


def _detect_dead_code_hints_python(tree: ast.AST, code: str) -> list[str]:
    """Best-effort dead code hints for Python.

    This is intentionally conservative: it flags obvious unreachable statements
    and unused imports in the single file.
    """
    hints: list[str] = []
    try:
        used_names: set[str] = set()
        imported_names: list[tuple[str, int]] = []

        for node in ast.walk(tree):
            if isinstance(node, ast.Name):
                used_names.add(node.id)
            elif isinstance(node, ast.Import):
                for alias in node.names:
                    name = alias.asname or alias.name.split(".")[0]
                    imported_names.append((name, getattr(node, "lineno", 0) or 0))
            elif isinstance(node, ast.ImportFrom):
                for alias in node.names:
                    name = alias.asname or alias.name
                    imported_names.append((name, getattr(node, "lineno", 0) or 0))

        for name, lineno in imported_names:
            if name and name not in used_names:
                hints.append(f"Unused import '{name}' (L{lineno})")

        def _scan_block_for_unreachable(stmts: list[ast.stmt], scope: str) -> None:
            terminated = False
            for st in stmts:
                if terminated:
                    ln = getattr(st, "lineno", None)
                    hints.append(
                        f"Unreachable statement after terminator in {scope} (L{ln})"
                    )
                    continue
                if isinstance(st, (ast.Return, ast.Raise)):  # simple terminators
                    terminated = True
                # Recurse into nested blocks for basic coverage
                if isinstance(st, ast.If):
                    _scan_block_for_unreachable(st.body or [], f"{scope} (if-body)")
                    _scan_block_for_unreachable(st.orelse or [], f"{scope} (if-else)")
                elif isinstance(st, (ast.For, ast.While, ast.With, ast.Try)):
                    _scan_block_for_unreachable(
                        getattr(st, "body", []) or [], f"{scope} (loop/with/try)"
                    )
                    _scan_block_for_unreachable(
                        getattr(st, "orelse", []) or [], f"{scope} (orelse)"
                    )
                    _scan_block_for_unreachable(
                        getattr(st, "finalbody", []) or [], f"{scope} (finally)"
                    )
                    for h in getattr(st, "handlers", []) or []:
                        _scan_block_for_unreachable(
                            getattr(h, "body", []) or [], f"{scope} (except)"
                        )

        for node in tree.body if isinstance(tree, ast.Module) else []:
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                _scan_block_for_unreachable(node.body or [], f"function '{node.name}'")
            elif isinstance(node, ast.ClassDef):
                for inner in node.body:
                    if isinstance(inner, (ast.FunctionDef, ast.AsyncFunctionDef)):
                        _scan_block_for_unreachable(
                            inner.body or [], f"method '{node.name}.{inner.name}'"
                        )

    except Exception:
        return hints

    # Deduplicate while keeping stable order
    seen: set[str] = set()
    out: list[str] = []
    for h in hints:
        if h not in seen:
            seen.add(h)
            out.append(h)
    return out


def _summarize_decorators_python(tree: ast.AST) -> dict[str, Any]:
    decorators: set[str] = set()

    def _decorator_name(d: ast.AST) -> str:
        if isinstance(d, ast.Name):
            return d.id
        if isinstance(d, ast.Attribute):
            # best-effort flatten
            parts: list[str] = []
            cur: ast.AST | None = d
            while isinstance(cur, ast.Attribute):
                parts.append(cur.attr)
                cur = cur.value
            if isinstance(cur, ast.Name):
                parts.append(cur.id)
            return ".".join(reversed(parts))
        if isinstance(d, ast.Call):
            return _decorator_name(d.func)
        return d.__class__.__name__

    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
            for d in getattr(node, "decorator_list", []) or []:
                decorators.add(_decorator_name(d))

    return {"decorators": sorted(decorators), "decorator_count": len(decorators)}


def _summarize_types_python(tree: ast.AST) -> dict[str, Any]:
    total_funcs = 0
    funcs_with_any_annotations = 0
    annotated_params = 0
    annotated_returns = 0
    generic_like_uses = 0

    generic_names = {
        "list",
        "dict",
        "set",
        "tuple",
        "optional",
        "union",
        "sequence",
        "mapping",
        "iterable",
        "type",
        "callable",
        "generic",
    }

    def _subscript_head(n: ast.AST) -> str | None:
        if isinstance(n, ast.Subscript):
            v = n.value
            if isinstance(v, ast.Name):
                return v.id
            if isinstance(v, ast.Attribute):
                return v.attr
        return None

    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            total_funcs += 1
            has_ann = False
            for a in list(getattr(node.args, "args", []) or []) + list(
                getattr(node.args, "kwonlyargs", []) or []
            ):
                if getattr(a, "annotation", None) is not None:
                    annotated_params += 1
                    has_ann = True
            if (
                getattr(node.args, "vararg", None) is not None
                and getattr(node.args.vararg, "annotation", None) is not None
            ):
                annotated_params += 1
                has_ann = True
            if (
                getattr(node.args, "kwarg", None) is not None
                and getattr(node.args.kwarg, "annotation", None) is not None
            ):
                annotated_params += 1
                has_ann = True
            if getattr(node, "returns", None) is not None:
                annotated_returns += 1
                has_ann = True
            if has_ann:
                funcs_with_any_annotations += 1

        head = _subscript_head(node)
        if head and head.lower() in generic_names:
            generic_like_uses += 1

    return {
        "functions_total": total_funcs,
        "functions_with_any_annotations": funcs_with_any_annotations,
        "annotated_params": annotated_params,
        "annotated_returns": annotated_returns,
        "generic_type_uses": generic_like_uses,
    }


def _compute_api_surface_from_symbols(
    functions: list[str], classes: list[str]
) -> dict[str, Any]:
    def _is_public(name: str) -> bool:
        # Treat async prefix as implementation detail in inventory
        norm = name.replace("async ", "")
        return bool(norm) and not norm.startswith("_")

    public_functions = sorted({f for f in functions if _is_public(f)})
    public_classes = sorted({c for c in classes if _is_public(c)})
    return {
        "public_functions": public_functions,
        "public_classes": public_classes,
        "public_function_count": len(public_functions),
        "public_class_count": len(public_classes),
    }


def _priority_sort(items: list[Any]) -> list[Any]:
    """Sort a list of issue/smell labels by severity keywords.

    If items are not strings, return them unchanged to avoid type errors.
    """

    if not items or not all(isinstance(s, str) for s in items):
        return list(items)

    def _rank(s: str) -> tuple[int, str]:
        low = s.lower()
        if "critical" in low:
            return (0, s)
        if "high" in low:
            return (1, s)
        if "medium" in low:
            return (2, s)
        if "low" in low:
            return (3, s)
        return (4, s)

    return sorted(items, key=_rank)


def _update_and_get_complexity_trends(
    *,
    file_path: str | None,
    cyclomatic: int,
    cognitive: int,
    max_points: int = 50,
) -> dict[str, Any] | None:
    if not file_path:
        return None
    key = str(file_path)
    history = _ANALYZE_CODE_COMPLEXITY_HISTORY.setdefault(key, [])
    history.append(
        {"cyclomatic": cyclomatic, "cognitive": cognitive, "ts": time.time()}
    )
    if len(history) > max_points:
        history[:] = history[-max_points:]

    if len(history) < 2:
        delta_cyclomatic = 0
        delta_cognitive = 0
    else:
        delta_cyclomatic = history[-1]["cyclomatic"] - history[0]["cyclomatic"]
        delta_cognitive = history[-1]["cognitive"] - history[0]["cognitive"]

    return {
        "file_path": key,
        "samples": history[-10:],
        "sample_count": len(history),
        "delta_cyclomatic": delta_cyclomatic,
        "delta_cognitive": delta_cognitive,
    }


def _analyze_code_sync(
    code: str, language: str = "auto", file_path: str | None = None
) -> AnalysisResult:
    """Synchronous implementation of analyze_code.

    [20251219_BUGFIX] v3.0.4 - Auto-detect language from content if not specified.
    [20251219_BUGFIX] v3.0.4 - Strip UTF-8 BOM if present.
    [20251220_FEATURE] v3.0.4 - Multi-language support for JavaScript/TypeScript.
    [20251221_FEATURE] v3.1.0 - Use unified_extractor for language detection.
    [20251229_FEATURE] v3.3.0 - Tier-based feature gating for advanced metrics.

    Tier Capabilities:
        COMMUNITY: Basic AST parsing, function/class inventory, cyclomatic complexity
        PRO: + Cognitive complexity, code smell detection
        ENTERPRISE: + Custom rules, compliance checks, organization patterns
    """
    # [20251229_FEATURE] v3.3.0 - Detect tier and get capabilities
    tier = get_current_tier_from_license()
    capabilities = get_tool_capabilities("analyze_code", tier)
    logger.debug(
        f"analyze_code running with tier={tier.title()}, capabilities={capabilities.get('capabilities', set())}"
    )

    # [20251219_BUGFIX] Strip UTF-8 BOM if present
    if code.startswith("\ufeff"):
        code = code[1:]

    valid, error = _validate_code(code)
    if not valid:
        return AnalysisResult(
            success=False,
            functions=[],
            classes=[],
            imports=[],
            complexity=0,
            lines_of_code=0,
            error=error,
        )

    # [20251221_FEATURE] v3.1.0 - Use unified_extractor for language detection
    if language == "auto" or language is None:
        # [20251228_BUGFIX] Avoid deprecated shim imports.
        from code_scalpel.surgery.unified_extractor import Language, detect_language

        detected = detect_language(None, code)
        lang_map = {
            Language.PYTHON: "python",
            Language.JAVASCRIPT: "javascript",
            Language.TYPESCRIPT: "typescript",
            Language.JAVA: "java",
        }
        language = lang_map.get(detected, "python")

    # [20260110_FEATURE] v1.0 - Explicit language validation
    SUPPORTED_LANGUAGES = {"python", "javascript", "typescript", "java"}
    if language.lower() not in SUPPORTED_LANGUAGES:
        return AnalysisResult(
            success=False,
            functions=[],
            classes=[],
            imports=[],
            complexity=0,
            lines_of_code=0,
            error=f"Unsupported language '{language}'. Supported: {', '.join(sorted(SUPPORTED_LANGUAGES))}. Roadmap: Go/Rust in Q1 2026.",
        )

    # Check cache first
    cache = _get_cache()
    cache_config = {
        "language": language,
        "tier": tier,
    }  # [20251229_FEATURE] v3.3.0 - Include tier in cache key
    if cache:
        cached = cache.get(code, "analysis", cache_config)
        if cached is not None:
            logger.debug("Cache hit for analyze_code")
            # Convert dict back to AnalysisResult if needed
            if isinstance(cached, dict):
                # [20251228_BUGFIX] Backward-compatibility for cached entries
                # from token-saving schemas where `success` was omitted/None.
                if cached.get("success") is None:
                    cached["success"] = True
                return AnalysisResult(**cached)
            return cached

    if language.lower() == "java":
        result = _analyze_java_code(code)
        if result.success:
            # [20260110_FEATURE] Populate metadata fields
            result.language_detected = "java"
            result.tier_applied = tier

            if has_capability("analyze_code", "framework_detection", tier):
                result.frameworks = _detect_frameworks_from_code(
                    code, "java", result.imports
                )

            if has_capability("analyze_code", "api_surface_analysis", tier):
                result.api_surface = _compute_api_surface_from_symbols(
                    result.functions, result.classes
                )

            if has_capability("analyze_code", "priority_ordering", tier):
                result.issues = _priority_sort(result.issues)
                result.code_smells = _priority_sort(result.code_smells)
                result.dead_code_hints = _priority_sort(result.dead_code_hints)
                result.prioritized = True

            if has_capability("analyze_code", "complexity_trends", tier):
                result.complexity_trends = _update_and_get_complexity_trends(
                    file_path=file_path,
                    cyclomatic=result.complexity,
                    cognitive=getattr(result, "cognitive_complexity", 0) or 0,
                )
        if cache and result.success:
            cache.set(code, "analysis", result.model_dump(), cache_config)
        return result

    # [20251220_FEATURE] v3.0.4 - Route JavaScript/TypeScript to tree-sitter analyzer
    if language.lower() == "javascript":
        result = _analyze_javascript_code(code, is_typescript=False)
        if result.success:
            # [20260110_FEATURE] Populate metadata fields
            result.language_detected = "javascript"
            result.tier_applied = tier

            if has_capability("analyze_code", "framework_detection", tier):
                result.frameworks = _detect_frameworks_from_code(
                    code, "javascript", result.imports
                )

            if has_capability("analyze_code", "api_surface_analysis", tier):
                result.api_surface = _compute_api_surface_from_symbols(
                    result.functions, result.classes
                )

            if has_capability("analyze_code", "priority_ordering", tier):
                result.issues = _priority_sort(result.issues)
                result.code_smells = _priority_sort(result.code_smells)
                result.dead_code_hints = _priority_sort(result.dead_code_hints)
                result.prioritized = True

            if has_capability("analyze_code", "complexity_trends", tier):
                result.complexity_trends = _update_and_get_complexity_trends(
                    file_path=file_path,
                    cyclomatic=result.complexity,
                    cognitive=getattr(result, "cognitive_complexity", 0) or 0,
                )
        if cache and result.success:
            cache.set(code, "analysis", result.model_dump(), cache_config)
        return result

    if language.lower() == "typescript":
        result = _analyze_javascript_code(code, is_typescript=True)
        if result.success:
            # [20260110_FEATURE] Populate metadata fields
            result.language_detected = "typescript"
            result.tier_applied = tier

            if has_capability("analyze_code", "framework_detection", tier):
                result.frameworks = _detect_frameworks_from_code(
                    code, "typescript", result.imports
                )

            if has_capability("analyze_code", "api_surface_analysis", tier):
                result.api_surface = _compute_api_surface_from_symbols(
                    result.functions, result.classes
                )

            if has_capability("analyze_code", "priority_ordering", tier):
                result.issues = _priority_sort(result.issues)
                result.code_smells = _priority_sort(result.code_smells)
                result.dead_code_hints = _priority_sort(result.dead_code_hints)
                result.prioritized = True

            if has_capability("analyze_code", "complexity_trends", tier):
                result.complexity_trends = _update_and_get_complexity_trends(
                    file_path=file_path,
                    cyclomatic=result.complexity,
                    cognitive=getattr(result, "cognitive_complexity", 0) or 0,
                )
        if cache and result.success:
            cache.set(code, "analysis", result.model_dump(), cache_config)
        return result

    # Python analysis using ast module
    try:
        tree = ast.parse(code)

        functions = []
        function_details = []
        classes = []
        class_details = []
        imports = []
        issues = []

        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                functions.append(node.name)
                function_details.append(
                    FunctionInfo(
                        name=node.name,
                        lineno=node.lineno,
                        end_lineno=getattr(node, "end_lineno", None),
                        is_async=False,
                    )
                )
                # Flag potential issues
                if len(node.name) < 2:
                    issues.append(f"Function '{node.name}' has very short name")
            elif isinstance(node, ast.AsyncFunctionDef):
                functions.append(f"async {node.name}")
                function_details.append(
                    FunctionInfo(
                        name=node.name,
                        lineno=node.lineno,
                        end_lineno=getattr(node, "end_lineno", None),
                        is_async=True,
                    )
                )
            elif isinstance(node, ast.ClassDef):
                classes.append(node.name)
                # Extract method names
                methods = [
                    n.name
                    for n in node.body
                    if isinstance(n, (ast.FunctionDef, ast.AsyncFunctionDef))
                ]
                class_details.append(
                    ClassInfo(
                        name=node.name,
                        lineno=node.lineno,
                        end_lineno=getattr(node, "end_lineno", None),
                        methods=methods,
                    )
                )
            elif isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append(alias.name)
            elif isinstance(node, ast.ImportFrom):
                module = node.module or ""
                for alias in node.names:
                    imports.append(f"{module}.{alias.name}")

        # [20251229_FEATURE] v3.3.0 - Compute tier-based advanced metrics
        cognitive_complexity = 0
        code_smells = []
        halstead_metrics: dict[str, float] | None = None
        duplicate_code_blocks: list[dict[str, Any]] = []
        dependency_graph: dict[str, list[str]] = {}
        naming_issues: list[str] = []
        compliance_issues: list[str] = []
        custom_rule_violations: list[dict[str, Any]] = []
        organization_patterns: list[str] = []

        # [20251231_FEATURE] Additional tier-gated enrichments
        frameworks: list[str] = []
        dead_code_hints: list[str] = []
        decorator_summary: dict[str, Any] | None = None
        type_summary: dict[str, Any] | None = None
        architecture_patterns: list[str] = []
        technical_debt: dict[str, Any] | None = None
        api_surface: dict[str, Any] | None = None
        prioritized: bool = False
        complexity_trends: dict[str, Any] | None = None

        # [20251228_BUGFIX] Complexity metrics now available at COMMUNITY tier
        # COMMUNITY: Basic cyclomatic complexity
        # PRO: Add cognitive complexity, code smells, halstead metrics
        # ENTERPRISE: Add duplicate detection, dependency graph
        if has_capability("analyze_code", "complexity_metrics", tier):
            # Basic cyclomatic complexity available at Community
            cyclomatic = _count_complexity(tree)
            logger.debug(f"Computed cyclomatic complexity: {cyclomatic}")

            # PRO tier: Cognitive complexity
            if has_capability("analyze_code", "cognitive_complexity", tier):
                cognitive_complexity = _calculate_cognitive_complexity_python(tree)
                logger.debug(f"Computed cognitive complexity: {cognitive_complexity}")

            # PRO tier: Code smell detection
            if has_capability("analyze_code", "code_smells", tier):
                code_smells = _detect_code_smells_python(tree, code)
                logger.debug(f"Detected {len(code_smells)} code smells")

            if has_capability("analyze_code", "halstead_metrics", tier):
                halstead_metrics = _compute_halstead_metrics_python(tree)
                logger.debug("Computed Halstead metrics")

            if has_capability("analyze_code", "duplicate_code_detection", tier):
                duplicate_code_blocks = _detect_duplicate_code_blocks(code)
                logger.debug(
                    f"Detected {len(duplicate_code_blocks)} duplicate code block(s)"
                )

            if has_capability("analyze_code", "dependency_graph", tier):
                dependency_graph = _build_dependency_graph_python(tree)
                logger.debug("Built dependency graph")

        if has_capability("analyze_code", "naming_conventions", tier):
            naming_issues = _detect_naming_issues_python(tree)

        if has_capability("analyze_code", "custom_rules", tier):
            custom_rule_violations = _apply_custom_rules_python(code)

        if has_capability("analyze_code", "compliance_checks", tier):
            compliance_issues = _detect_compliance_issues_python(tree, code)

        if has_capability("analyze_code", "organization_patterns", tier):
            organization_patterns = _detect_organization_patterns_python(tree)

        if has_capability("analyze_code", "framework_detection", tier):
            frameworks = _detect_frameworks_from_code(code, "python", imports)

        if has_capability("analyze_code", "dead_code_detection", tier):
            dead_code_hints = _detect_dead_code_hints_python(tree, code)

        if has_capability("analyze_code", "decorator_analysis", tier):
            decorator_summary = _summarize_decorators_python(tree)
            type_summary = _summarize_types_python(tree)

        if has_capability("analyze_code", "architecture_patterns", tier):
            # Reuse org pattern detector output as a baseline.
            architecture_patterns = list(organization_patterns)
            for fw in frameworks:
                architecture_patterns.append(f"framework:{fw}")
            architecture_patterns = sorted({p for p in architecture_patterns if p})

        if has_capability("analyze_code", "technical_debt_scoring", tier):
            try:
                from code_scalpel.code_parsers.python_parsers.python_parsers_code_quality import (
                    PythonCodeQualityAnalyzer,
                )

                analyzer = PythonCodeQualityAnalyzer()
                report = analyzer.analyze_string(code, filename=file_path or "<string>")
                technical_debt = {
                    "technical_debt_hours": float(
                        getattr(
                            getattr(report, "technical_debt", None), "total_hours", 0
                        )
                        or 0
                    ),
                    "maintainability_index": float(
                        getattr(
                            getattr(report, "maintainability", None),
                            "maintainability_index",
                            0,
                        )
                        or 0
                    ),
                    "smell_count": int(getattr(report, "smell_count", 0) or 0),
                }
            except Exception as e:
                technical_debt = {"error": f"Technical debt scoring failed: {e}"}

        if has_capability("analyze_code", "api_surface_analysis", tier):
            api_surface = _compute_api_surface_from_symbols(functions, classes)

        if has_capability("analyze_code", "priority_ordering", tier):
            issues = _priority_sort(issues)
            code_smells = _priority_sort(code_smells)
            dead_code_hints = _priority_sort(dead_code_hints)
            prioritized = True

        if has_capability("analyze_code", "complexity_trends", tier):
            complexity_trends = _update_and_get_complexity_trends(
                file_path=file_path,
                cyclomatic=_count_complexity(tree),
                cognitive=cognitive_complexity,
            )

        result = AnalysisResult(
            success=True,
            functions=functions,
            classes=classes,
            imports=imports,
            complexity=_count_complexity(tree),
            lines_of_code=len(code.splitlines()),
            issues=issues,
            function_details=function_details,
            class_details=class_details,
            cognitive_complexity=cognitive_complexity,
            code_smells=code_smells,
            halstead_metrics=halstead_metrics,
            duplicate_code_blocks=duplicate_code_blocks,
            dependency_graph=dependency_graph,
            naming_issues=naming_issues,
            compliance_issues=compliance_issues,
            custom_rule_violations=custom_rule_violations,
            organization_patterns=organization_patterns,
            frameworks=frameworks,
            dead_code_hints=dead_code_hints,
            decorator_summary=decorator_summary,
            type_summary=type_summary,
            architecture_patterns=architecture_patterns,
            technical_debt=technical_debt,
            api_surface=api_surface,
            prioritized=prioritized,
            complexity_trends=complexity_trends,
            # [20260110_FEATURE] v1.0 - Metadata fields
            language_detected="python",
            tier_applied=tier,
        )

        # Cache successful result
        if cache:
            cache.set(code, "analysis", result.model_dump(), cache_config)

        return result

    except SyntaxError as e:
        return AnalysisResult(
            success=False,
            functions=[],
            classes=[],
            imports=[],
            complexity=0,
            lines_of_code=0,
            error=f"Syntax error at line {e.lineno}: {e.msg}. Please check your code syntax.",
        )
    except Exception as e:
        return AnalysisResult(
            success=False,
            functions=[],
            classes=[],
            imports=[],
            complexity=0,
            lines_of_code=0,
            error=f"Analysis failed: {str(e)}",
        )


@mcp.tool()
async def analyze_code(
    code: str, language: str = "auto", file_path: str | None = None
) -> AnalysisResult:
    """
    Analyze source code structure.

    Use this tool to understand the high-level architecture (classes, functions, imports)
    of a file before attempting to edit it. This helps prevent hallucinating non-existent
    methods or classes.

    [20251219_BUGFIX] v3.0.4 - Now auto-detects language from code content.

    Example::

        result = await analyze_code('''
        import math

        class Calculator:
            def add(self, a: int, b: int) -> int:
                return a + b

        def helper(x):
            if x > 10:
                return x * 2
            return x
        ''')

        # Returns AnalysisResult:
        # - functions: ["helper"]
        # - classes: ["Calculator"]
        # - imports: ["math"]
        # - complexity_score: 2 (one branch in helper)
        # - has_main: False
        # - lines_of_code: 10

    Args:
        code: Source code to analyze
        language: Language of the code ("auto", "python", "javascript", "typescript", "java")
                  Default "auto" detects from code content.

    Returns:
        AnalysisResult with functions, classes, imports, complexity_score, and metrics
    """
    return await asyncio.to_thread(_analyze_code_sync, code, language, file_path)


def _security_scan_sync(
    code: Optional[str] = None,
    file_path: Optional[str] = None,
    tier: str | None = None,
    capabilities: dict | None = None,
) -> SecurityResult:
    """
    Synchronous implementation of security_scan.

    [20251214_FEATURE] v2.0.0 - Added file_path parameter support.
    [20251220_FEATURE] v3.0.4 - Multi-language support via UnifiedSinkDetector
    [20251226_FEATURE] v3.3.0 - Tier-aware limits and enrichments (sanitizers, policies)
    """

    tier = tier or _get_current_tier()
    caps = capabilities or get_tool_capabilities("security_scan", tier)
    caps_set = set(caps.get("capabilities", set()) or [])
    limits = caps.get("limits", {}) or {}

    detected_language = "python"  # Default to Python

    def _basic_scan_patterns(
        code_str: str,
    ) -> tuple[list[VulnerabilityInfo], list[str]]:
        """[20251226_FEATURE] Minimal AST-free scan for ImportError fallback."""
        patterns = [
            (
                "execute(",
                "SQL Injection",
                "CWE-89",
                "Possible SQL injection via execute()",
            ),
            (
                "cursor.execute",
                "SQL Injection",
                "CWE-89",
                "SQL query execution detected",
            ),
            ("os.system(", "Command Injection", "CWE-78", "os.system() call detected"),
            (
                "subprocess.call(",
                "Command Injection",
                "CWE-78",
                "subprocess.call() detected",
            ),
            ("eval(", "Code Injection", "CWE-94", "eval() call detected"),
            ("exec(", "Code Injection", "CWE-94", "exec() call detected"),
            ("render_template_string(", "XSS", "CWE-79", "Template injection risk"),
        ]
        vulnerabilities: list[VulnerabilityInfo] = []
        taint_sources: list[str] = []
        for line_num, line in enumerate(code_str.splitlines(), 1):
            for pattern, vuln_type, cwe, desc in patterns:
                if pattern in line:
                    vulnerabilities.append(
                        VulnerabilityInfo(
                            type=vuln_type,
                            cwe=cwe,
                            severity="high" if "Injection" in vuln_type else "medium",
                            line=line_num,
                            description=desc,
                        )
                    )

        for source_pattern in ["request.args", "request.form", "input(", "sys.argv"]:
            if source_pattern in code_str:
                taint_sources.append(source_pattern)

        return vulnerabilities, taint_sources

    def _detect_sanitizers(code_str: str) -> list[str]:
        patterns = [r"\bsanitize\w*\s*\(", r"html\.escape\s*\(", r"\bescape\w*\s*\("]
        hits: list[str] = []
        for idx, line in enumerate(code_str.splitlines(), 1):
            for pat in patterns:
                if re.search(pat, line):
                    hits.append(f"L{idx}: {line.strip()}")
        return hits

    def _build_confidence_scores(vulns: list[VulnerabilityInfo]) -> dict[str, float]:
        scores: dict[str, float] = {}
        for vuln in vulns:
            key = f"{vuln.type}@L{vuln.line or 0}"
            scores[key] = 0.9 if vuln.severity.lower() in {"high", "critical"} else 0.7
        return scores

    def _build_false_positive_analysis(
        count: int, max_limit: int | None
    ) -> dict[str, Any]:
        suppressed = 0
        if max_limit is not None and max_limit >= 0 and count > max_limit:
            suppressed = count - max_limit
        return {
            "total_findings": count,
            "suppressed": suppressed,
            "notes": [] if suppressed == 0 else ["Findings truncated by tier limits"],
        }

    def _detect_policy_violations(code_str: str) -> list[dict[str, Any]]:
        """Use PolicyEngine for weak crypto detection."""
        try:
            policy_engine = PolicyEngine()
            violations_list = policy_engine.check_weak_crypto(code_str)

            # Convert PolicyViolation objects to dict format
            return [
                {
                    "rule": v.policy_id,
                    "line": v.line,
                    "severity": v.severity,
                    "detail": v.description,
                    "remediation": v.remediation,
                }
                for v in violations_list
            ]
        except Exception as e:
            logger.warning(f"Policy engine check failed: {e}")
            return []

    def _build_compliance_mappings(
        vulns: list[VulnerabilityInfo], policies: list[dict[str, Any]]
    ) -> dict[str, list[str]]:
        if not vulns and not policies:
            return {}
        return {
            "OWASP_TOP_10": ["A01-Broken Access Control", "A03-Injection"],
            "HIPAA": ["164.312(e)(1)"],
            "SOC2": ["CC6.7"],
            "PCI_DSS": ["6.5.1"],
        }

    def _detect_custom_logging_rules(code_str: str) -> list[dict[str, Any]]:
        """Use PolicyEngine for sensitive data logging detection."""
        try:
            policy_engine = PolicyEngine()
            violations_list = policy_engine.check_sensitive_logging(code_str)

            # Convert PolicyViolation objects to dict format
            return [
                {
                    "rule": v.policy_id,
                    "line": v.line,
                    "severity": v.severity,
                    "detail": v.description,
                    "remediation": v.remediation,
                }
                for v in violations_list
            ]
        except Exception as e:
            logger.warning(f"Policy engine logging check failed: {e}")
            return []

    # [20251230_FEATURE] v1.0 roadmap Enterprise tier helper functions
    def _build_priority_ordered_findings(
        vulns: list[VulnerabilityInfo],
    ) -> list[dict[str, Any]]:
        """Build priority-ordered findings list for Enterprise tier.

        Priority is determined by:
        1. Severity (critical > high > medium > low)
        2. CWE risk score (injection > XSS > traversal > other)
        3. Line number (earlier findings first)
        """
        severity_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
        cwe_priority = {
            "CWE-89": 1,  # SQL Injection
            "CWE-78": 1,  # Command Injection
            "CWE-94": 1,  # Code Injection
            "CWE-79": 2,  # XSS
            "CWE-22": 3,  # Path Traversal
            "CWE-90": 2,  # LDAP Injection
            "CWE-943": 2,  # NoSQL Injection
        }

        def get_priority(v: VulnerabilityInfo) -> tuple[int, int, int]:
            sev = severity_order.get(v.severity.lower(), 4)
            cwe = cwe_priority.get(v.cwe, 5)
            line = v.line or 9999
            return (sev, cwe, line)

        sorted_vulns = sorted(vulns, key=get_priority)

        return [
            {
                "rank": idx + 1,
                "type": v.type,
                "cwe": v.cwe,
                "severity": v.severity,
                "line": v.line,
                "description": v.description,
                "priority_score": 100
                - (get_priority(v)[0] * 20 + get_priority(v)[1] * 5),
            }
            for idx, v in enumerate(sorted_vulns)
        ]

    def _build_reachability_analysis(
        vulns: list[VulnerabilityInfo], code_str: str
    ) -> dict[str, Any]:
        """Build vulnerability reachability analysis for Enterprise tier.

        Analyzes which vulnerabilities are reachable from entry points
        (main functions, web handlers, API endpoints).
        """
        entry_point_patterns = [
            r"def main\s*\(",
            r"@app\.route\s*\(",
            r"@router\.",
            r"async def \w+_handler\s*\(",
            r"def \w+_view\s*\(",
            r"class \w+View\s*\(",
            r"if __name__\s*==",
        ]

        entry_points: list[dict[str, Any]] = []
        for idx, line in enumerate(code_str.splitlines(), 1):
            for pattern in entry_point_patterns:
                if re.search(pattern, line):
                    entry_points.append(
                        {
                            "line": idx,
                            "pattern": pattern.replace("\\s*", " ").replace("\\(", "("),
                            "code": line.strip()[:80],
                        }
                    )
                    break

        # Determine reachability based on line proximity to entry points
        reachable_vulns = []
        unreachable_vulns = []

        for v in vulns:
            vuln_line = v.line or 0
            is_reachable = False

            for ep in entry_points:
                # Simple heuristic: vulnerability is reachable if it's after an entry point
                if ep["line"] < vuln_line:
                    is_reachable = True
                    break

            if is_reachable or not entry_points:
                reachable_vulns.append(
                    {
                        "type": v.type,
                        "cwe": v.cwe,
                        "line": v.line,
                        "reachable": True,
                        "reason": (
                            "Downstream of entry point"
                            if entry_points
                            else "No entry point analysis possible"
                        ),
                    }
                )
            else:
                unreachable_vulns.append(
                    {
                        "type": v.type,
                        "cwe": v.cwe,
                        "line": v.line,
                        "reachable": False,
                        "reason": "No entry point precedes this vulnerability",
                    }
                )

        return {
            "entry_points_found": len(entry_points),
            "entry_points": entry_points[:5],  # Limit for output size
            "reachable_count": len(reachable_vulns),
            "unreachable_count": len(unreachable_vulns),
            "reachable_vulnerabilities": reachable_vulns,
            "unreachable_vulnerabilities": unreachable_vulns,
            "analysis_confidence": "high" if entry_points else "low",
        }

    def _build_false_positive_tuning(
        vulns: list[VulnerabilityInfo], sanitizers: list[str]
    ) -> dict[str, Any]:
        """Build false positive tuning results for Enterprise tier.

        Provides tuning recommendations based on detected sanitizers
        and vulnerability patterns.
        """
        tuning_suggestions: list[dict[str, Any]] = []

        # Check if sanitizers could reduce false positives
        if sanitizers:
            for v in vulns:
                if v.type in {"XSS", "SQL Injection", "Command Injection"}:
                    tuning_suggestions.append(
                        {
                            "vulnerability_type": v.type,
                            "line": v.line,
                            "suggestion": "Review if sanitizer at nearby line handles this input",
                            "sanitizers_detected": sanitizers[:3],
                            "confidence_adjustment": -0.1,
                        }
                    )

        # Detect common false positive patterns
        fp_patterns = {
            "test_": "Test file - likely intentional vulnerable code",
            "mock_": "Mock data - likely not real vulnerability",
            "example_": "Example code - likely documentation",
        }

        suppression_candidates = []
        for v in vulns:
            desc_lower = (v.description or "").lower()
            for pattern, reason in fp_patterns.items():
                if pattern in desc_lower:
                    suppression_candidates.append(
                        {
                            "type": v.type,
                            "line": v.line,
                            "pattern": pattern,
                            "reason": reason,
                        }
                    )

        return {
            "sanitizers_detected": len(sanitizers),
            "tuning_suggestions": tuning_suggestions,
            "suppression_candidates": suppression_candidates,
            "recommended_actions": (
                [
                    "Review sanitizer coverage for detected vulnerabilities",
                    "Consider suppressing test/mock file findings",
                    "Add custom rules for organization-specific patterns",
                ]
                if tuning_suggestions or suppression_candidates
                else []
            ),
        }

    def _build_remediation_suggestions_list(
        vulns: list[VulnerabilityInfo],
    ) -> list[str]:
        """Build remediation suggestions for Pro/Enterprise tier.

        [20260118_FEATURE] v1.0 - Pro tier remediation suggestions

        Returns a list of actionable remediation strings based on vulnerability types.
        """
        remediation_map = {
            "CWE-89": "Use parameterized queries or ORM to prevent SQL injection",
            "CWE-78": "Avoid shell=True in subprocess; use list-based arguments",
            "CWE-94": "Avoid eval()/exec(); use safe alternatives like ast.literal_eval()",
            "CWE-79": "Escape user input before rendering; use template auto-escaping",
            "CWE-22": "Validate and sanitize file paths; use pathlib with resolve()",
            "CWE-90": "Use parameterized LDAP queries; escape special characters",
            "CWE-943": "Use parameterized NoSQL queries; validate input types",
            "CWE-502": "Avoid deserializing untrusted data; use JSON instead of pickle",
            "CWE-918": "Validate and whitelist URLs for SSRF prevention",
            "CWE-352": "Implement CSRF tokens for state-changing operations",
            "CWE-347": "Always verify JWT signatures; avoid algorithm='none'",
            "CWE-327": "Use strong cryptographic algorithms (e.g., SHA-256, bcrypt)",
            "CWE-798": "Move secrets to environment variables or secret managers",
            "CWE-611": "Disable external entities in XML parsers",
            "CWE-1336": "Avoid rendering user input directly in templates (SSTI)",
        }

        suggestions: list[str] = []
        seen_cwes: set[str] = set()

        for vuln in vulns:
            cwe = vuln.cwe
            if cwe and cwe not in seen_cwes:
                seen_cwes.add(cwe)
                if cwe in remediation_map:
                    suggestions.append(f"{cwe}: {remediation_map[cwe]}")
                else:
                    suggestions.append(f"{cwe}: Review and remediate {vuln.type}")

        return suggestions

    # Handle file_path parameter
    if file_path is not None:
        try:
            path = Path(file_path)
            if not path.exists():
                return SecurityResult(
                    success=False,
                    has_vulnerabilities=False,
                    vulnerability_count=0,
                    risk_level="unknown",
                    error=f"File not found: {file_path}.",
                )
            if not path.is_file():
                return SecurityResult(
                    success=False,
                    has_vulnerabilities=False,
                    vulnerability_count=0,
                    risk_level="unknown",
                    error=f"Path is not a file: {file_path}.",
                )
            code = path.read_text(encoding="utf-8")

            # [20251220_FEATURE] v3.0.4 - Detect language from file extension
            ext = path.suffix.lower()
            extension_map = {
                ".py": "python",
                ".pyi": "python",
                ".pyw": "python",
                ".js": "javascript",
                ".mjs": "javascript",
                ".cjs": "javascript",
                ".jsx": "javascript",
                ".ts": "typescript",
                ".tsx": "typescript",
                ".mts": "typescript",
                ".cts": "typescript",
                ".java": "java",
            }
            detected_language = extension_map.get(ext, "python")
        except Exception as e:
            return SecurityResult(
                success=False,
                has_vulnerabilities=False,
                vulnerability_count=0,
                risk_level="unknown",
                error=f"Failed to read file: {str(e)}.",
            )

    if code is None:
        return SecurityResult(
            success=False,
            has_vulnerabilities=False,
            vulnerability_count=0,
            risk_level="unknown",
            error="Either 'code' or 'file_path' must be provided.",
        )

    valid, validation_error = _validate_code(code)
    if not valid:
        return SecurityResult(
            success=False,
            has_vulnerabilities=False,
            vulnerability_count=0,
            risk_level="unknown",
            error=validation_error,
        )

    # [20251226_FEATURE] Enforce tier file size limits
    max_file_size_kb = limits.get("max_file_size_kb")
    if max_file_size_kb is not None:
        code_size_kb = len(code.encode("utf-8")) / 1024
        if code_size_kb > max_file_size_kb:
            return SecurityResult(
                success=False,
                has_vulnerabilities=False,
                vulnerability_count=0,
                risk_level="unknown",
                error=(
                    f"Code size {code_size_kb:.1f}KB exceeds tier limit "
                    f"{max_file_size_kb}KB for tier {tier.title()}."
                ),
            )

    # Check cache first (tier-aware key to avoid cross-tier bleed)
    cache = _get_cache()
    cache_key = None
    if cache:
        cache_key = f"{tier}:{hashlib.sha256(code.encode('utf-8')).hexdigest()}"
        cached = cache.get(cache_key, "security")
        if cached is not None:
            logger.debug("Cache hit for security_scan")
            # [20251227_BUGFIX] v3.1.1 - Handle both dict (JSON cache) and object (pickle cache)
            if isinstance(cached, SecurityResult):
                return cached
            if isinstance(cached, dict):
                if "vulnerabilities" in cached:
                    # Handle mixed case: dict with VulnerabilityInfo objects OR plain dicts
                    vuln_list = cached["vulnerabilities"]
                    if vuln_list and isinstance(vuln_list[0], dict):
                        cached["vulnerabilities"] = [
                            VulnerabilityInfo(**v) for v in vuln_list
                        ]
                    # else: already VulnerabilityInfo objects, leave as-is
                return SecurityResult(**cached)
            return cached

    vulnerabilities: list[VulnerabilityInfo] = []
    taint_sources: list[str] = []

    try:
        # [20251220_FEATURE] v3.0.4 - Use UnifiedSinkDetector for non-Python languages
        if detected_language != "python":
            detector = _get_sink_detector()
            detected_sinks = detector.detect_sinks(
                code, detected_language, min_confidence=0.7
            )

            for sink in detected_sinks:
                vulnerabilities.append(
                    VulnerabilityInfo(
                        type=getattr(sink, "vulnerability_type", "") or sink.pattern,
                        cwe=f"CWE-{_get_cwe_from_sink_type(sink.sink_type)}",
                        severity="high" if sink.confidence >= 0.9 else "medium",
                        line=sink.line,
                        description=f"Detected {sink.pattern} with {sink.confidence:.0%} confidence",
                    )
                )

            if detected_language == "typescript":
                try:
                    from code_scalpel.security.type_safety import (
                        TypeEvaporationDetector,
                    )

                    te_detector = TypeEvaporationDetector()
                    te_result = te_detector.analyze(code, file_path or "<string>")

                    for vuln in te_result.vulnerabilities:
                        vulnerabilities.append(
                            VulnerabilityInfo(
                                type=f"Type Evaporation: {vuln.risk_type.name}",
                                cwe=vuln.cwe_id,
                                severity=vuln.severity.lower(),
                                line=vuln.location[0],
                                description=vuln.description,
                            )
                        )
                except ImportError:
                    pass
        else:
            from code_scalpel.security.analyzers import SecurityAnalyzer

            analyzer = SecurityAnalyzer()
            result = analyzer.analyze(code).to_dict()

            for vuln in result.get("vulnerabilities", []):
                sink_loc = vuln.get("sink_location")
                line_number = (
                    sink_loc[0]
                    if sink_loc and isinstance(sink_loc, (list, tuple))
                    else None
                )

                vulnerabilities.append(
                    VulnerabilityInfo(
                        type=vuln.get("type", "Unknown"),
                        cwe=vuln.get("cwe", "Unknown"),
                        severity=vuln.get("severity", "medium"),
                        line=line_number,
                        description=vuln.get("description", ""),
                    )
                )

            for source in result.get("taint_sources", []):
                taint_sources.append(str(source))

    except ImportError:
        vulnerabilities, taint_sources = _basic_scan_patterns(code)
    except Exception as exc:
        return SecurityResult(
            success=False,
            has_vulnerabilities=False,
            vulnerability_count=0,
            risk_level="unknown",
            error=f"Security scan failed: {str(exc)}.",
        )

    max_findings = limits.get("max_findings")
    if max_findings is not None and max_findings >= 0:
        vulnerabilities = vulnerabilities[:max_findings]

    vuln_count = len(vulnerabilities)
    if vuln_count == 0:
        risk_level = "low"
    elif vuln_count <= 2:
        risk_level = "medium"
    elif vuln_count <= 5:
        risk_level = "high"
    else:
        risk_level = "critical"

    sanitizer_paths: list[str] | None = None
    confidence_scores: dict[str, float] | None = None
    false_positive_analysis: dict[str, Any] | None = None
    policy_violations: list[dict[str, Any]] | None = None
    compliance_mappings: dict[str, list[str]] | None = None
    custom_rule_results: list[dict[str, Any]] | None = None
    # [20260118_FEATURE] Pro tier remediation suggestions
    remediation_suggestions: list[str] | None = None
    # [20251230_FEATURE] v1.0 roadmap Enterprise tier fields
    priority_ordered_findings: list[dict[str, Any]] | None = None
    reachability_analysis: dict[str, Any] | None = None
    false_positive_tuning: dict[str, Any] | None = None

    if {"sanitizer_recognition", "context_aware_scanning"} & caps_set:
        sanitizer_paths = _detect_sanitizers(code)

    if {"false_positive_reduction", "data_flow_sensitive_analysis"} & caps_set:
        confidence_scores = _build_confidence_scores(vulnerabilities)
        false_positive_analysis = _build_false_positive_analysis(
            vuln_count, max_findings
        )

    if {"custom_policy_engine", "compliance_rule_checking"} & caps_set:
        policy_violations = _detect_policy_violations(code)
        compliance_map = _build_compliance_mappings(vulnerabilities, policy_violations)
        compliance_mappings = compliance_map if compliance_map else None

    if {"custom_policy_engine", "org_specific_rules"} & caps_set:
        custom_rule_results = _detect_custom_logging_rules(code)
        if custom_rule_results == []:
            custom_rule_results = None

    # [20260118_FEATURE] Pro tier remediation suggestions
    if "remediation_suggestions" in caps_set:
        remediation_suggestions = _build_remediation_suggestions_list(vulnerabilities)
        if not remediation_suggestions:
            remediation_suggestions = None
    # [20251230_FEATURE] v1.0 roadmap Enterprise tier features
    if "priority_finding_ordering" in caps_set:
        priority_ordered_findings = _build_priority_ordered_findings(vulnerabilities)

    if "vulnerability_reachability_analysis" in caps_set:
        reachability_analysis = _build_reachability_analysis(vulnerabilities, code)

    if "false_positive_tuning" in caps_set:
        false_positive_tuning = _build_false_positive_tuning(
            vulnerabilities, sanitizer_paths or []
        )

    security_result = SecurityResult(
        success=True,
        has_vulnerabilities=vuln_count > 0,
        vulnerability_count=vuln_count,
        risk_level=risk_level,
        vulnerabilities=vulnerabilities,
        taint_sources=taint_sources,
        sanitizer_paths=sanitizer_paths,
        confidence_scores=confidence_scores,
        false_positive_analysis=false_positive_analysis,
        remediation_suggestions=remediation_suggestions,
        policy_violations=policy_violations,
        compliance_mappings=compliance_mappings,
        custom_rule_results=custom_rule_results,
        priority_ordered_findings=priority_ordered_findings,
        reachability_analysis=reachability_analysis,
        false_positive_tuning=false_positive_tuning,
    )

    if cache and cache_key:
        cache.set(cache_key, "security", security_result.model_dump())

    return security_result


def _get_cwe_from_sink_type(sink_type) -> str:
    """[20251220_FEATURE] v3.0.4 - Map sink types to CWE IDs.
    [20251220_FIX] v3.0.5 - Added more sink types, fallback to CWE-20 instead of Unknown.
    """
    cwe_map = {
        "SQL_QUERY": "89",
        "HTML_OUTPUT": "79",
        "DOM_XSS": "79",
        "FILE_PATH": "22",
        "SHELL_COMMAND": "78",
        "EVAL": "94",
        "DESERIALIZATION": "502",
        "XXE": "611",
        "SSRF": "918",
        "SSTI": "1336",
        "WEAK_CRYPTO": "327",
        "PROTOTYPE_POLLUTION": "1321",
        "HARDCODED_SECRET": "798",
        "LDAP_QUERY": "90",
        "NOSQL_QUERY": "943",
        "XPATH_QUERY": "643",
        "LOG_INJECTION": "117",
        "HTTP_REDIRECT": "601",
        "REGEX_DOS": "1333",
    }
    sink_name = getattr(sink_type, "name", str(sink_type))
    # Fallback to CWE-20 (Improper Input Validation) instead of Unknown
    return cwe_map.get(sink_name, "20")


# ==========================================================================
# [20251216_FEATURE] v2.5.0 - Unified sink detection MCP tool
# ==========================================================================


def _sink_coverage_summary(detector: UnifiedSinkDetector) -> dict[str, Any]:
    """Compute coverage summary across languages."""

    by_language: dict[str, int] = {}
    total_patterns = 0

    for vuln_sinks in detector.sinks.values():
        for lang, sink_list in vuln_sinks.items():
            by_language[lang] = by_language.get(lang, 0) + len(sink_list)
            total_patterns += len(sink_list)

    return {
        "total_patterns": total_patterns,
        "by_language": by_language,
    }


# =============================================================================
# [20251231_FEATURE] v1.0 - Helper functions for unified_sink_detect
# =============================================================================


def _get_cwe_for_sink(vulnerability_type: str | None, sink_type: Any) -> str | None:
    """Map vulnerability/sink type to CWE identifier."""
    vuln = (vulnerability_type or str(sink_type) or "").upper()

    cwe_mapping = {
        "SQL": "CWE-89",  # SQL Injection
        "COMMAND": "CWE-78",  # OS Command Injection
        "SHELL": "CWE-78",
        "EVAL": "CWE-94",  # Code Injection
        "EXEC": "CWE-94",
        "XSS": "CWE-79",  # Cross-site Scripting
        "PATH": "CWE-22",  # Path Traversal
        "FILE": "CWE-73",  # External Control of File Name
        "LDAP": "CWE-90",  # LDAP Injection
        "XML": "CWE-91",  # XML Injection
        "XPATH": "CWE-643",  # XPath Injection
        "DESERIALIZATION": "CWE-502",  # Deserialization
        "SSRF": "CWE-918",  # Server-Side Request Forgery
        "REDIRECT": "CWE-601",  # Open Redirect
        "LOG": "CWE-117",  # Log Injection
    }

    for key, cwe in cwe_mapping.items():
        if key in vuln:
            return cwe

    return None


def _analyze_sink_context(
    code: str,
    sinks: list[UnifiedDetectedSink],
    language: str,
) -> dict[str, Any]:
    """
    [20251231_FEATURE] Analyze context around detected sinks for Pro tier.

    Determines if sinks are in dangerous contexts (user input handling,
    request processing, etc.)
    """
    lines = code.splitlines()

    # Context patterns that indicate higher risk
    high_risk_patterns = {
        "python": ["request.", "input(", "sys.argv", "os.environ", "flask.request"],
        "javascript": [
            "req.body",
            "req.query",
            "req.params",
            "document.",
            "window.location",
        ],
        "typescript": [
            "req.body",
            "req.query",
            "req.params",
            "document.",
            "window.location",
        ],
        "java": ["getParameter", "getHeader", "getInputStream", "Scanner"],
    }

    patterns = high_risk_patterns.get(language.lower(), [])

    context_results: dict[str, Any] = {
        "analyzed_sinks": 0,
        "high_risk_context": 0,
        "medium_risk_context": 0,
        "low_risk_context": 0,
        "details": [],
    }

    for sink in sinks:
        context_results["analyzed_sinks"] += 1

        # Check surrounding lines for context
        start_line = max(0, sink.line - 5)
        end_line = min(len(lines), sink.line + 5)
        context_window = "\n".join(lines[start_line:end_line])

        # Determine risk level based on context
        risk = "low"
        risk_reason = "No immediate user input detected"

        for pattern in patterns:
            if pattern in context_window:
                risk = "high"
                risk_reason = f"User input pattern '{pattern}' found in context"
                break

        if risk == "low" and (
            "param" in context_window.lower() or "input" in context_window.lower()
        ):
            risk = "medium"
            risk_reason = "Potential input handling detected"

        if risk == "high":
            context_results["high_risk_context"] += 1
        elif risk == "medium":
            context_results["medium_risk_context"] += 1
        else:
            context_results["low_risk_context"] += 1

        context_results["details"].append(
            {
                "line": sink.line,
                "sink_type": sink.sink_type,
                "context_risk": risk,
                "reason": risk_reason,
            }
        )

    return context_results


def _detect_framework_sinks(code: str, language: str) -> list[dict[str, Any]]:
    """
    [20251231_FEATURE] Detect framework-specific sinks for Pro tier.
    """
    framework_sinks: list[dict[str, Any]] = []

    # Framework-specific dangerous patterns
    frameworks = {
        "python": {
            "flask": {
                "patterns": ["render_template_string", "make_response", "send_file"],
                "risk": "Template injection or file exposure",
            },
            "django": {
                "patterns": ["mark_safe", "SafeString", "format_html"],
                "risk": "XSS through unsafe HTML marking",
            },
            "sqlalchemy": {
                "patterns": ["text(", "execute("],
                "risk": "Raw SQL execution",
            },
        },
        "javascript": {
            "express": {
                "patterns": ["res.send(", "res.render(", "res.sendFile("],
                "risk": "Response injection or file exposure",
            },
            "react": {
                "patterns": ["dangerouslySetInnerHTML", "__html"],
                "risk": "XSS through raw HTML insertion",
            },
        },
        "java": {
            "spring": {
                "patterns": ["@RequestMapping", "ResponseEntity", "JdbcTemplate"],
                "risk": "Web exposure or SQL execution",
            },
        },
    }

    lang_frameworks = frameworks.get(language.lower(), {})

    for framework, config in lang_frameworks.items():
        for pattern in config["patterns"]:
            for i, line in enumerate(code.splitlines(), 1):
                if pattern in line:
                    framework_sinks.append(
                        {
                            "framework": framework,
                            "pattern": pattern,
                            "line": i,
                            "risk_description": config["risk"],
                            "code_snippet": line.strip()[:80],
                        }
                    )

    return framework_sinks


def _build_sink_compliance_mapping(sinks: list[UnifiedDetectedSink]) -> dict[str, Any]:
    """
    [20251231_FEATURE] Map sinks to compliance standards for Enterprise tier.
    """
    compliance: dict[str, Any] = {
        "standards_affected": [],
        "violations": [],
        "recommendations": [],
    }

    # Compliance mappings
    compliance_rules = {
        "CWE-89": {  # SQL Injection
            "standards": ["SOC2", "PCI-DSS", "HIPAA"],
            "requirement": "Input validation and parameterized queries required",
        },
        "CWE-78": {  # Command Injection
            "standards": ["SOC2", "PCI-DSS"],
            "requirement": "Command execution must be sandboxed or avoided",
        },
        "CWE-79": {  # XSS
            "standards": ["SOC2", "OWASP"],
            "requirement": "Output encoding required for user-facing content",
        },
        "CWE-94": {  # Code Injection
            "standards": ["SOC2", "PCI-DSS", "HIPAA"],
            "requirement": "Dynamic code execution prohibited without strict controls",
        },
        "CWE-502": {  # Deserialization
            "standards": ["SOC2", "PCI-DSS"],
            "requirement": "Avoid deserializing untrusted data",
        },
    }

    standards_set: set[str] = set()

    for sink in sinks:
        cwe = sink.cwe_id
        if cwe and cwe in compliance_rules:
            rule = compliance_rules[cwe]
            standards_set.update(rule["standards"])

            compliance["violations"].append(
                {
                    "cwe": cwe,
                    "line": sink.line,
                    "sink_type": sink.sink_type,
                    "standards_violated": rule["standards"],
                    "requirement": rule["requirement"],
                }
            )

    compliance["standards_affected"] = list(standards_set)

    # Generate recommendations
    if "PCI-DSS" in standards_set:
        compliance["recommendations"].append(
            "PCI-DSS: Implement input validation and output encoding per Requirement 6.5"
        )
    if "HIPAA" in standards_set:
        compliance["recommendations"].append(
            "HIPAA: Ensure PHI access controls meet Technical Safeguard requirements"
        )
    if "SOC2" in standards_set:
        compliance["recommendations"].append(
            "SOC2: Address security vulnerabilities per CC6.1 control requirements"
        )

    return compliance


def _build_historical_comparison(sinks: list[UnifiedDetectedSink]) -> dict[str, Any]:
    """
    [20251231_FEATURE] Build historical comparison for Enterprise tier.

    Note: Full historical tracking requires persistent storage.
    This provides a snapshot structure for integration.
    """
    return {
        "current_scan": {
            "timestamp": __import__("datetime").datetime.now().isoformat(),
            "total_sinks": len(sinks),
            "by_severity": {
                "critical": sum(
                    1 for s in sinks if "SQL" in (s.sink_type or "").upper()
                ),
                "high": sum(
                    1
                    for s in sinks
                    if "COMMAND" in (s.sink_type or "").upper()
                    or "SHELL" in (s.sink_type or "").upper()
                ),
                "medium": sum(1 for s in sinks if "XSS" in (s.sink_type or "").upper()),
                "low": len(sinks)
                - sum(
                    1
                    for s in sinks
                    if any(
                        k in (s.sink_type or "").upper()
                        for k in ["SQL", "COMMAND", "SHELL", "XSS"]
                    )
                ),
            },
        },
        "comparison_available": False,
        "message": "Historical data requires persistent storage integration",
        "integration_hint": "Connect to sink_history table or external tracking service",
    }


def _generate_sink_remediation(
    sinks: list[UnifiedDetectedSink],
    language: str,
) -> list[dict[str, Any]]:
    """
    [20251231_FEATURE] Generate automated remediation suggestions for Enterprise tier.
    """
    remediation: list[dict[str, Any]] = []

    # Remediation templates by CWE
    remediation_templates = {
        "CWE-89": {
            "title": "SQL Injection Prevention",
            "python": "Use parameterized queries: cursor.execute('SELECT * FROM users WHERE id = ?', (user_id,))",
            "javascript": "Use prepared statements: db.query('SELECT * FROM users WHERE id = $1', [userId])",
            "java": 'Use PreparedStatement: stmt = conn.prepareStatement("SELECT * FROM users WHERE id = ?"); stmt.setInt(1, userId);',
            "priority": "critical",
        },
        "CWE-78": {
            "title": "Command Injection Prevention",
            "python": "Use subprocess with list args: subprocess.run(['ls', '-la', user_path], shell=False)",
            "javascript": "Use execFile instead of exec: execFile('ls', ['-la', userPath])",
            "java": 'Use ProcessBuilder with argument list: new ProcessBuilder("ls", "-la", userPath)',
            "priority": "critical",
        },
        "CWE-79": {
            "title": "XSS Prevention",
            "python": "Use template auto-escaping or markupsafe.escape(user_input)",
            "javascript": "Use textContent instead of innerHTML: element.textContent = userInput",
            "java": "Use OWASP Encoder: Encode.forHtml(userInput)",
            "priority": "high",
        },
        "CWE-94": {
            "title": "Code Injection Prevention",
            "python": "Avoid eval/exec. Use ast.literal_eval() for safe parsing",
            "javascript": "Avoid eval(). Use JSON.parse() for data parsing",
            "java": "Avoid ScriptEngine with user input. Use predefined operations",
            "priority": "critical",
        },
    }

    lang_key = language.lower()

    for sink in sinks:
        cwe = sink.cwe_id
        if cwe and cwe in remediation_templates:
            template = remediation_templates[cwe]
            fix = template.get(
                lang_key, template.get("python", "Review and sanitize input")
            )

            remediation.append(
                {
                    "line": sink.line,
                    "sink_type": sink.sink_type,
                    "cwe": cwe,
                    "title": template["title"],
                    "priority": template["priority"],
                    "suggested_fix": fix,
                    "code_example": f"// Before: {sink.code_snippet[:50]}...\n// After: {fix}",
                }
            )

    return remediation


def _unified_sink_detect_sync(
    code: str,
    language: str,
    min_confidence: float,
    tier: str = "community",
    capabilities: dict | None = None,
) -> UnifiedSinkResult:
    """Synchronous unified sink detection wrapper.

    [20251225_FEATURE] v3.3.0 - Tier-based gating and outputs.
    """

    from code_scalpel.licensing.features import get_tool_capabilities

    lang = (language or "").lower()

    def _make_sink_id(
        *,
        pattern: str,
        sink_type: str,
        line: int,
        column: int,
        vulnerability_type: str | None,
        cwe_id: str | None,
    ) -> str:
        """Generate a stable sink id for correlation across runs."""

        raw = (
            f"{lang}|{pattern}|{sink_type}|{line}|{column}|"
            f"{vulnerability_type or ''}|{cwe_id or ''}"
        )
        return hashlib.sha256(raw.encode("utf-8")).hexdigest()[:12]

    def _snippet_from_source(line_no: int) -> str:
        if line_no <= 0:
            return ""
        lines = code.splitlines()
        if 1 <= line_no <= len(lines):
            return lines[line_no - 1]
        return ""

    def _truncate_snippet(
        snippet: str, *, max_len: int = 200
    ) -> tuple[str, bool, int | None]:
        if len(snippet) <= max_len:
            return snippet, False, None
        # Keep output length stable at max_len, ending with an ellipsis.
        return snippet[: max_len - 1] + "", True, len(snippet)

    if code is None or code.strip() == "":
        return UnifiedSinkResult(
            success=False,
            error_code="UNIFIED_SINK_DETECT_MISSING_CODE",
            language=lang,
            sink_count=0,
            error="Parameter 'code' is required.",
            coverage_summary={},
        )

    if not 0.0 <= min_confidence <= 1.0:
        return UnifiedSinkResult(
            success=False,
            error_code="UNIFIED_SINK_DETECT_INVALID_MIN_CONFIDENCE",
            language=lang,
            sink_count=0,
            error="Parameter 'min_confidence' must be between 0.0 and 1.0.",
            coverage_summary={},
        )

    # Load capabilities if not provided
    if capabilities is None:
        capabilities = get_tool_capabilities("unified_sink_detect", tier) or {}
    caps_set: set[str] = set((capabilities.get("capabilities", []) or []))
    limits = capabilities.get("limits", {}) or {}

    # Enforce language limits
    allowed_langs = limits.get("languages")
    if allowed_langs is not None:
        # [20260102_REFACTOR] Avoid ambiguous loop variable names in allowlist check.
        if isinstance(allowed_langs, str):
            allowed_langs_lower = [allowed_langs.lower()]
        else:
            allowed_langs_lower = [str(allowed).lower() for allowed in allowed_langs]

        if allowed_langs_lower and allowed_langs_lower != ["all"]:
            if lang not in allowed_langs_lower:
                return UnifiedSinkResult(
                    success=False,
                    error_code="UNIFIED_SINK_DETECT_UNSUPPORTED_LANGUAGE",
                    language=lang,
                    sink_count=0,
                    error=f"Unsupported language for tier {tier.title()}: {language}",
                    coverage_summary={},
                )

    # [20251220_PERF] v3.0.5 - Use singleton detector to avoid rebuilding patterns
    detector = _get_sink_detector()
    try:
        detected = detector.detect_sinks(code, lang, min_confidence)
    except ValueError as e:
        msg = str(e)
        lower_msg = msg.lower()
        if "unsupported" in lower_msg and "language" in lower_msg:
            error_code = "UNIFIED_SINK_DETECT_UNSUPPORTED_LANGUAGE"
        else:
            error_code = "UNIFIED_SINK_DETECT_DETECTOR_ERROR"
        return UnifiedSinkResult(
            success=False,
            error_code=error_code,
            language=lang,
            sink_count=0,
            error=msg,
            coverage_summary=_sink_coverage_summary(detector),
        )

    # Build base sinks
    sinks: list[UnifiedDetectedSink] = []
    for sink in detected:
        owasp = detector.get_owasp_category(sink.vulnerability_type)
        # [20251231_FEATURE] v1.0 - Map CWE from vulnerability type
        cwe_id = _get_cwe_for_sink(sink.vulnerability_type, sink.sink_type)

        sink_type = getattr(sink.sink_type, "name", str(sink.sink_type))
        line = int(getattr(sink, "line", 0) or 0)
        column = int(getattr(sink, "column", 0) or 0)
        vulnerability_type = getattr(sink, "vulnerability_type", None)

        raw_snippet = getattr(sink, "code_snippet", "") or _snippet_from_source(line)
        code_snippet, snippet_truncated, original_len = _truncate_snippet(raw_snippet)
        sinks.append(
            UnifiedDetectedSink(
                sink_id=_make_sink_id(
                    pattern=sink.pattern,
                    sink_type=sink_type,
                    line=line,
                    column=column,
                    vulnerability_type=vulnerability_type,
                    cwe_id=cwe_id,
                ),
                pattern=sink.pattern,
                sink_type=sink_type,
                confidence=sink.confidence,
                line=line,
                column=column,
                code_snippet=code_snippet,
                code_snippet_truncated=snippet_truncated,
                code_snippet_original_len=original_len,
                vulnerability_type=vulnerability_type,
                owasp_category=owasp,
                cwe_id=cwe_id,
            )
        )

    # Enforce max_sinks limit
    max_sinks = limits.get("max_sinks")
    truncated: bool | None = None
    sinks_detected: int | None = None
    max_sinks_applied: int | None = None
    if max_sinks is not None and len(sinks) > max_sinks:
        sinks_detected = len(sinks)
        sinks = sinks[:max_sinks]
        truncated = True
        max_sinks_applied = int(max_sinks)

    # Tier-specific enrichments
    logic_sinks: list[dict[str, Any]] = []
    confidence_scores: dict[str, float] = {}
    sink_categories: dict[str, list[dict[str, Any]]] = {}
    risk_assessments: list[dict[str, Any]] = []
    custom_sink_matches: list[dict[str, Any]] = []
    extended_language_sinks: dict[str, list[dict[str, Any]]] = {}

    def _line_lookup(patterns: list[str]) -> int:
        for idx, line_text in enumerate(code.splitlines(), start=1):
            for p in patterns:
                if p in line_text:
                    return idx
        return 0

    # Pro/Enterprise: Logic sinks
    if "logic_sink_detection" in caps_set:
        if "s3_public_write_detection" in caps_set and "put_object" in code:
            logic_sinks.append(
                {
                    "type": "S3_PUBLIC_WRITE",
                    "line": _line_lookup(["put_object"]),
                    "confidence": 0.8,
                    "recommendation": "Avoid public-read ACL; use private bucket policies.",
                }
            )
        if "payment_api_detection" in caps_set and "stripe" in code:
            logic_sinks.append(
                {
                    "type": "PAYMENT_STRIPE",
                    "line": _line_lookup(["stripe"]),
                    "confidence": 0.82,
                    "recommendation": "Validate payment inputs and use secure tokens.",
                }
            )
        if "email_send_detection" in caps_set and (
            "SendGridAPIClient" in code or "mail.send" in code
        ):
            logic_sinks.append(
                {
                    "type": "EMAIL_SEND",
                    "line": _line_lookup(["SendGridAPIClient", "mail.send"]),
                    "confidence": 0.75,
                    "recommendation": "Sanitize email content and enforce rate limits.",
                }
            )

    # Confidence scoring (Pro/Enterprise)
    if "sink_confidence_scoring" in caps_set or "logic_sink_detection" in caps_set:
        for sink in sinks:
            key = sink.sink_id
            base = sink.confidence or min_confidence
            multiplier = 1.0
            vuln = (sink.vulnerability_type or sink.sink_type or "").upper()
            if "SQL" in vuln:
                multiplier = 0.95
            elif "XSS" in vuln:
                multiplier = 0.85
            confidence_scores[key] = max(0.0, min(1.0, base * multiplier))

    # Enterprise: Custom sink patterns
    if "custom_sink_patterns" in caps_set:
        patterns = {
            "CUSTOM_INTERNAL_API": "internal_api_call",
            "CUSTOM_LEGACY_EXECUTE": "legacy_system.execute",
            "CUSTOM_PRIVILEGED_OPERATION": "privileged_operation",
        }
        for ctype, marker in patterns.items():
            if marker in code:
                custom_sink_matches.append(
                    {
                        "type": ctype,
                        "line": _line_lookup([marker]),
                        "confidence": 0.8,
                        "recommendation": "Review custom sink usage and enforce input validation.",
                    }
                )

    # Enterprise: Categorization and risk assessments
    if "sink_categorization" in caps_set or "risk_level_tagging" in caps_set:
        sink_categories = {"critical": [], "high": [], "medium": [], "low": []}
        for sink in sinks:
            vuln = (sink.vulnerability_type or sink.sink_type or "").upper()
            if "SQL" in vuln:
                category = "critical"
            elif "COMMAND" in vuln or "SHELL" in vuln:
                category = "high"
            elif "XSS" in vuln:
                category = "medium"
            else:
                category = "low"
            sink_categories[category].append(
                {
                    "sink_id": sink.sink_id,
                    "type": vuln,
                    "confidence": confidence_scores.get(sink.sink_id, sink.confidence),
                }
            )

    if "risk_level_tagging" in caps_set:
        # Simple risk scoring based on counts
        critical = len(sink_categories.get("critical", []))
        high = len(sink_categories.get("high", []))
        medium = len(sink_categories.get("medium", []))
        base_score = 10.0 - (critical * 2.5 + high * 1.5 + medium * 0.5)
        base_score = max(0.0, min(10.0, base_score))
        clearance = "ANY"
        if critical > 0:
            clearance = "ADMIN_ONLY"
        elif high > 0:
            clearance = "SENIOR_DEV"
        elif medium > 0:
            clearance = "DEVELOPER"
        risk_assessments.append(
            {
                "risk_score": base_score,
                "clearance_required": clearance,
                "rationale": "Calculated from categorized sinks",
            }
        )

    # Extended language support marker (informational)
    if "extended_language_support" in caps_set and allowed_langs:
        extended_language_sinks[lang] = [s.model_dump() for s in sinks]

    # [20251231_FEATURE] v1.0 - New Pro/Enterprise features
    context_analysis: dict[str, Any] | None = None
    framework_sinks: list[dict[str, Any]] = []
    compliance_mapping: dict[str, Any] | None = None
    historical_comparison: dict[str, Any] | None = None
    remediation_suggestions: list[dict[str, Any]] = []

    # Pro: Context-aware detection
    if "context_aware_detection" in caps_set:
        context_analysis = _analyze_sink_context(code, sinks, lang)

    # Pro: Framework-specific sinks
    if "framework_specific_sinks" in caps_set:
        framework_sinks = _detect_framework_sinks(code, lang)

    # Enterprise: Compliance mapping
    if "compliance_mapping" in caps_set:
        compliance_mapping = _build_sink_compliance_mapping(sinks)

    # Enterprise: Historical tracking (placeholder - requires persistent storage)
    if "historical_sink_tracking" in caps_set:
        historical_comparison = _build_historical_comparison(sinks)

    # Enterprise: Automated remediation
    if "automated_sink_remediation" in caps_set:
        remediation_suggestions = _generate_sink_remediation(sinks, lang)

    return UnifiedSinkResult(
        success=True,
        language=lang,
        sink_count=len(sinks),
        sinks=sinks,
        coverage_summary=_sink_coverage_summary(detector),
        logic_sinks=logic_sinks,
        extended_language_sinks=extended_language_sinks,
        confidence_scores=confidence_scores,
        sink_categories=sink_categories,
        risk_assessments=risk_assessments,
        custom_sink_matches=custom_sink_matches,
        # [20251231_FEATURE] v1.0 - New fields
        context_analysis=context_analysis,
        framework_sinks=framework_sinks,
        compliance_mapping=compliance_mapping,
        historical_comparison=historical_comparison,
        remediation_suggestions=remediation_suggestions,
        truncated=truncated,
        sinks_detected=sinks_detected,
        max_sinks_applied=max_sinks_applied,
    )


@mcp.tool()
async def unified_sink_detect(
    code: str, language: str, min_confidence: float = DEFAULT_MIN_CONFIDENCE
) -> UnifiedSinkResult:
    """
    Unified polyglot sink detection with confidence thresholds.

    Sinks are dangerous functions where untrusted data should never reach directly
    (e.g., eval(), execute(), os.system()). This tool detects sinks across multiple
    languages and maps them to CWE identifiers.

    [20251216_FEATURE] v2.5.0 "Guardian" - Expose unified sink detector via MCP.
    [20251220_BUGFIX] v3.0.5 - Use DEFAULT_MIN_CONFIDENCE for consistency.

    Example::

        result = await unified_sink_detect(
            code="eval(user_input)",
            language="python"
        )

        # Returns UnifiedSinkResult:
        # - sinks: [SinkInfo(name="eval", line=1, cwe="CWE-94", confidence=0.95)]
        # - sink_count: 1
        # - coverage: {"code_injection": True, "sql_injection": False, ...}
        # - language: "python"

        # Multi-language support:
        js_result = await unified_sink_detect(
            code="document.innerHTML = userInput;",
            language="javascript"
        )
        # Detects XSS sink with CWE-79

    Args:
        code: Source code to analyze
        language: Programming language (python, java, typescript, javascript)
        min_confidence: Minimum confidence threshold (0.0-1.0, default: 0.7)

    Returns:
        UnifiedSinkResult with detected sinks, CWE mappings, and coverage summary
    """

    tier = _get_current_tier()
    capabilities = get_tool_capabilities("unified_sink_detect", tier)
    return await asyncio.to_thread(
        _unified_sink_detect_sync, code, language, min_confidence, tier, capabilities
    )


# =============================================================================
# [20251229_FEATURE] v3.0.4 - Cross-File Type Evaporation Detection
# =============================================================================


class TypeEvaporationResultModel(BaseModel):
    """Result of type evaporation analysis.

    [20251226_FEATURE] v3.3.0 - Added Pro/Enterprise tier fields for full capabilities.
    """

    success: bool = Field(description="Whether analysis succeeded")
    server_version: str = Field(default=__version__, description="Code Scalpel version")
    frontend_vulnerabilities: int = Field(
        default=0, description="Number of frontend vulnerabilities"
    )
    backend_vulnerabilities: int = Field(
        default=0, description="Number of backend vulnerabilities"
    )
    cross_file_issues: int = Field(default=0, description="Number of cross-file issues")
    matched_endpoints: list[str] = Field(
        default_factory=list, description="Correlated API endpoints"
    )
    vulnerabilities: list[VulnerabilityInfo] = Field(
        default_factory=list, description="All vulnerabilities"
    )
    summary: str = Field(default="", description="Analysis summary")
    error: str | None = Field(default=None, description="Error message if failed")

    # Pro tier fields
    implicit_any_count: int = Field(
        default=0, description="[Pro] Count of implicit any detections"
    )
    network_boundaries: list[dict[str, Any]] = Field(
        default_factory=list, description="[Pro] Detected network call boundaries"
    )
    boundary_violations: list[dict[str, Any]] = Field(
        default_factory=list, description="[Pro] Type violations at boundaries"
    )
    library_boundaries: list[dict[str, Any]] = Field(
        default_factory=list, description="[Pro] Library call type boundaries"
    )
    json_parse_locations: list[dict[str, Any]] = Field(
        default_factory=list, description="[Pro] JSON.parse() without validation"
    )

    # Enterprise tier fields
    generated_schemas: list[dict[str, Any]] = Field(
        default_factory=list, description="[Enterprise] Generated Zod schemas"
    )
    validation_code: str | None = Field(
        default=None, description="[Enterprise] Generated validation code"
    )
    schema_coverage: float | None = Field(
        default=None, description="[Enterprise] Coverage ratio of validated endpoints"
    )
    pydantic_models: list[dict[str, Any]] = Field(
        default_factory=list, description="[Enterprise] Generated Pydantic models"
    )
    api_contract: dict[str, Any] | None = Field(
        default=None, description="[Enterprise] API contract validation results"
    )
    # [20251231_FEATURE] v1.0 - Added missing Enterprise fields
    remediation_suggestions: list[dict[str, Any]] = Field(
        default_factory=list,
        description="[Enterprise] Automated remediation suggestions",
    )
    custom_rule_violations: list[dict[str, Any]] = Field(
        default_factory=list, description="[Enterprise] Custom type rule violations"
    )
    compliance_report: dict[str, Any] | None = Field(
        default=None, description="[Enterprise] Type compliance validation report"
    )
    warnings: list[str] = Field(
        default_factory=list, description="Warnings such as limit truncation"
    )


def _split_virtual_files(code: str) -> list[str]:
    """Split a single code string into virtual files using // FILE: markers.

    If no markers are present, the entire string is treated as one file. This
    allows tests to simulate multi-file inputs for tier limit enforcement without
    changing the public MCP API shape.
    """

    lines = code.splitlines()
    segments: list[list[str]] = []
    current: list[str] = []

    for line in lines:
        if line.strip().startswith("// FILE:"):
            if current:
                segments.append(current)
                current = []
            current.append(line)
        else:
            current.append(line)

    if current:
        segments.append(current)

    if not segments:
        return [code]

    return ["\n".join(seg) for seg in segments]


def _enforce_file_limits(
    frontend_code: str,
    backend_code: str,
    max_files: int | None,
) -> tuple[str, str, list[str]]:
    """Apply virtual file limits using // FILE: markers; return truncated code and warnings."""

    warnings: list[str] = []
    frontend_files = _split_virtual_files(frontend_code)
    backend_files = _split_virtual_files(backend_code)

    total_files = len(frontend_files) + len(backend_files)

    if max_files is not None and max_files >= 0 and total_files > max_files:
        # Reserve at least one slot for backend code
        allowed_frontend = max(max_files - len(backend_files), 0)
        warnings.append(
            f"Truncated virtual files from {total_files} to {max_files} due to tier max_files limit"
        )
        frontend_files = frontend_files[:allowed_frontend] if allowed_frontend else []

    truncated_frontend = "\n\n".join(frontend_files) if frontend_files else ""
    truncated_backend = "\n\n".join(backend_files) if backend_files else ""

    return truncated_frontend, truncated_backend, warnings


def _type_evaporation_scan_sync(
    frontend_code: str,
    backend_code: str,
    frontend_file: str = "frontend.ts",
    backend_file: str = "backend.py",
    enable_pro_features: bool = False,
    enable_enterprise_features: bool = False,
    frontend_only: bool = False,
    max_files: int | None = None,
) -> TypeEvaporationResultModel:
    """
    Synchronous implementation of cross-file type evaporation analysis.

    [20251229_FEATURE] v3.0.4 - Ninja Warrior Stage 3.1 Type System Evaporation
    [20251226_FEATURE] v3.3.0 - Added Pro/Enterprise tier capabilities
    """
    try:
        # Apply virtual file limits (// FILE: markers) before analysis
        frontend_code, backend_code, warnings = _enforce_file_limits(
            frontend_code, backend_code, max_files
        )

        # [20251230_FIX][tiering] Community tier is frontend-only per capability matrix.
        if frontend_only:
            from code_scalpel.security.type_safety.type_evaporation_detector import (
                TypeEvaporationDetector,
            )

            detector = TypeEvaporationDetector()
            frontend_result = detector.analyze(frontend_code, frontend_file)

            all_vulns: list[VulnerabilityInfo] = [
                VulnerabilityInfo(
                    type=f"[Frontend] {v.risk_type.name}",
                    cwe=v.cwe_id,
                    severity=v.severity.lower(),
                    line=v.location[0],
                    description=v.description,
                )
                for v in frontend_result.vulnerabilities
            ]

            # Initialize Pro/Enterprise fields with defaults
            implicit_any_count = 0
            network_boundaries: list[dict[str, Any]] = []
            boundary_violations: list[dict[str, Any]] = []
            library_boundaries: list[dict[str, Any]] = []
            json_parse_locations: list[dict[str, Any]] = []

            # Pro-tier helper analyses are frontend-only; safe to run in this mode when enabled.
            if enable_pro_features:
                implicit_any_count = _detect_implicit_any(frontend_code)
                network_boundaries = _detect_network_boundaries(frontend_code)
                library_boundaries = _detect_library_boundaries(frontend_code)
                json_parse_locations = _detect_json_parse_locations(frontend_code)
                boundary_violations = _detect_boundary_violations(
                    frontend_code, frontend_result
                )

            return TypeEvaporationResultModel(
                success=True,
                frontend_vulnerabilities=len(frontend_result.vulnerabilities),
                backend_vulnerabilities=0,
                cross_file_issues=0,
                matched_endpoints=[],
                vulnerabilities=all_vulns,
                summary=frontend_result.summary(),
                # Pro tier fields
                implicit_any_count=implicit_any_count,
                network_boundaries=network_boundaries,
                boundary_violations=boundary_violations,
                library_boundaries=library_boundaries,
                json_parse_locations=json_parse_locations,
                # Enterprise tier fields
                generated_schemas=[],
                validation_code=None,
                schema_coverage=None,
                pydantic_models=[],
                api_contract=None,
                # [20251231_FEATURE] v1.0 - New Enterprise fields (defaults for frontend-only)
                remediation_suggestions=[],
                custom_rule_violations=[],
                compliance_report=None,
                warnings=warnings,
            )

        from code_scalpel.security.type_safety import (
            analyze_type_evaporation_cross_file,
        )

        result = analyze_type_evaporation_cross_file(
            frontend_code, backend_code, frontend_file, backend_file
        )

        all_vulns: list[VulnerabilityInfo] = []

        # Add frontend vulnerabilities
        for v in result.frontend_result.vulnerabilities:
            all_vulns.append(
                VulnerabilityInfo(
                    type=f"[Frontend] {v.risk_type.name}",
                    cwe=v.cwe_id,
                    severity=v.severity.lower(),
                    line=v.location[0],
                    description=v.description,
                )
            )

        # Add backend vulnerabilities
        for v in result.backend_vulnerabilities:
            all_vulns.append(
                VulnerabilityInfo(
                    type=f"[Backend] {v.vulnerability_type}",
                    cwe=v.cwe_id,
                    severity=getattr(v, "severity", "high"),
                    line=v.sink_location[0] if v.sink_location else None,
                    description=getattr(v, "description", ""),
                )
            )

        # Add cross-file issues
        for v in result.cross_file_issues:
            all_vulns.append(
                VulnerabilityInfo(
                    type=f"[Cross-File] {v.risk_type.name}",
                    cwe=v.cwe_id,
                    severity=v.severity.lower(),
                    line=v.location[0],
                    description=v.description,
                )
            )

        matched = [
            f"{endpoint}: TS line {ts_line}  Python line {py_line}"
            for endpoint, ts_line, py_line in result.matched_endpoints
        ]

        # Initialize Pro/Enterprise fields with defaults
        implicit_any_count = 0
        network_boundaries: list[dict[str, Any]] = []
        boundary_violations: list[dict[str, Any]] = []
        library_boundaries: list[dict[str, Any]] = []
        json_parse_locations: list[dict[str, Any]] = []
        generated_schemas: list[dict[str, Any]] = []
        validation_code: str | None = None
        schema_coverage: float | None = None
        pydantic_models: list[dict[str, Any]] = []
        api_contract: dict[str, Any] | None = None

        # [20251226_FEATURE] Pro tier: Network boundary and implicit any detection
        if enable_pro_features:
            # Detect implicit any from .json() calls without type annotation
            implicit_any_count = _detect_implicit_any(frontend_code)

            # Detect network boundaries (fetch, axios, XMLHttpRequest)
            network_boundaries = _detect_network_boundaries(frontend_code)

            # Detect library call boundaries
            library_boundaries = _detect_library_boundaries(frontend_code)

            # Detect JSON.parse without validation
            json_parse_locations = _detect_json_parse_locations(frontend_code)

            # Track boundary violations (unvalidated type assertions at boundaries)
            boundary_violations = _detect_boundary_violations(
                frontend_code, result.frontend_result
            )

        # [20251226_FEATURE] Enterprise tier: Schema generation and contract validation
        if enable_enterprise_features:
            # Generate Zod schemas for matched endpoints
            generated_schemas = _generate_zod_schemas(
                result.frontend_result.type_definitions, result.matched_endpoints
            )

            # Generate complete validation code
            if generated_schemas:
                validation_code = _generate_validation_code(generated_schemas)

            # Generate Pydantic models for backend
            pydantic_models = _generate_pydantic_models(
                result.frontend_result.type_definitions, result.matched_endpoints
            )

            # Calculate schema coverage
            if result.matched_endpoints:
                covered = len(generated_schemas)
                total = len(result.matched_endpoints)
                schema_coverage = covered / total if total > 0 else 0.0

            # API contract validation
            api_contract = _validate_api_contract(
                result.frontend_result,
                result.backend_vulnerabilities,
                result.matched_endpoints,
            )

            # [20251231_FEATURE] v1.0 - New Enterprise features
            # Generate automated remediation suggestions
            remediation_suggestions = _generate_remediation_suggestions(
                all_vulns,
                generated_schemas,
                pydantic_models,
            )

            # Check custom type rules
            custom_rule_violations = _check_custom_type_rules(
                frontend_code,
                backend_code,
            )

            # Generate compliance report
            compliance_report = _generate_type_compliance_report(
                all_vulns,
                api_contract,
                custom_rule_violations,
                generated_schemas,
            )
        else:
            remediation_suggestions = []
            custom_rule_violations = []
            compliance_report = None

        return TypeEvaporationResultModel(
            success=True,
            frontend_vulnerabilities=len(result.frontend_result.vulnerabilities),
            backend_vulnerabilities=len(result.backend_vulnerabilities),
            cross_file_issues=len(result.cross_file_issues),
            matched_endpoints=matched,
            vulnerabilities=all_vulns,
            summary=result.summary(),
            # Pro tier fields
            implicit_any_count=implicit_any_count,
            network_boundaries=network_boundaries,
            boundary_violations=boundary_violations,
            library_boundaries=library_boundaries,
            json_parse_locations=json_parse_locations,
            # Enterprise tier fields
            generated_schemas=generated_schemas,
            validation_code=validation_code,
            schema_coverage=schema_coverage,
            pydantic_models=pydantic_models,
            api_contract=api_contract,
            # [20251231_FEATURE] v1.0 - New Enterprise fields
            remediation_suggestions=remediation_suggestions,
            custom_rule_violations=custom_rule_violations,
            compliance_report=compliance_report,
            warnings=warnings,
        )

    except ImportError as e:
        return TypeEvaporationResultModel(
            success=False,
            error=f"Type evaporation detector not available: {str(e)}.",
            warnings=[],
        )
    except Exception as e:
        return TypeEvaporationResultModel(
            success=False,
            error=f"Analysis failed: {str(e)}.",
            warnings=[],
        )


# =============================================================================
# [20251226_FEATURE] Pro tier helper functions for type_evaporation_scan
# =============================================================================


def _detect_implicit_any(code: str) -> int:
    """
    [20251226_FEATURE] Detect implicit any from untyped .json() responses.

    Patterns detected:
    - response.json() without type annotation
    - .then(r => r.json()) - implicit any in callback
    - await fetch().json() without type cast
    """
    count = 0
    lines = code.splitlines()

    for line in lines:
        # .json() call without 'as Type' or ': Type' annotation
        if ".json()" in line:
            # Check if there's a type annotation or cast
            if (
                " as " not in line and ": " not in line.split("=")[0]
                if "=" in line
                else True
            ):
                count += 1

        # Untyped destructuring from response
        if re.search(r"const\s+\{.*\}\s*=\s*.*\.json\(\)", line):
            if " as " not in line:
                count += 1

        # response.data without type (axios pattern)
        if ".data" in line and "response" in line.lower():
            if (
                " as " not in line and ":" not in line.split("=")[0]
                if "=" in line
                else True
            ):
                count += 1

    return count


def _detect_network_boundaries(code: str) -> list[dict[str, Any]]:
    """
    [20251226_FEATURE] Detect network call boundaries where types evaporate.

    Detects: fetch(), axios.*, XMLHttpRequest
    """
    boundaries: list[dict[str, Any]] = []
    lines = code.splitlines()

    network_patterns = [
        (r"\bfetch\s*\(", "fetch"),
        (r"\baxios\.(get|post|put|delete|patch)\s*\(", "axios"),
        (r"\baxios\s*\(", "axios"),
        (r"new\s+XMLHttpRequest\s*\(", "XMLHttpRequest"),
        (r"XMLHttpRequest\s*\(\s*\)", "XMLHttpRequest"),
    ]

    for i, line in enumerate(lines, 1):
        for pattern, boundary_type in network_patterns:
            if re.search(pattern, line, re.IGNORECASE):
                boundaries.append(
                    {
                        "line": i,
                        "type": "network_call",
                        "boundary_type": boundary_type,
                        "code_snippet": line.strip()[:100],
                        "risk": "high" if ".json()" in line else "medium",
                    }
                )
                break  # One match per line

    return boundaries


def _detect_library_boundaries(code: str) -> list[dict[str, Any]]:
    """
    [20251226_FEATURE] Detect library call boundaries where external data enters.

    Detects: localStorage, sessionStorage, postMessage, WebSocket
    """
    boundaries: list[dict[str, Any]] = []
    lines = code.splitlines()

    library_patterns = [
        (r"localStorage\.(getItem|setItem)", "localStorage"),
        (r"sessionStorage\.(getItem|setItem)", "sessionStorage"),
        (r"\.postMessage\s*\(", "postMessage"),
        (r"new\s+WebSocket\s*\(", "WebSocket"),
        (r"\.addEventListener\s*\(\s*['\"]message['\"]", "message_event"),
    ]

    for i, line in enumerate(lines, 1):
        for pattern, lib_type in library_patterns:
            if re.search(pattern, line):
                boundaries.append(
                    {
                        "line": i,
                        "type": "library_boundary",
                        "library": lib_type,
                        "code_snippet": line.strip()[:100],
                    }
                )
                break

    return boundaries


def _detect_json_parse_locations(code: str) -> list[dict[str, Any]]:
    """
    [20251226_FEATURE] Detect JSON.parse() calls without validation.
    """
    locations: list[dict[str, Any]] = []
    lines = code.splitlines()

    for i, line in enumerate(lines, 1):
        if "JSON.parse" in line:
            has_validation = (
                " as " in line or "schema" in line.lower() or "validate" in line.lower()
            )
            locations.append(
                {
                    "line": i,
                    "code_snippet": line.strip()[:100],
                    "validated": has_validation,
                    "risk": "low" if has_validation else "high",
                }
            )

    return locations


def _detect_boundary_violations(
    code: str, frontend_result: Any
) -> list[dict[str, Any]]:
    """
    [20251226_FEATURE] Detect type violations at serialization boundaries.

    A violation occurs when:
    - Type assertion immediately follows network/JSON call
    - No runtime validation between boundary and type usage
    """
    violations: list[dict[str, Any]] = []

    # Check type assertions from the frontend result
    for type_name, line_num, context in getattr(frontend_result, "type_assertions", []):
        # Skip safe built-in types
        if type_name in ("string", "number", "boolean", "any", "unknown"):
            continue

        # Check if this assertion is near a boundary
        if ".json()" in context or "JSON.parse" in context or ".data" in context:
            violations.append(
                {
                    "line": line_num,
                    "type": type_name,
                    "context": context[:100],
                    "violation": f"Unvalidated type assertion '{type_name}' at boundary",
                    "risk": "high",
                }
            )

    return violations


# =============================================================================
# [20251226_FEATURE] Enterprise tier helper functions for type_evaporation_scan
# =============================================================================


def _generate_zod_schemas(
    type_definitions: dict[str, tuple[int, str]],
    matched_endpoints: list[tuple[str, int, int]],
) -> list[dict[str, Any]]:
    """
    [20251226_FEATURE] Generate Zod validation schemas from TypeScript types.
    """
    schemas: list[dict[str, Any]] = []

    for type_name, (line, definition) in type_definitions.items():
        # Skip internal/utility types
        if type_name.startswith("_") or type_name in ("Props", "State"):
            continue

        # Parse type definition and generate Zod schema
        zod_schema = _ts_type_to_zod(type_name, definition)

        if zod_schema:
            # Find associated endpoint if any
            endpoint = None
            for ep, ts_line, py_line in matched_endpoints:
                if abs(ts_line - line) < 20:  # Within 20 lines
                    endpoint = ep
                    break

            schemas.append(
                {
                    "type_name": type_name,
                    "endpoint": endpoint,
                    "schema_type": "zod",
                    "schema": zod_schema,
                    "source_line": line,
                }
            )

    return schemas


def _ts_type_to_zod(type_name: str, definition: str) -> str | None:
    """
    [20251226_FEATURE] Convert TypeScript type definition to Zod schema.
    """
    # Handle interface
    if definition.startswith("interface"):
        fields = _extract_interface_fields(definition)
        if not fields:
            return None

        field_schemas = []
        for field_name, field_type in fields.items():
            zod_type = _ts_primitive_to_zod(field_type)
            field_schemas.append(f"  {field_name}: {zod_type}")

        return (
            f"const {type_name}Schema = z.object({{\n"
            + ",\n".join(field_schemas)
            + "\n});"
        )

    # Handle type alias with union
    union_match = re.search(r"type\s+\w+\s*=\s*(.+)", definition)
    if union_match:
        union_value = union_match.group(1).strip().rstrip(";")

        # String literal union: 'admin' | 'user'
        if "'" in union_value or '"' in union_value:
            literals = re.findall(r"['\"]([^'\"]+)['\"]", union_value)
            if literals:
                enum_values = ", ".join(f'"{lit}"' for lit in literals)
                return f"const {type_name}Schema = z.enum([{enum_values}]);"

        # Primitive union: string | number
        parts = [p.strip() for p in union_value.split("|")]
        if all(
            p in ("string", "number", "boolean", "null", "undefined") for p in parts
        ):
            zod_parts = [_ts_primitive_to_zod(p) for p in parts]
            return f"const {type_name}Schema = z.union([{', '.join(zod_parts)}]);"

    return None


def _extract_interface_fields(definition: str) -> dict[str, str]:
    """Extract field name -> type mapping from interface definition."""
    fields: dict[str, str] = {}

    # Match field: type patterns
    field_pattern = re.compile(r"(\w+)\s*\??\s*:\s*([^;,}]+)")
    for match in field_pattern.finditer(definition):
        field_name = match.group(1)
        field_type = match.group(2).strip()
        if field_name not in ("interface", "type"):
            fields[field_name] = field_type

    return fields


def _ts_primitive_to_zod(ts_type: str) -> str:
    """Convert TypeScript primitive to Zod validator."""
    ts_type = ts_type.strip()
    mapping = {
        "string": "z.string()",
        "number": "z.number()",
        "boolean": "z.boolean()",
        "null": "z.null()",
        "undefined": "z.undefined()",
        "any": "z.any()",
        "unknown": "z.unknown()",
    }

    # Handle arrays
    if ts_type.endswith("[]"):
        inner = ts_type[:-2]
        return f"z.array({_ts_primitive_to_zod(inner)})"

    # Handle optional
    if ts_type.endswith("?"):
        inner = ts_type[:-1]
        return f"{_ts_primitive_to_zod(inner)}.optional()"

    return mapping.get(ts_type, "z.unknown()")


def _generate_validation_code(schemas: list[dict[str, Any]]) -> str:
    """
    [20251226_FEATURE] Generate complete validation code from schemas.
    """
    lines = [
        "// Auto-generated validation code by Code Scalpel",
        "// [20251226_FEATURE] Enterprise tier schema generation",
        "",
        "import { z } from 'zod';",
        "",
    ]

    for schema in schemas:
        lines.append(f"// Schema for {schema['type_name']}")
        lines.append(schema["schema"])
        lines.append("")

        # Add validation helper
        type_name = schema["type_name"]
        lines.append(
            f"export function validate{type_name}(data: unknown): {type_name} {{"
        )
        lines.append(f"  return {type_name}Schema.parse(data);")
        lines.append("}")
        lines.append("")

    # Add safe fetch wrapper
    lines.extend(
        [
            "// Safe fetch wrapper with validation",
            "export async function safeFetch<T>(url: string, schema: z.Schema<T>): Promise<T> {",
            "  const response = await fetch(url);",
            "  const data = await response.json();",
            "  return schema.parse(data);",
            "}",
        ]
    )

    return "\n".join(lines)


def _generate_pydantic_models(
    type_definitions: dict[str, tuple[int, str]],
    matched_endpoints: list[tuple[str, int, int]],
) -> list[dict[str, Any]]:
    """
    [20251226_FEATURE] Generate Pydantic models for Python backend validation.
    """
    models: list[dict[str, Any]] = []

    for type_name, (line, definition) in type_definitions.items():
        if type_name.startswith("_"):
            continue

        pydantic_model = _ts_type_to_pydantic(type_name, definition)

        if pydantic_model:
            models.append(
                {
                    "type_name": type_name,
                    "model_type": "pydantic",
                    "model": pydantic_model,
                    "source_line": line,
                }
            )

    return models


def _ts_type_to_pydantic(type_name: str, definition: str) -> str | None:
    """
    [20251226_FEATURE] Convert TypeScript type to Pydantic model.
    """
    # Handle interface
    if definition.startswith("interface"):
        fields = _extract_interface_fields(definition)
        if not fields:
            return None

        field_defs = []
        for field_name, field_type in fields.items():
            py_type = _ts_to_python_type(field_type)
            field_defs.append(f"    {field_name}: {py_type}")

        return f"class {type_name}(BaseModel):\n" + "\n".join(field_defs)

    # Handle type alias with union (becomes Literal)
    union_match = re.search(r"type\s+\w+\s*=\s*(.+)", definition)
    if union_match:
        union_value = union_match.group(1).strip().rstrip(";")

        if "'" in union_value or '"' in union_value:
            literals = re.findall(r"['\"]([^'\"]+)['\"]", union_value)
            if literals:
                literal_values = ", ".join(f'"{lit}"' for lit in literals)
                return f"{type_name} = Literal[{literal_values}]"

    return None


def _ts_to_python_type(ts_type: str) -> str:
    """Convert TypeScript type to Python type annotation."""
    ts_type = ts_type.strip()
    mapping = {
        "string": "str",
        "number": "float",
        "boolean": "bool",
        "null": "None",
        "undefined": "None",
        "any": "Any",
    }

    if ts_type.endswith("[]"):
        inner = ts_type[:-2]
        return f"list[{_ts_to_python_type(inner)}]"

    return mapping.get(ts_type, "Any")


def _validate_api_contract(
    frontend_result: Any,
    backend_vulnerabilities: list[Any],
    matched_endpoints: list[tuple[str, int, int]],
) -> dict[str, Any]:
    """
    [20251226_FEATURE] Validate API contract between frontend and backend.
    """
    contract: dict[str, Any] = {
        "total_endpoints": len(matched_endpoints),
        "validated_endpoints": 0,
        "violations": [],
        "recommendations": [],
    }

    # Check each matched endpoint for contract violations
    for endpoint, ts_line, py_line in matched_endpoints:
        violation_found = False

        # Check if backend has input validation vulnerability
        for vuln in backend_vulnerabilities:
            vuln_line = (
                getattr(vuln, "sink_location", (0,))[0]
                if hasattr(vuln, "sink_location") and vuln.sink_location
                else 0
            )
            if abs(vuln_line - py_line) < 10:
                contract["violations"].append(
                    {
                        "endpoint": endpoint,
                        "type": "missing_validation",
                        "frontend_line": ts_line,
                        "backend_line": py_line,
                        "description": f"Backend at {endpoint} lacks input validation",
                    }
                )
                violation_found = True
                break

        if not violation_found:
            contract["validated_endpoints"] += 1

    # Generate recommendations
    if contract["violations"]:
        contract["recommendations"].append(
            "Add runtime validation (Pydantic/marshmallow) to backend endpoints"
        )
        contract["recommendations"].append(
            "Use Zod schemas on frontend for response validation"
        )

    return contract


# =============================================================================
# [20251231_FEATURE] v1.0 - Missing Enterprise tier helper functions
# =============================================================================


def _generate_remediation_suggestions(
    vulnerabilities: list[Any],
    generated_schemas: list[dict[str, Any]],
    pydantic_models: list[dict[str, Any]],
) -> list[dict[str, Any]]:
    """
    [20251231_FEATURE] Generate automated remediation suggestions for type evaporation issues.

    Analyzes vulnerabilities and provides actionable fix suggestions with code examples.
    """
    suggestions: list[dict[str, Any]] = []

    for vuln in vulnerabilities:
        vuln_type = (
            getattr(vuln, "type", str(vuln)) if hasattr(vuln, "type") else str(vuln)
        )
        vuln_line = getattr(vuln, "line", 0) if hasattr(vuln, "line") else 0
        vuln_desc = (
            getattr(vuln, "description", "") if hasattr(vuln, "description") else ""
        )

        suggestion: dict[str, Any] = {
            "vulnerability_type": vuln_type,
            "line": vuln_line,
            "priority": (
                "high"
                if "Backend" in str(vuln_type) or "Cross-File" in str(vuln_type)
                else "medium"
            ),
            "fixes": [],
        }

        # Generate specific fixes based on vulnerability type
        if "implicit" in str(vuln_type).lower() or "any" in str(vuln_type).lower():
            suggestion["fixes"].append(
                {
                    "type": "add_type_annotation",
                    "description": "Add explicit type annotation to prevent implicit any",
                    "example": "const data: ResponseType = await response.json();",
                }
            )
            suggestion["fixes"].append(
                {
                    "type": "add_runtime_validation",
                    "description": "Add Zod schema validation at parse boundary",
                    "example": "const data = ResponseSchema.parse(await response.json());",
                }
            )

        if "Backend" in str(vuln_type):
            suggestion["fixes"].append(
                {
                    "type": "add_pydantic_validation",
                    "description": "Add Pydantic model for request validation",
                    "example": "def endpoint(request: RequestModel) -> ResponseModel:",
                }
            )
            suggestion["fixes"].append(
                {
                    "type": "add_input_sanitization",
                    "description": "Validate and sanitize all external input",
                    "example": "validated_data = RequestModel.model_validate(request.json())",
                }
            )

        if "network" in str(vuln_type).lower() or "fetch" in str(vuln_desc).lower():
            suggestion["fixes"].append(
                {
                    "type": "wrap_fetch_with_validation",
                    "description": "Use validated fetch wrapper",
                    "example": "const data = await safeFetch(url, ResponseSchema);",
                }
            )

        if "Cross-File" in str(vuln_type):
            suggestion["fixes"].append(
                {
                    "type": "sync_frontend_backend_types",
                    "description": "Ensure frontend and backend types match",
                    "example": "Generate shared types from OpenAPI spec or use code generation",
                }
            )

        # Link to generated schemas if available
        if generated_schemas:
            suggestion["available_schemas"] = [
                s["type_name"] for s in generated_schemas
            ]
        if pydantic_models:
            suggestion["available_models"] = [m["type_name"] for m in pydantic_models]

        suggestions.append(suggestion)

    return suggestions


def _check_custom_type_rules(
    frontend_code: str,
    backend_code: str,
    custom_rules: list[dict[str, Any]] | None = None,
) -> list[dict[str, Any]]:
    """
    [20251231_FEATURE] Check code against custom type rules.

    Supports user-defined rules for type safety enforcement.
    Default rules check for common type safety anti-patterns.
    """
    violations: list[dict[str, Any]] = []

    # Default rules if none provided
    default_rules = [
        {
            "id": "NO_EXPLICIT_ANY",
            "pattern": r":\s*any\b",
            "message": "Avoid explicit 'any' type - use specific types or 'unknown'",
            "severity": "warning",
            "applies_to": "frontend",
        },
        {
            "id": "NO_TYPE_ASSERTION_ANY",
            "pattern": r"as\s+any\b",
            "message": "Avoid 'as any' type assertion - loses type safety",
            "severity": "error",
            "applies_to": "frontend",
        },
        {
            "id": "NO_NON_NULL_ASSERTION",
            "pattern": r"\w+!(?:\.|$|\))",
            "message": "Avoid non-null assertion (!) - use proper null checking",
            "severity": "warning",
            "applies_to": "frontend",
        },
        {
            "id": "REQUIRE_TYPE_ON_JSON",
            "pattern": r"\.json\(\)\s*(?:;|$|\))",
            "message": ".json() should have type annotation or validation",
            "severity": "error",
            "applies_to": "frontend",
        },
        {
            "id": "NO_UNTYPED_REQUEST",
            "pattern": r"request\.get_json\(\)\s*(?:\n|$|#)",
            "message": "request.get_json() should be validated with Pydantic",
            "severity": "error",
            "applies_to": "backend",
        },
        {
            "id": "NO_RAW_DICT_ACCESS",
            "pattern": r"request\.json\[['\"]",
            "message": "Direct dict access on request.json - use validated model",
            "severity": "warning",
            "applies_to": "backend",
        },
    ]

    rules = custom_rules if custom_rules else default_rules

    for rule in rules:
        code_to_check = ""
        if rule.get("applies_to") == "frontend":
            code_to_check = frontend_code
        elif rule.get("applies_to") == "backend":
            code_to_check = backend_code
        else:
            code_to_check = frontend_code + "\n" + backend_code

        pattern = rule.get("pattern", "")
        if pattern:
            for i, line in enumerate(code_to_check.split("\n"), 1):
                if re.search(pattern, line):
                    violations.append(
                        {
                            "rule_id": rule.get("id", "CUSTOM"),
                            "line": i,
                            "code": line.strip()[:80],
                            "message": rule.get("message", "Custom rule violation"),
                            "severity": rule.get("severity", "warning"),
                            "applies_to": rule.get("applies_to", "both"),
                        }
                    )

    return violations


def _generate_type_compliance_report(
    vulnerabilities: list[Any],
    api_contract: dict[str, Any] | None,
    custom_rule_violations: list[dict[str, Any]],
    generated_schemas: list[dict[str, Any]],
) -> dict[str, Any]:
    """
    [20251231_FEATURE] Generate type compliance validation report.

    Assesses overall type safety compliance and provides scoring.
    """
    report: dict[str, Any] = {
        "compliance_score": 100.0,
        "grade": "A",
        "findings": [],
        "summary": {
            "total_issues": 0,
            "critical": 0,
            "high": 0,
            "medium": 0,
            "low": 0,
        },
        "recommendations": [],
    }

    # Count vulnerabilities by severity
    for vuln in vulnerabilities:
        severity = (
            getattr(vuln, "severity", "medium")
            if hasattr(vuln, "severity")
            else "medium"
        )
        severity = severity.lower() if isinstance(severity, str) else "medium"

        report["summary"]["total_issues"] += 1

        if severity in ("critical", "high"):
            report["summary"]["high"] += 1
            report["compliance_score"] -= 15
            report["findings"].append(
                {
                    "type": "vulnerability",
                    "severity": severity,
                    "description": str(getattr(vuln, "description", vuln)),
                }
            )
        elif severity == "medium":
            report["summary"]["medium"] += 1
            report["compliance_score"] -= 5
        else:
            report["summary"]["low"] += 1
            report["compliance_score"] -= 2

    # Factor in custom rule violations
    for violation in custom_rule_violations:
        report["summary"]["total_issues"] += 1
        severity = violation.get("severity", "warning")

        if severity == "error":
            report["summary"]["high"] += 1
            report["compliance_score"] -= 10
        else:
            report["summary"]["medium"] += 1
            report["compliance_score"] -= 3

        report["findings"].append(
            {
                "type": "rule_violation",
                "rule_id": violation.get("rule_id"),
                "severity": severity,
                "message": violation.get("message"),
            }
        )

    # Factor in API contract compliance
    if api_contract:
        total_endpoints = api_contract.get("total_endpoints", 0)
        validated = api_contract.get("validated_endpoints", 0)

        if total_endpoints > 0:
            contract_score = (validated / total_endpoints) * 20
            report["compliance_score"] = min(
                100, report["compliance_score"] + contract_score - 10
            )

        for violation in api_contract.get("violations", []):
            report["findings"].append(
                {
                    "type": "contract_violation",
                    "severity": "high",
                    "endpoint": violation.get("endpoint"),
                    "description": violation.get("description"),
                }
            )

    # Bonus for having generated schemas
    if generated_schemas:
        report["compliance_score"] = min(100, report["compliance_score"] + 5)

    # Ensure score is within bounds
    report["compliance_score"] = max(0, min(100, report["compliance_score"]))

    # Assign grade
    score = report["compliance_score"]
    if score >= 90:
        report["grade"] = "A"
    elif score >= 80:
        report["grade"] = "B"
    elif score >= 70:
        report["grade"] = "C"
    elif score >= 60:
        report["grade"] = "D"
    else:
        report["grade"] = "F"

    # Generate recommendations based on findings
    if report["summary"]["high"] > 0:
        report["recommendations"].append(
            "CRITICAL: Address high-severity type safety issues immediately"
        )
    if report["summary"]["medium"] > 3:
        report["recommendations"].append(
            "Consider adding runtime validation to reduce medium-severity issues"
        )
    if not generated_schemas:
        report["recommendations"].append(
            "Generate Zod schemas for frontend type validation"
        )
    if api_contract and api_contract.get("violations"):
        report["recommendations"].append(
            "Align frontend/backend types to resolve contract violations"
        )

    return report


@mcp.tool()
async def type_evaporation_scan(
    frontend_code: str,
    backend_code: str,
    frontend_file: str = "frontend.ts",
    backend_file: str = "backend.py",
) -> TypeEvaporationResultModel:
    """
    Detect Type System Evaporation vulnerabilities across TypeScript frontend and Python backend.

    [20251229_FEATURE] v3.0.4 - Ninja Warrior Stage 3.1
    [20251226_FEATURE] v3.3.0 - Added Pro/Enterprise tier capabilities

    Type System Evaporation occurs when TypeScript compile-time types (like union types)
    are trusted but evaporate at serialization boundaries (JSON.stringify).

    This tool analyzes:
    - TypeScript frontend: unsafe type assertions, DOM input, fetch boundaries
    - Python backend: unvalidated external input in HTTP responses
    - Cross-file: correlates TS fetch() endpoints with Python @app.route() decorators

    **Pro tier adds:**
    - Implicit any detection from untyped .json() responses
    - Network boundary analysis (fetch, axios, XMLHttpRequest)
    - Library boundary tracking (localStorage, postMessage)
    - JSON.parse location detection

    **Enterprise tier adds:**
    - Zod schema generation for TypeScript validation
    - Pydantic model generation for Python backend
    - API contract validation
    - Schema coverage metrics

    Example vulnerability:
        Frontend: type Role = 'admin' | 'user'; const role = input.value as Role;
        Backend: role = request.get_json()['role']  # No validation!

    The TypeScript type provides NO runtime enforcement - attacker can send any value.

    Args:
        frontend_code: TypeScript/JavaScript frontend code
        backend_code: Python backend code
        frontend_file: Frontend filename for error messages
        backend_file: Backend filename for error messages

    Returns:
        TypeEvaporationResultModel with frontend, backend, and cross-file vulnerabilities.
    """
    # [20251226_FEATURE] Tier-aware feature enablement
    tier = _get_current_tier()
    caps = get_tool_capabilities("type_evaporation_scan", tier) or {}
    cap_set = set(caps.get("capabilities", []))
    limits = caps.get("limits", {}) or {}
    frontend_only = bool(limits.get("frontend_only", False))
    raw_max_files = limits.get("max_files")
    try:
        max_files = int(raw_max_files) if raw_max_files is not None else None
    except (TypeError, ValueError):
        max_files = None

    # Pro features: implicit any, network boundaries, library boundaries
    enable_pro = bool(
        {
            "implicit_any_tracing",
            "network_boundary_analysis",
            "library_boundary_analysis",
        }
        & cap_set
    )

    # Enterprise features: schema generation, contract validation
    enable_enterprise = bool(
        {
            "runtime_validation_generation",
            "zod_schema_generation",
            "api_contract_validation",
        }
        & cap_set
    )

    return await asyncio.to_thread(
        _type_evaporation_scan_sync,
        frontend_code,
        backend_code,
        frontend_file,
        backend_file,
        enable_pro,
        enable_enterprise,
        frontend_only,
        max_files,
    )


# =============================================================================
# [20251219_FEATURE] v3.0.4 - A06 Vulnerable Components Detection (OSV API)
# [20251220_FEATURE] v3.0.5 - Added DependencyInfo/DependencyVulnerability models for test compatibility
# =============================================================================


class DependencyVulnerability(BaseModel):
    """A vulnerability associated with a specific dependency.

    [20251220_FEATURE] v3.0.5 - Model for per-dependency vulnerability tracking.
    """

    id: str = Field(description="Vulnerability ID (OSV, CVE, or GHSA)")
    summary: str = Field(
        default="", description="Brief description of the vulnerability"
    )
    severity: str = Field(
        default="UNKNOWN", description="Severity: CRITICAL, HIGH, MEDIUM, LOW, UNKNOWN"
    )
    package: str = Field(description="Name of the vulnerable package")
    vulnerable_version: str = Field(description="Version that is vulnerable")
    fixed_version: str | None = Field(
        default=None, description="First version that fixes this vulnerability"
    )


class DependencyInfo(BaseModel):
    """Information about a scanned dependency.

    [20251220_FEATURE] v3.0.5 - Model for tracking individual dependencies and their vulnerabilities.
    [20251229_FEATURE] v3.3.1 - Added reachability analysis and license compliance fields.
    [20251231_FEATURE] v3.3.1 - Added supply chain risk scoring per roadmap v1.0.
    """

    name: str = Field(description="Package name")
    version: str = Field(description="Package version (may be '*' for unspecified)")
    ecosystem: str = Field(description="Package ecosystem: PyPI, npm, Maven, etc.")
    vulnerabilities: list[DependencyVulnerability] = Field(
        default_factory=list, description="Vulnerabilities affecting this dependency"
    )
    is_imported: bool | None = Field(
        default=None,
        description="Whether package is imported in codebase (Pro tier reachability analysis)",
    )
    license: str | None = Field(
        default=None, description="Package license (Pro tier license compliance)"
    )
    license_compliant: bool | None = Field(
        default=None, description="Whether license matches org policy (Pro tier)"
    )
    typosquatting_risk: bool | None = Field(
        default=None, description="Potential typosquatting detected (Pro tier)"
    )
    supply_chain_risk_score: float | None = Field(
        default=None, description="Supply chain risk score 0.0-1.0 (Pro tier)"
    )
    supply_chain_risk_factors: list[str] | None = Field(
        default=None, description="Factors contributing to supply chain risk (Pro tier)"
    )


class DependencyScanResult(BaseModel):
    """Result of a dependency vulnerability scan with per-dependency details.

    [20251220_FEATURE] v3.0.5 - Comprehensive scan result with dependency-level tracking.
    [20251231_FEATURE] v3.3.1 - Added compliance reporting fields per roadmap v1.0.
    """

    success: bool = Field(description="Whether the scan completed successfully")
    server_version: str = Field(default=__version__, description="Code Scalpel version")
    error: str | None = Field(default=None, description="Error message if failed")
    total_dependencies: int = Field(
        default=0, description="Number of dependencies found"
    )
    vulnerable_count: int = Field(
        default=0, description="Number of dependencies with vulnerabilities"
    )
    total_vulnerabilities: int = Field(
        default=0, description="Total number of vulnerabilities found"
    )
    severity_summary: dict[str, int] = Field(
        default_factory=dict, description="Count of vulnerabilities by severity"
    )
    dependencies: list[DependencyInfo] = Field(
        default_factory=list,
        description="All scanned dependencies with their vulnerabilities",
    )
    # [20251231_FEATURE] v3.3.1 - Enterprise compliance reporting
    compliance_report: dict[str, Any] | None = Field(
        default=None,
        description="Compliance report for SOC2/ISO standards (Enterprise tier)",
    )
    policy_violations: list[dict[str, Any]] | None = Field(
        default=None, description="Policy violations detected (Enterprise tier)"
    )
    errors: list[str] = Field(
        default_factory=list,
        description="Non-fatal errors/warnings encountered during scan (e.g. tier truncation warnings)",
    )


class VulnerabilityFindingModel(BaseModel):
    """A vulnerability found in a dependency."""

    id: str = Field(description="OSV vulnerability ID (e.g., GHSA-xxxx-xxxx-xxxx)")
    cve_id: str | None = Field(default=None, description="CVE ID if available")
    severity: str = Field(
        default="UNKNOWN", description="Severity: CRITICAL, HIGH, MEDIUM, LOW, UNKNOWN"
    )
    package_name: str = Field(description="Name of the vulnerable package")
    package_version: str = Field(description="Version of the vulnerable package")
    ecosystem: str = Field(description="Package ecosystem (npm, Maven, PyPI)")
    summary: str = Field(
        default="", description="Brief description of the vulnerability"
    )
    fixed_versions: list[str] = Field(
        default_factory=list, description="Versions that fix this vulnerability"
    )
    source_file: str = Field(
        default="", description="Dependency file where package was found"
    )


class DependencyScanResultModel(BaseModel):
    """Result of a dependency vulnerability scan."""

    success: bool = Field(description="Whether the scan completed successfully")
    server_version: str = Field(default=__version__, description="Code Scalpel version")
    error: str | None = Field(default=None, description="Error message if failed")
    dependencies_scanned: int = Field(
        default=0, description="Number of dependencies checked"
    )
    vulnerabilities_found: int = Field(
        default=0, description="Number of vulnerabilities found"
    )
    critical_count: int = Field(default=0, description="Number of CRITICAL severity")
    high_count: int = Field(default=0, description="Number of HIGH severity")
    medium_count: int = Field(default=0, description="Number of MEDIUM severity")
    low_count: int = Field(default=0, description="Number of LOW severity")
    findings: list[VulnerabilityFindingModel] = Field(
        default_factory=list, description="Detailed findings"
    )
    errors: list[str] = Field(
        default_factory=list, description="Errors encountered during scan"
    )
    summary: str = Field(default="", description="Human-readable summary")


def _analyze_reachability(project_root: Path) -> set[str]:
    """
    Analyze Python files to find imported packages.

    [20251229_FEATURE] v3.3.1 - Pro tier reachability analysis.

    Args:
        project_root: Path to project directory

    Returns:
        Set of imported package names
    """
    imported_packages: set[str] = set()

    # Find all Python files (excluding common ignore patterns)
    ignore_dirs = {
        ".venv",
        "venv",
        "node_modules",
        ".git",
        "__pycache__",
        "build",
        "dist",
        ".tox",
        ".pytest_cache",
    }

    for py_file in project_root.rglob("*.py"):
        # Skip ignored directories
        if any(ignore_dir in py_file.parts for ignore_dir in ignore_dirs):
            continue

        try:
            content = py_file.read_text(encoding="utf-8", errors="ignore")
            tree = ast.parse(content, filename=str(py_file))

            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        # Extract top-level package name
                        # e.g., "requests.auth" -> "requests"
                        package = alias.name.split(".")[0]
                        imported_packages.add(package)

                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        # Extract top-level package name
                        # e.g., "from urllib.parse import ..." -> "urllib"
                        package = node.module.split(".")[0]
                        imported_packages.add(package)

        except (SyntaxError, UnicodeDecodeError, OSError):
            # Skip files that can't be parsed
            continue

    return imported_packages


def _fetch_package_license(package_name: str, ecosystem: str) -> str | None:
    """
    Fetch license information from package registries.

    [20251229_FEATURE] v3.3.1 - Enterprise tier license compliance.

    Args:
        package_name: Name of the package
        ecosystem: Package ecosystem (PyPI, npm, etc.)

    Returns:
        License string or None if unavailable
    """
    import json
    import urllib.parse
    import urllib.request

    try:
        if ecosystem.lower() == "pypi":
            # Query PyPI JSON API
            url = f"https://pypi.org/pypi/{package_name}/json"
            parsed = urllib.parse.urlparse(url)
            # [20260102_BUGFIX] Restrict registry calls to HTTPS only.
            if parsed.scheme not in {"http", "https"}:
                raise ValueError(f"Unsupported registry scheme: {parsed.scheme}")
            with urllib.request.urlopen(url, timeout=5) as response:
                data = json.loads(response.read().decode())
                return data.get("info", {}).get("license") or "Unknown"

        elif ecosystem.lower() == "npm":
            # Query npm registry API
            url = f"https://registry.npmjs.org/{package_name}/latest"
            parsed = urllib.parse.urlparse(url)
            # [20260102_BUGFIX] Restrict registry calls to HTTPS only.
            if parsed.scheme not in {"http", "https"}:
                raise ValueError(f"Unsupported registry scheme: {parsed.scheme}")
            with urllib.request.urlopen(url, timeout=5) as response:
                data = json.loads(response.read().decode())
                license_info = data.get("license")
                if isinstance(license_info, dict):
                    return license_info.get("type", "Unknown")
                return str(license_info) if license_info else "Unknown"

    except Exception:
        pass

    return None


def _check_license_compliance(license_str: str | None) -> bool:
    """
    Check if license complies with organization policy.

    [20251229_FEATURE] v3.3.1 - Enterprise tier license compliance.

    Args:
        license_str: License string

    Returns:
        True if compliant, False otherwise
    """
    if not license_str or license_str == "Unknown":
        return False

    # Common permissive licenses (generally acceptable)
    permissive_licenses = {
        "mit",
        "apache",
        "bsd",
        "isc",
        "0bsd",
        "wtfpl",
        "apache-2.0",
        "apache 2.0",
        "bsd-3-clause",
        "bsd-2-clause",
        "cc0",
        "unlicense",
        "zlib",
        "mpl-2.0",
    }

    # Copyleft licenses (may require policy review)
    copyleft_licenses = {"gpl", "lgpl", "agpl", "gpl-3.0", "lgpl-3.0", "agpl-3.0"}

    license_lower = license_str.lower()

    # Check if permissive
    if any(perm in license_lower for perm in permissive_licenses):
        return True

    # Flag copyleft as non-compliant (organization may want to review)
    if any(copy in license_lower for copy in copyleft_licenses):
        return False

    # Unknown licenses default to non-compliant (requires review)
    return False


def _check_typosquatting(package_name: str, ecosystem: str) -> bool:
    """
    Check for potential typosquatting by comparing against popular packages.

    [20251229_FEATURE] v3.3.1 - Enterprise tier typosquatting detection.

    Uses Levenshtein distance to detect packages with similar names to popular packages.

    Args:
        package_name: Name of the package to check
        ecosystem: Package ecosystem

    Returns:
        True if typosquatting risk detected
    """
    # Popular package names by ecosystem
    popular_packages = {
        "pypi": {
            "requests",
            "urllib3",
            "certifi",
            "setuptools",
            "pip",
            "wheel",
            "numpy",
            "pandas",
            "scipy",
            "matplotlib",
            "django",
            "flask",
            "pytest",
            "boto3",
            "sqlalchemy",
            "pillow",
            "cryptography",
            "beautifulsoup4",
            "lxml",
            "pyyaml",
            "click",
            "jinja2",
        },
        "npm": {
            "react",
            "lodash",
            "express",
            "axios",
            "webpack",
            "babel",
            "typescript",
            "eslint",
            "prettier",
            "jest",
            "moment",
            "chalk",
            "commander",
            "async",
            "request",
            "underscore",
            "jquery",
            "vue",
        },
    }

    target_packages = popular_packages.get(ecosystem.lower(), set())
    if not target_packages:
        return False

    package_lower = package_name.lower()

    # Check for exact match (legitimate)
    if package_lower in target_packages:
        return False

    # Compute edit distance for similar names
    def levenshtein_distance(s1: str, s2: str) -> int:
        if len(s1) < len(s2):
            return levenshtein_distance(s2, s1)
        if len(s2) == 0:
            return len(s1)

        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row

        return previous_row[-1]

    # Check for suspiciously similar names (edit distance <= 2)
    for popular_pkg in target_packages:
        distance = levenshtein_distance(package_lower, popular_pkg)
        # Threshold: 1-2 character difference suggests typosquatting
        if 0 < distance <= 2 and len(package_lower) > 3:
            return True

    return False


def _calculate_supply_chain_risk(
    package_name: str,
    version: str,
    ecosystem: str,
    vulnerabilities: list,
    is_imported: bool | None,
    typosquatting_risk: bool | None,
    license_compliant: bool | None,
) -> tuple[float, list[str]]:
    """
    Calculate supply chain risk score for a dependency.

    [20251231_FEATURE] v3.3.1 - Pro tier supply chain risk scoring per roadmap v1.0.

    Risk factors:
    - Known vulnerabilities (weighted by severity)
    - Typosquatting risk
    - License compliance issues
    - Version pinning (using "*" or unpinned versions)
    - Package age/popularity (heuristic)
    - Not imported but has vulnerabilities (false positive indicator)

    Args:
        package_name: Name of the package
        version: Package version
        ecosystem: Package ecosystem
        vulnerabilities: List of vulnerabilities
        is_imported: Whether package is imported in codebase
        typosquatting_risk: Whether typosquatting was detected
        license_compliant: Whether license is compliant

    Returns:
        Tuple of (risk_score 0.0-1.0, list of risk factors)
    """
    risk_score = 0.0
    risk_factors: list[str] = []

    # Factor 1: Vulnerabilities (max 0.5 contribution)
    if vulnerabilities:
        vuln_score = 0.0
        severity_weights = {"CRITICAL": 0.5, "HIGH": 0.3, "MEDIUM": 0.15, "LOW": 0.05}
        for vuln in vulnerabilities:
            severity = getattr(vuln, "severity", "UNKNOWN")
            vuln_score += severity_weights.get(severity, 0.1)
        vuln_score = min(vuln_score, 0.5)  # Cap at 0.5
        risk_score += vuln_score
        risk_factors.append(f"{len(vulnerabilities)} vulnerabilities detected")

    # Factor 2: Typosquatting (0.3 contribution)
    if typosquatting_risk:
        risk_score += 0.3
        risk_factors.append("Potential typosquatting detected")

    # Factor 3: License issues (0.1 contribution)
    if license_compliant is False:
        risk_score += 0.1
        risk_factors.append("License compliance issue")

    # Factor 4: Unpinned version (0.05 contribution)
    if version in ("*", "", "latest", "^", "~"):
        risk_score += 0.05
        risk_factors.append("Unpinned/floating version")

    # Factor 5: False positive reduction - if not imported but has vulns, reduce concern
    if is_imported is False and vulnerabilities:
        risk_score *= 0.5  # Reduce score by 50% for unreachable vulnerabilities
        risk_factors.append("Vulnerability may be unreachable (not imported)")

    # Clamp to 0.0-1.0
    risk_score = min(max(risk_score, 0.0), 1.0)

    return round(risk_score, 3), risk_factors


def _generate_compliance_report(
    dependencies: list,
    severity_summary: dict[str, int],
    total_vulnerabilities: int,
    vulnerable_count: int,
) -> dict[str, Any]:
    """
    Generate compliance report for SOC2/ISO standards.

    [20251231_FEATURE] v3.3.1 - Enterprise tier compliance reporting per roadmap v1.0.

    Args:
        dependencies: List of DependencyInfo objects
        severity_summary: Count by severity
        total_vulnerabilities: Total vulnerability count
        vulnerable_count: Number of vulnerable dependencies

    Returns:
        Compliance report dictionary
    """
    # Count license issues
    license_issues = sum(
        1
        for d in dependencies
        if hasattr(d, "license_compliant") and d.license_compliant is False
    )

    # Count typosquatting risks
    typosquat_risks = sum(
        1
        for d in dependencies
        if hasattr(d, "typosquatting_risk") and d.typosquatting_risk is True
    )

    # Calculate overall compliance score (0-100)
    compliance_score = 100.0

    # Deduct for critical vulnerabilities
    compliance_score -= severity_summary.get("CRITICAL", 0) * 20
    # Deduct for high vulnerabilities
    compliance_score -= severity_summary.get("HIGH", 0) * 10
    # Deduct for medium vulnerabilities
    compliance_score -= severity_summary.get("MEDIUM", 0) * 5
    # Deduct for license issues
    compliance_score -= license_issues * 5
    # Deduct for typosquatting risks
    compliance_score -= typosquat_risks * 15

    compliance_score = max(compliance_score, 0.0)

    # Determine compliance status
    if compliance_score >= 90:
        status = "COMPLIANT"
    elif compliance_score >= 70:
        status = "NEEDS_ATTENTION"
    else:
        status = "NON_COMPLIANT"

    return {
        "generated_at": __import__("datetime").datetime.now().isoformat(),
        "compliance_score": round(compliance_score, 1),
        "status": status,
        "total_dependencies": len(dependencies),
        "vulnerable_dependencies": vulnerable_count,
        "total_vulnerabilities": total_vulnerabilities,
        "severity_breakdown": severity_summary,
        "license_issues": license_issues,
        "typosquatting_risks": typosquat_risks,
        "frameworks": ["SOC2", "ISO27001"],
        "recommendations": _generate_compliance_recommendations(
            severity_summary, license_issues, typosquat_risks
        ),
    }


def _generate_compliance_recommendations(
    severity_summary: dict[str, int],
    license_issues: int,
    typosquat_risks: int,
) -> list[str]:
    """Generate prioritized compliance recommendations."""
    recommendations = []

    if severity_summary.get("CRITICAL", 0) > 0:
        recommendations.append(
            f"URGENT: Remediate {severity_summary['CRITICAL']} critical vulnerabilities immediately"
        )

    if severity_summary.get("HIGH", 0) > 0:
        recommendations.append(
            f"HIGH PRIORITY: Address {severity_summary['HIGH']} high-severity vulnerabilities"
        )

    if typosquat_risks > 0:
        recommendations.append(
            f"SECURITY: Review {typosquat_risks} packages flagged for potential typosquatting"
        )

    if license_issues > 0:
        recommendations.append(
            f"LEGAL: Review {license_issues} packages with license compliance issues"
        )

    if severity_summary.get("MEDIUM", 0) > 0:
        recommendations.append(
            f"Plan remediation for {severity_summary['MEDIUM']} medium-severity vulnerabilities"
        )

    if not recommendations:
        recommendations.append(
            "No immediate action required - continue regular monitoring"
        )

    return recommendations


def _scan_dependencies_sync(
    project_root: str | None = None,
    path: str | None = None,
    scan_vulnerabilities: bool = True,
    include_dev: bool = True,
    timeout: float = 30.0,
    tier: str | None = None,
    capabilities: dict | None = None,
) -> DependencyScanResult:
    """
    Synchronous implementation of dependency vulnerability scanning.

    [20251219_FEATURE] v3.0.4 - A06 Vulnerable Components
    [20251220_FIX] v3.0.5 - Added timeout parameter for OSV API calls
    [20251220_FEATURE] v3.0.5 - Returns DependencyScanResult with per-dependency tracking
    [20251229_FEATURE] v3.3.1 - Added tier enforcement and advanced features

    Args:
        project_root: Path to the project root directory (for backward compatibility)
        path: Alternative parameter name for project_root
        scan_vulnerabilities: Whether to check OSV for vulnerabilities (default True)
        include_dev: Whether to include dev dependencies (default True)
        timeout: Timeout for OSV API calls in seconds
        tier: User tier (community, pro, enterprise)
        capabilities: Tier capabilities dict

    Returns:
        DependencyScanResult with dependency-level vulnerability tracking
    """
    # Get tier and capabilities
    tier = tier or _get_current_tier()
    caps = capabilities or get_tool_capabilities("scan_dependencies", tier)
    caps_set = set(caps.get("capabilities", set()) or [])
    limits = caps.get("limits", {}) or {}
    # Handle parameter aliasing (support both 'path' and 'project_root')
    resolved_path_str = project_root or path or str(PROJECT_ROOT)

    try:
        from code_scalpel.security.dependencies import (
            DependencyParser,
            VulnerabilityScanner,
        )

        try:
            resolved_path = Path(resolved_path_str)
        except TypeError as exc:
            return DependencyScanResult(
                success=False,
                error=f"Invalid path: {exc}",
            )

        if not resolved_path.is_absolute():
            resolved_path = PROJECT_ROOT / resolved_path_str

        if not resolved_path.exists():
            return DependencyScanResult(
                success=False,
                error=f"Path not found: {resolved_path_str}",
            )

        # Parse dependencies from files
        all_deps: list[Any] = []
        errors: list[str] = []

        if resolved_path.is_file():
            # Single file scan
            try:
                if resolved_path.name == "requirements.txt":
                    all_deps = DependencyParser.parse_requirements_txt(resolved_path)
                elif resolved_path.name == "pyproject.toml":
                    all_deps = DependencyParser.parse_pyproject_toml(resolved_path)
                elif resolved_path.name == "package.json":
                    all_deps = DependencyParser.parse_package_json(resolved_path)
                elif resolved_path.name == "pom.xml":
                    all_deps = DependencyParser.parse_pom_xml(resolved_path)
                elif resolved_path.name == "build.gradle":
                    all_deps = DependencyParser.parse_build_gradle(resolved_path)
            except Exception as e:
                errors.append(f"Failed to parse {resolved_path}: {str(e)}")
        else:
            # Directory scan - find all dependency files
            dep_files = [
                ("requirements.txt", DependencyParser.parse_requirements_txt),
                ("pyproject.toml", DependencyParser.parse_pyproject_toml),
                ("package.json", DependencyParser.parse_package_json),
                ("pom.xml", DependencyParser.parse_pom_xml),
                ("build.gradle", DependencyParser.parse_build_gradle),
            ]

            for filename, parser in dep_files:
                file_path = resolved_path / filename
                if file_path.exists():
                    try:
                        all_deps.extend(parser(file_path))
                    except Exception as e:
                        errors.append(f"Failed to parse {filename}: {str(e)}")

        # Filter dev dependencies if requested
        if not include_dev:
            all_deps = [d for d in all_deps if not getattr(d, "is_dev", False)]

        # [20251229_FEATURE] v3.3.1 - Enforce tier limits
        max_dependencies = limits.get("max_dependencies")
        original_count = len(all_deps)

        if max_dependencies is not None and max_dependencies > 0:
            if len(all_deps) > max_dependencies:
                all_deps = all_deps[:max_dependencies]
                errors.append(
                    f"Dependency count ({original_count}) exceeds tier limit "
                    f"({max_dependencies}). Only first {max_dependencies} analyzed."
                )

        # Build DependencyInfo list
        dependency_infos: list[DependencyInfo] = []
        severity_summary: dict[str, int] = {}
        total_vulns = 0
        vulnerable_count = 0

        # [20251229_FEATURE] v3.3.1 - Pro tier: Reachability analysis
        imported_packages: set[str] = set()
        if "reachability_analysis" in caps_set and resolved_path.is_dir():
            imported_packages = _analyze_reachability(resolved_path)

        # Query vulnerabilities if requested
        vuln_map: dict[str, list[dict[str, Any]]] = {}
        if scan_vulnerabilities and all_deps:
            try:
                with VulnerabilityScanner(timeout=timeout) as scanner:
                    vuln_map = scanner.osv_client.query_batch(all_deps)
            except Exception as e:
                errors.append(f"OSV query failed: {str(e)}")

        for dep in all_deps:
            dep_vulns: list[DependencyVulnerability] = []
            dep_key = f"{dep.name}@{dep.version}"

            if dep_key in vuln_map:
                for vuln in vuln_map[dep_key]:
                    severity = _extract_severity(vuln)
                    fixed = _extract_fixed_version(vuln, dep.name)

                    dep_vulns.append(
                        DependencyVulnerability(
                            id=vuln.get("id", "UNKNOWN"),
                            summary=vuln.get("summary", ""),
                            severity=severity,
                            package=dep.name,
                            vulnerable_version=dep.version,
                            fixed_version=fixed,
                        )
                    )

                    severity_summary[severity] = severity_summary.get(severity, 0) + 1
                    total_vulns += 1

            if dep_vulns:
                vulnerable_count += 1

            # [20251229_FEATURE] v3.3.1 - Check reachability if analysis was performed
            is_imported = None
            if imported_packages:
                # Normalize package name (e.g., "Django" -> "django")
                normalized_name = dep.name.lower().replace("-", "_")
                is_imported = dep.name in imported_packages or normalized_name in {
                    p.lower().replace("-", "_") for p in imported_packages
                }

            # [20251229_FEATURE] v3.3.1 - Enterprise tier: License compliance
            license_str = None
            license_compliant = None
            if "license_compliance" in caps_set:
                license_str = _fetch_package_license(
                    dep.name, dep.ecosystem if hasattr(dep, "ecosystem") else "unknown"
                )
                if license_str:
                    license_compliant = _check_license_compliance(license_str)

            # [20251229_FEATURE] v3.3.1 - Pro tier: Typosquatting detection
            typosquatting_risk = None
            if "typosquatting_detection" in caps_set:
                typosquatting_risk = _check_typosquatting(
                    dep.name, dep.ecosystem if hasattr(dep, "ecosystem") else "unknown"
                )

            # [20251231_FEATURE] v3.3.1 - Pro tier: Supply chain risk scoring
            supply_chain_risk_score = None
            supply_chain_risk_factors = None
            if "supply_chain_risk_scoring" in caps_set:
                supply_chain_risk_score, supply_chain_risk_factors = (
                    _calculate_supply_chain_risk(
                        dep.name,
                        dep.version,
                        (
                            dep.ecosystem.value
                            if hasattr(dep.ecosystem, "value")
                            else str(dep.ecosystem)
                        ),
                        dep_vulns,
                        is_imported,
                        typosquatting_risk,
                        license_compliant,
                    )
                )

            dependency_infos.append(
                DependencyInfo(
                    name=dep.name,
                    version=dep.version,
                    ecosystem=(
                        dep.ecosystem.value
                        if hasattr(dep.ecosystem, "value")
                        else str(dep.ecosystem)
                    ),
                    vulnerabilities=dep_vulns,
                    is_imported=is_imported,
                    license=license_str,
                    license_compliant=license_compliant,
                    typosquatting_risk=typosquatting_risk,
                    supply_chain_risk_score=supply_chain_risk_score,
                    supply_chain_risk_factors=supply_chain_risk_factors,
                )
            )

        # [20251231_FEATURE] v3.3.1 - Enterprise tier: Compliance reporting
        compliance_report = None
        policy_violations = None
        if "compliance_reporting" in caps_set:
            compliance_report = _generate_compliance_report(
                dependency_infos,
                severity_summary,
                total_vulns,
                vulnerable_count,
            )
            # Collect policy violations from various checks
            policy_violations = []
            for dep_info in dependency_infos:
                if dep_info.typosquatting_risk:
                    policy_violations.append(
                        {
                            "type": "typosquatting",
                            "package": dep_info.name,
                            "severity": "HIGH",
                            "message": f"Package '{dep_info.name}' flagged for potential typosquatting",
                        }
                    )
                if dep_info.license_compliant is False:
                    policy_violations.append(
                        {
                            "type": "license",
                            "package": dep_info.name,
                            "severity": "MEDIUM",
                            "message": f"Package '{dep_info.name}' has non-compliant license: {dep_info.license}",
                        }
                    )
                if (
                    dep_info.supply_chain_risk_score
                    and dep_info.supply_chain_risk_score >= 0.7
                ):
                    policy_violations.append(
                        {
                            "type": "supply_chain",
                            "package": dep_info.name,
                            "severity": "HIGH",
                            "message": f"Package '{dep_info.name}' has high supply chain risk score: {dep_info.supply_chain_risk_score}",
                        }
                    )

        return DependencyScanResult(
            success=True,
            total_dependencies=len(dependency_infos),
            vulnerable_count=vulnerable_count,
            total_vulnerabilities=total_vulns,
            severity_summary=severity_summary,
            dependencies=dependency_infos,
            compliance_report=compliance_report,
            policy_violations=policy_violations,
            errors=errors,
        )

    except ImportError as e:
        return DependencyScanResult(
            success=False,
            error=f"Vulnerability scanner not available: {str(e)}",
        )
    except Exception as e:
        return DependencyScanResult(
            success=False,
            error=f"Scan failed: {str(e)}",
        )


def _extract_severity(vuln: dict[str, Any]) -> str:
    """Extract severity from OSV vulnerability data.

    [20251220_FIX] v3.0.5 - Improved severity extraction from OSV responses.
    """
    # Try database_specific.severity first (most common for GitHub advisories)
    if "database_specific" in vuln:
        db_severity = vuln["database_specific"].get("severity", "")
        if db_severity:
            # Map GitHub severity names to standard names
            severity_map = {
                "CRITICAL": "CRITICAL",
                "HIGH": "HIGH",
                "MODERATE": "MEDIUM",
                "MEDIUM": "MEDIUM",
                "LOW": "LOW",
            }
            return severity_map.get(db_severity.upper(), "UNKNOWN")

    # Try CVSS severity array
    if "severity" in vuln:
        for sev in vuln.get("severity", []):
            if sev.get("type") == "CVSS_V3":
                score_str = sev.get("score", "")
                # Parse CVSS score to severity
                try:
                    # CVSS format: "CVSS:3.1/AV:N/AC:L/..."
                    # or just the score like "7.5"
                    if "/" in score_str:
                        # Extract base score if full vector
                        pass
                    else:
                        score = float(score_str)
                        if score >= 9.0:
                            return "CRITICAL"
                        elif score >= 7.0:
                            return "HIGH"
                        elif score >= 4.0:
                            return "MEDIUM"
                        else:
                            return "LOW"
                except (ValueError, TypeError):
                    pass

    # Try ecosystem_specific
    if "ecosystem_specific" in vuln:
        eco_severity = vuln["ecosystem_specific"].get("severity", "")
        if eco_severity.upper() in ("CRITICAL", "HIGH", "MEDIUM", "MODERATE", "LOW"):
            return (
                "MEDIUM" if eco_severity.upper() == "MODERATE" else eco_severity.upper()
            )

    return "UNKNOWN"


def _extract_fixed_version(vuln: dict[str, Any], package_name: str) -> str | None:
    """Extract fixed version from OSV vulnerability data."""
    for affected in vuln.get("affected", []):
        if affected.get("package", {}).get("name") == package_name:
            for rng in affected.get("ranges", []):
                for event in rng.get("events", []):
                    if "fixed" in event:
                        return event["fixed"]
    return None


def _scan_dependencies_sync_legacy(
    path: str, timeout: float = 30.0
) -> DependencyScanResultModel:
    """
    Synchronous implementation of dependency vulnerability scanning.

    [20251219_FEATURE] v3.0.4 - A06 Vulnerable Components
    [20251220_FIX] v3.0.5 - Added timeout parameter for OSV API calls
    """
    try:
        from code_scalpel.security.dependencies import VulnerabilityScanner

        resolved_path = Path(path)
        if not resolved_path.is_absolute():
            resolved_path = PROJECT_ROOT / path

        if not resolved_path.exists():
            return DependencyScanResultModel(
                success=False,
                errors=[f"Path not found: {path}"],
            )

        with VulnerabilityScanner(timeout=timeout) as scanner:
            if resolved_path.is_file():
                result = scanner.scan_file(resolved_path)
            else:
                result = scanner.scan_directory(resolved_path)

        # Convert to Pydantic models and count severities
        findings = []
        severity_counts = {"CRITICAL": 0, "HIGH": 0, "MEDIUM": 0, "LOW": 0}

        for f in result.findings:
            findings.append(
                VulnerabilityFindingModel(
                    id=f.id,
                    cve_id=f.cve_id,
                    severity=f.severity,
                    package_name=f.package_name,
                    package_version=f.package_version,
                    ecosystem=f.ecosystem,
                    summary=f.summary,
                    fixed_versions=f.fixed_versions,
                    source_file=f.source_file,
                )
            )
            if f.severity in severity_counts:
                severity_counts[f.severity] += 1

        # Generate summary
        if result.vulnerabilities_found == 0:
            summary = f" No vulnerabilities found in {result.dependencies_scanned} dependencies."
        else:
            summary = (
                f" Found {result.vulnerabilities_found} vulnerabilities in "
                f"{result.dependencies_scanned} dependencies: "
                f"{severity_counts['CRITICAL']} critical, {severity_counts['HIGH']} high, "
                f"{severity_counts['MEDIUM']} medium, {severity_counts['LOW']} low."
            )

        return DependencyScanResultModel(
            success=True,
            dependencies_scanned=result.dependencies_scanned,
            vulnerabilities_found=result.vulnerabilities_found,
            critical_count=severity_counts["CRITICAL"],
            high_count=severity_counts["HIGH"],
            medium_count=severity_counts["MEDIUM"],
            low_count=severity_counts["LOW"],
            findings=findings,
            errors=result.errors,
            summary=summary,
        )

    except ImportError as e:
        return DependencyScanResultModel(
            success=False,
            errors=[f"Vulnerability scanner not available: {e}"],
        )
    except Exception as e:
        return DependencyScanResultModel(
            success=False,
            errors=[f"Scan failed: {e}"],
        )


@mcp.tool()
async def scan_dependencies(
    path: str | None = None,
    project_root: str | None = None,
    scan_vulnerabilities: bool = True,
    include_dev: bool = True,
    timeout: float = 30.0,
    ctx: Context | None = None,
) -> DependencyScanResult:
    """
    Scan project dependencies for known vulnerabilities (A06:2021 - Vulnerable Components).

    [20251219_FEATURE] v3.0.4 - A06 Vulnerable and Outdated Components
    [20251220_FIX] v3.0.5 - Added timeout parameter for OSV API calls
    [20251220_FEATURE] v3.0.5 - Progress reporting during vulnerability scan
    [20251220_FEATURE] v3.0.5 - Returns DependencyScanResult with per-dependency tracking

    This tool scans dependency files and checks them against the Google OSV
    (Open Source Vulnerabilities) database for known CVEs and security advisories.

    Supported dependency files:
    - npm: package.json
    - Maven: pom.xml, build.gradle
    - Python: requirements.txt, pyproject.toml

    Example usage:
    - Scan a single file: scan_dependencies(path="package.json")
    - Scan a project directory: scan_dependencies(project_root="/path/to/project")
    - Without vulnerability check: scan_dependencies(path="requirements.txt", scan_vulnerabilities=False)

    The scan will recursively find all dependency files in a directory,
    skipping node_modules and .venv directories.

    Args:
        path: Path to a dependency file or project directory
        project_root: Alias for 'path' (for backward compatibility)
        scan_vulnerabilities: Whether to check OSV for vulnerabilities (default True)
        include_dev: Whether to include dev dependencies (default True)
        timeout: Timeout in seconds for OSV API calls (default: 30.0)

    Returns:
        DependencyScanResult with dependency-level vulnerability tracking.
    """
    # Resolve path parameter (support both 'path' and 'project_root')
    resolved_path = path or project_root or str(PROJECT_ROOT)

    # [20251229_FEATURE] v3.3.1 - Get tier and capabilities
    tier = _get_current_tier()
    caps = get_tool_capabilities("scan_dependencies", tier)

    # [20251220_FEATURE] v3.0.5 - Progress reporting
    if ctx:
        await ctx.report_progress(
            0, 100, f"Scanning dependencies in {resolved_path}..."
        )

    result = await asyncio.to_thread(
        _scan_dependencies_sync,
        project_root=resolved_path,
        scan_vulnerabilities=scan_vulnerabilities,
        include_dev=include_dev,
        timeout=timeout,
        tier=tier,
        capabilities=caps,
    )

    if ctx:
        vuln_count = result.total_vulnerabilities
        await ctx.report_progress(
            100, 100, f"Scan complete: {vuln_count} vulnerabilities found"
        )

    return result


@mcp.tool()
async def security_scan(
    code: Optional[str] = None, file_path: Optional[str] = None
) -> SecurityResult:
    """
    Scan Python code for security vulnerabilities using taint analysis.

    Use this tool to audit code for security vulnerabilities before deploying
    or committing changes. It tracks data flow from sources to sinks.

    [20251214_FEATURE] v2.0.0 - Now accepts file_path parameter to scan files directly.

    Detects:
    - SQL Injection (CWE-89)
    - NoSQL Injection (CWE-943) - MongoDB
    - LDAP Injection (CWE-90)
    - Cross-Site Scripting (CWE-79)
    - Command Injection (CWE-78)
    - Path Traversal (CWE-22)
    - XXE - XML External Entity (CWE-611) [v1.4.0]
    - SSTI - Server-Side Template Injection (CWE-1336) [v1.4.0]
    - Hardcoded Secrets (CWE-798) - 30+ patterns
    - Weak Cryptography (CWE-327) - MD5, SHA-1 [v2.0.0]
    - Dangerous Patterns - shell=True, eval(), pickle [v2.0.0]

    Example::

        # Scan code directly
        result = await security_scan(code='''
        def get_user(user_id):
            query = f"SELECT * FROM users WHERE id = {user_id}"
            cursor.execute(query)  # SQL Injection!
        ''')

        # Returns SecurityResult:
        # - has_vulnerabilities: True
        # - vulnerability_count: 1
        # - risk_level: "high"
        # - vulnerabilities: [
        #     VulnerabilityInfo(
        #         type="SQL Injection",
        #         cwe="CWE-89",
        #         line=3,
        #         description="Tainted data flows to SQL execution"
        #     )
        # ]
        # - taint_flows: [{source: "user_id", sink: "execute", path: [...]}]

        # Or scan a file
        result = await security_scan(file_path="/app/handlers/user.py")

    Args:
        code: Python source code to scan (provide either code or file_path)
        file_path: Path to Python file to scan (provide either code or file_path)

    Returns:
        SecurityResult with vulnerabilities, risk_level, taint_flows, and remediation hints
    """
    tier = _get_current_tier()
    caps = get_tool_capabilities("security_scan", tier)
    return await asyncio.to_thread(_security_scan_sync, code, file_path, tier, caps)


def _basic_security_scan(code: str) -> SecurityResult:
    """Fallback security scan using pattern matching."""
    vulnerabilities = []
    taint_sources = []

    # Detect common dangerous patterns
    patterns = [
        (
            "execute(",
            "SQL Injection",
            "CWE-89",
            "Possible SQL injection via execute()",
        ),
        ("cursor.execute", "SQL Injection", "CWE-89", "SQL query execution detected"),
        ("os.system(", "Command Injection", "CWE-78", "os.system() call detected"),
        (
            "subprocess.call(",
            "Command Injection",
            "CWE-78",
            "subprocess.call() detected",
        ),
        ("eval(", "Code Injection", "CWE-94", "eval() call detected"),
        ("exec(", "Code Injection", "CWE-94", "exec() call detected"),
        (
            "render_template_string(",
            "XSS",
            "CWE-79",
            "Template injection risk",
        ),
    ]

    for line_num, line in enumerate(code.splitlines(), 1):
        for pattern, vuln_type, cwe, desc in patterns:
            if pattern in line:
                vulnerabilities.append(
                    VulnerabilityInfo(
                        type=vuln_type,
                        cwe=cwe,
                        severity="high" if "Injection" in vuln_type else "medium",
                        line=line_num,
                        description=desc,
                    )
                )

    # Detect taint sources
    source_patterns = ["request.args", "request.form", "input(", "sys.argv"]
    for pattern in source_patterns:
        if pattern in code:
            taint_sources.append(pattern)

    # [20251226_BUGFIX] Apply conservative finding cap for fallback path
    vulnerabilities = vulnerabilities[:50]

    vuln_count = len(vulnerabilities)
    if vuln_count == 0:
        risk_level = "low"
    elif vuln_count <= 2:
        risk_level = "medium"
    else:
        risk_level = "high"

    return SecurityResult(
        success=True,
        has_vulnerabilities=vuln_count > 0,
        vulnerability_count=vuln_count,
        risk_level=risk_level,
        vulnerabilities=vulnerabilities,
        taint_sources=taint_sources,
    )


def _detect_requested_constraint_types(code: str) -> set[str]:
    """Best-effort extraction of constraint types implied by code.

    [20251226_FEATURE] Enforce configurable constraint type limits in symbolic_execute.
    """

    requested: set[str] = set()
    try:
        tree = ast.parse(code)
    except Exception:
        return requested

    def _norm_type_name(name: str) -> str | None:
        n = name.lower()
        if n in {"int", "integer"}:
            return "int"
        if n in {"bool", "boolean"}:
            return "bool"
        if n in {"str", "string"}:
            return "string"
        if n in {"float", "double", "real"}:
            return "float"
        return None

    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            for arg in node.args.args:
                ann = arg.annotation
                if isinstance(ann, ast.Name):
                    t = _norm_type_name(ann.id)
                    if t:
                        requested.add(t)
                elif isinstance(ann, ast.Attribute):
                    t = _norm_type_name(ann.attr)
                    if t:
                        requested.add(t)

        if isinstance(node, ast.Call):
            if (
                isinstance(node.func, ast.Name)
                and node.func.id == "symbolic"
                and len(node.args) >= 2
            ):
                type_arg = node.args[1]
                if isinstance(type_arg, ast.Name):
                    t = _norm_type_name(type_arg.id)
                    if t:
                        requested.add(t)
                elif isinstance(type_arg, ast.Constant) and isinstance(
                    type_arg.value, str
                ):
                    t = _norm_type_name(type_arg.value)
                    if t:
                        requested.add(t)

    return requested


def _basic_symbolic_analysis(code: str, max_paths: int) -> SymbolicResult:
    """Fallback symbolic analysis using AST inspection."""
    try:
        tree = ast.parse(code)

        branch_count = 0
        symbolic_vars: list[str] = []
        conditions: list[str] = []

        for node in ast.walk(tree):
            if isinstance(node, ast.If):
                branch_count += 1
                conditions.append(ast.unparse(node.test))
            elif isinstance(node, ast.While):
                branch_count += 1
                conditions.append(f"while: {ast.unparse(node.test)}")
            elif isinstance(node, ast.For):
                branch_count += 1
                if isinstance(node.target, ast.Name):
                    symbolic_vars.append(node.target.id)
            elif isinstance(node, ast.FunctionDef):
                for arg in node.args.args:
                    symbolic_vars.append(arg.arg)

        estimated_paths = min(2 ** min(branch_count, 20), max_paths)
        paths = [
            ExecutionPath(
                path_id=i,
                conditions=conditions[: i + 1] if i < len(conditions) else conditions,
                final_state={},
                reproduction_input=None,
                is_reachable=True,
            )
            for i in range(estimated_paths)
        ]

        return SymbolicResult(
            success=True,
            paths_explored=estimated_paths,
            total_paths=estimated_paths,
            paths=paths,
            symbolic_variables=list(set(symbolic_vars)),
            constraints=conditions,
        )

    except Exception as e:
        return SymbolicResult(
            success=False,
            paths_explored=0,
            error=f"Basic analysis failed: {str(e)}",
        )


# [20251230_FEATURE] v1.0 roadmap Pro/Enterprise tier helper functions
def _build_path_prioritization(
    paths: list[ExecutionPath], code: str, smart: bool = False
) -> dict[str, Any]:
    """Build path prioritization metadata for Pro/Enterprise tier.

    Smart prioritization (Pro) uses heuristics to rank paths by:
    - Constraint complexity
    - Branch depth
    - Error-prone patterns
    """
    if not paths:
        return {"total_paths": 0, "prioritized_paths": []}

    prioritized = []
    for path in paths:
        # Calculate priority score based on conditions
        complexity = len(path.conditions)
        has_error_patterns = any(
            keyword in str(path.conditions)
            for keyword in ["None", "null", "0", "empty", "negative"]
        )

        priority_score = complexity * 10
        if has_error_patterns:
            priority_score += 25  # Boost paths with error-prone conditions

        prioritized.append(
            {
                "path_id": path.path_id,
                "priority_score": priority_score,
                "complexity": complexity,
                "has_error_patterns": has_error_patterns,
                "is_reachable": path.is_reachable,
            }
        )

    # Sort by priority score descending
    prioritized.sort(key=lambda x: x["priority_score"], reverse=True)

    return {
        "algorithm": "smart_heuristic" if smart else "complexity_based",
        "total_paths": len(paths),
        "prioritized_paths": prioritized[:10],  # Top 10 for output size
        "highest_priority_path": prioritized[0]["path_id"] if prioritized else None,
    }


def _build_concolic_results(paths: list[ExecutionPath], code: str) -> dict[str, Any]:
    """Build concolic execution results for Pro/Enterprise tier.

    Concolic = concrete + symbolic execution combined.
    Identifies paths that benefit from concrete value injection.
    """
    if not paths:
        return {"mode": "concolic", "concrete_hints": []}

    concrete_hints = []
    for path in paths:
        if path.reproduction_input:
            for var, value in path.reproduction_input.items():
                concrete_hints.append(
                    {
                        "path_id": path.path_id,
                        "variable": var,
                        "concrete_value": value,
                        "type": type(value).__name__,
                    }
                )

    return {
        "mode": "concolic",
        "paths_with_concrete_values": sum(1 for p in paths if p.reproduction_input),
        "total_concrete_hints": len(concrete_hints),
        "concrete_hints": concrete_hints[:20],  # Limit output size
    }


def _build_state_space_analysis(
    paths: list[ExecutionPath], constraints: list[str]
) -> dict[str, Any]:
    """Build state space reduction analysis for Enterprise tier.

    Analyzes the symbolic state space for reduction opportunities.
    """
    # Analyze constraint patterns for reduction opportunities
    unique_vars = set()
    constraint_types = {"equality": 0, "inequality": 0, "boolean": 0, "other": 0}

    for constraint in constraints:
        constraint_lower = constraint.lower()
        if "==" in constraint or "is" in constraint_lower:
            constraint_types["equality"] += 1
        elif any(op in constraint for op in ["<", ">", "<=", ">="]):
            constraint_types["inequality"] += 1
        elif any(
            kw in constraint_lower for kw in ["and", "or", "not", "true", "false"]
        ):
            constraint_types["boolean"] += 1
        else:
            constraint_types["other"] += 1

        # Extract variable-like tokens
        import re

        vars_found = re.findall(r"\b[a-zA-Z_][a-zA-Z0-9_]*\b", constraint)
        unique_vars.update(v for v in vars_found if not v.isupper() and len(v) > 1)

    # Estimate state space size
    estimated_space = 2 ** min(len(unique_vars), 30)  # Cap at 2^30

    return {
        "unique_variables": len(unique_vars),
        "constraint_breakdown": constraint_types,
        "total_constraints": len(constraints),
        "estimated_state_space": estimated_space,
        "reduction_opportunities": [
            "Constraint merging" if constraint_types["equality"] > 3 else None,
            "Boolean simplification" if constraint_types["boolean"] > 2 else None,
            "Variable pruning" if len(unique_vars) > 10 else None,
        ],
        "reduction_potential": "high" if len(constraints) > 10 else "low",
    }


def _build_memory_model(code: str) -> dict[str, Any]:
    """Build memory modeling results for Enterprise tier.

    Analyzes pointer/reference patterns and aliasing in code.
    """
    try:
        tree = ast.parse(code)
    except SyntaxError:
        return {"analysis": "unavailable", "reason": "syntax_error"}

    heap_allocations = []
    pointer_operations = []
    aliases = []

    for node in ast.walk(tree):
        # Detect list/dict creation (heap allocation)
        if isinstance(node, (ast.List, ast.Dict, ast.Set)):
            heap_allocations.append(
                {
                    "type": type(node).__name__,
                    "line": getattr(node, "lineno", 0),
                }
            )
        # Detect assignments that create aliases
        elif isinstance(node, ast.Assign):
            for target in node.targets:
                if isinstance(target, ast.Name) and isinstance(node.value, ast.Name):
                    aliases.append(
                        {
                            "target": target.id,
                            "source": node.value.id,
                            "line": node.lineno,
                        }
                    )
                elif isinstance(target, ast.Subscript):
                    pointer_operations.append(
                        {
                            "type": "subscript_write",
                            "line": node.lineno,
                        }
                    )

    return {
        "heap_allocations": len(heap_allocations),
        "pointer_operations": len(pointer_operations),
        "potential_aliases": len(aliases),
        "aliases": aliases[:10],  # Limit output
        "memory_safety_risk": "medium" if aliases else "low",
        "analysis_depth": "basic",
    }


def _symbolic_execute_sync(
    code: str,
    max_paths: int | None = None,
    max_depth: int | None = None,
    constraint_types: Any = None,
    tier: str | None = None,
    capabilities: dict | None = None,
) -> SymbolicResult:
    """Synchronous implementation of symbolic_execute.

    [20251226_FEATURE] Enforce tier/tool limits via limits.toml (max_paths/max_depth/constraint_types).
    [20251230_FEATURE] v1.0 roadmap alignment - tier-aware Pro/Enterprise features.
    """
    tier = tier or _get_current_tier()
    caps = capabilities or get_tool_capabilities("symbolic_execute", tier)
    caps_set = set(caps.get("capabilities", set()) or [])
    limits = caps.get("limits", {}) or {}

    # Apply tier limits if not explicitly provided
    if max_paths is None:
        max_paths = limits.get("max_paths")
    if max_depth is None:
        max_depth = limits.get("max_depth")
    if constraint_types is None:
        constraint_types = limits.get("constraint_types")

    valid, error = _validate_code(code)
    if not valid:
        return SymbolicResult(success=False, paths_explored=0, error=error)

    if max_paths is not None and max_paths < 1:
        return SymbolicResult(
            success=False, paths_explored=0, error="max_paths must be >= 1"
        )

    if max_depth is not None and max_depth < 1:
        return SymbolicResult(
            success=False, paths_explored=0, error="max_depth must be >= 1"
        )

    fallback_max_paths = 10 if max_paths is None else int(max_paths)
    effective_max_depth = 10 if max_depth is None else int(max_depth)

    if constraint_types not in (None, "all"):
        try:
            allowed_types = {str(t).lower() for t in constraint_types}
        except Exception:
            allowed_types = set()

        if allowed_types:
            requested_types = _detect_requested_constraint_types(code)
            disallowed = {t for t in requested_types if t not in allowed_types}
            if disallowed:
                basic_result = _basic_symbolic_analysis(
                    code, max_paths=fallback_max_paths
                )
                basic_result.error = (
                    f"[LIMIT] Symbolic constraint types not enabled ({sorted(disallowed)}); "
                    "using AST-only analysis"
                )
                return basic_result

    cache = _get_cache()
    cache_config = {
        "max_paths": max_paths,
        "max_depth": max_depth,
        "constraint_types": constraint_types,
        "model_schema": "friendly_names_v20251214",
    }

    if cache:
        cached = cache.get(code, "symbolic", cache_config)
        if cached is not None:
            logger.debug("Cache hit for symbolic_execute")
            # [20251227_BUGFIX] v3.1.1 - Handle both dict (JSON cache) and object (pickle cache)
            if isinstance(cached, SymbolicResult):
                return cached
            if isinstance(cached, dict):
                if "paths" in cached:
                    # Handle mixed case: dict with ExecutionPath objects OR plain dicts
                    paths_list = cached["paths"]
                    if paths_list and isinstance(paths_list[0], dict):
                        cached["paths"] = [ExecutionPath(**p) for p in paths_list]
                    # else: already ExecutionPath objects, leave as-is
                return SymbolicResult(**cached)
            return cached

    try:
        from code_scalpel.symbolic_execution_tools import SymbolicAnalyzer
        from code_scalpel.symbolic_execution_tools.engine import PathStatus

        analyzer = SymbolicAnalyzer(max_loop_iterations=effective_max_depth)
        result = analyzer.analyze(code)

        paths: list[ExecutionPath] = []
        all_constraints: list[str] = []

        for path in result.paths:
            conditions = [str(c) for c in path.constraints] if path.constraints else []
            all_constraints.extend(conditions)
            paths.append(
                ExecutionPath(
                    path_id=path.path_id,
                    conditions=conditions,
                    final_state=path.variables or {},
                    reproduction_input=path.model or {},
                    is_reachable=path.status == PathStatus.FEASIBLE,
                )
            )

        total_paths = len(paths)
        truncated = False
        truncation_warning: str | None = None
        if max_paths is not None and total_paths > max_paths:
            truncated = True
            truncation_warning = (
                f"Result limited to {max_paths} paths by current configuration."
            )
            paths = paths[:max_paths]

        symbolic_vars = (
            list(result.all_variables.keys()) if result.all_variables else []
        )
        constraints_list = list(set(all_constraints))

        if not symbolic_vars or not constraints_list:
            basic = _basic_symbolic_analysis(code, max_paths=fallback_max_paths)
            if not symbolic_vars and basic.symbolic_variables:
                symbolic_vars = basic.symbolic_variables
            if not constraints_list and basic.constraints:
                constraints_list = basic.constraints
            if not paths and basic.paths:
                paths = basic.paths
                total_paths = (
                    basic.total_paths if basic.total_paths is not None else total_paths
                )

        # [20251230_FEATURE] v1.0 roadmap Pro/Enterprise tier features
        path_prioritization: dict[str, Any] | None = None
        concolic_results: dict[str, Any] | None = None
        state_space_analysis: dict[str, Any] | None = None
        memory_model: dict[str, Any] | None = None

        if "smart_path_prioritization" in caps_set:
            path_prioritization = _build_path_prioritization(paths, code, smart=True)

        if "concolic_execution" in caps_set:
            concolic_results = _build_concolic_results(paths, code)

        if "state_space_reduction" in caps_set:
            state_space_analysis = _build_state_space_analysis(paths, constraints_list)

        if "memory_modeling" in caps_set:
            memory_model = _build_memory_model(code)

        symbolic_result = SymbolicResult(
            success=True,
            paths_explored=len(paths),
            total_paths=total_paths,
            truncated=truncated,
            truncation_warning=truncation_warning,
            paths=paths,
            symbolic_variables=symbolic_vars,
            constraints=constraints_list,
            path_prioritization=path_prioritization,
            concolic_results=concolic_results,
            state_space_analysis=state_space_analysis,
            memory_model=memory_model,
        )

        if cache:
            cache.set(code, "symbolic", symbolic_result.model_dump(), cache_config)

        return symbolic_result

    except ImportError as e:
        logger.warning(
            f"Symbolic execution not available (ImportError: {e}), using basic analysis"
        )
        basic_result = _basic_symbolic_analysis(code, max_paths=fallback_max_paths)
        basic_result.error = (
            f"[FALLBACK] Symbolic engine not available, using AST analysis: {e}"
        )
        # Add tier-aware features even in fallback
        basic_result = _add_tier_features_to_result(basic_result, code, caps_set)
        return basic_result
    except Exception as e:
        logger.warning(f"Symbolic execution failed, using basic analysis: {e}")
        basic_result = _basic_symbolic_analysis(code, max_paths=fallback_max_paths)
        basic_result.error = f"[FALLBACK] Symbolic execution failed ({type(e).__name__}: {e}), using AST analysis"
        # Add tier-aware features even in fallback
        basic_result = _add_tier_features_to_result(basic_result, code, caps_set)
        return basic_result


def _add_tier_features_to_result(
    result: SymbolicResult, code: str, caps_set: set
) -> SymbolicResult:
    """Add tier-aware features to a SymbolicResult (including fallback results)."""
    if "smart_path_prioritization" in caps_set:
        result.path_prioritization = _build_path_prioritization(
            result.paths, code, smart=True
        )

    if "concolic_execution" in caps_set:
        result.concolic_results = _build_concolic_results(result.paths, code)

    if "state_space_reduction" in caps_set:
        result.state_space_analysis = _build_state_space_analysis(
            result.paths, result.constraints
        )

    if "memory_modeling" in caps_set:
        result.memory_model = _build_memory_model(code)

    return result


@mcp.tool()
async def symbolic_execute(
    code: str,
    max_paths: int | None = None,
    max_depth: int | None = None,
) -> SymbolicResult:
    """Perform symbolic execution on Python code.

    [20251226_FEATURE] Tier-aware limits are loaded from limits.toml via get_tool_capabilities.

    Args:
        code: Python source code to analyze
        max_paths: Maximum number of paths to return (defaults to current configuration)
        max_depth: Maximum loop unrolling depth (defaults to current configuration)
    """

    tier = _get_current_tier()
    caps = get_tool_capabilities("symbolic_execute", tier)
    limits = caps.get("limits", {}) if isinstance(caps, dict) else {}

    configured_max_paths = limits.get("max_paths")
    configured_max_depth = limits.get("max_depth")
    constraint_types = limits.get("constraint_types")

    effective_max_paths: int | None
    if max_paths is None:
        effective_max_paths = (
            None if configured_max_paths is None else int(configured_max_paths)
        )
    else:
        effective_max_paths = int(max_paths)
        if configured_max_paths is not None:
            effective_max_paths = min(effective_max_paths, int(configured_max_paths))

    effective_max_depth: int | None
    if max_depth is None:
        effective_max_depth = (
            None if configured_max_depth is None else int(configured_max_depth)
        )
    else:
        effective_max_depth = int(max_depth)
        if configured_max_depth is not None:
            effective_max_depth = min(effective_max_depth, int(configured_max_depth))

    return await asyncio.to_thread(
        _symbolic_execute_sync,
        code,
        effective_max_paths,
        effective_max_depth,
        constraint_types,
        tier,
        caps,
    )


# ============================================================================
# TEST GENERATION
# ============================================================================


def _generate_tests_sync(
    code: str | None = None,
    file_path: str | None = None,
    function_name: str | None = None,
    framework: str = "pytest",
    max_test_cases: int | None = None,
    data_driven: bool = False,
    crash_log: str | None = None,
) -> TestGenerationResult:
    """Synchronous implementation of generate_unit_tests.

    [20251220_FIX] v3.0.5 - Added file_path parameter for consistency with other tools.
    [20251229_FEATURE] v3.3.0 - Added data_driven parameter for Pro tier parametrized tests.
    [20251229_FEATURE] v3.3.0 - Added crash_log parameter for Enterprise tier bug reproduction.
    """
    # Read from file if file_path provided
    if file_path and not code:
        try:
            resolved = Path(file_path)
            if not resolved.is_absolute():
                resolved = PROJECT_ROOT / file_path
            if not resolved.exists():
                return TestGenerationResult(
                    success=False,
                    function_name=function_name or "unknown",
                    test_count=0,
                    total_test_cases=0,
                    error=f"File not found: {file_path}.",
                )
            code = resolved.read_text(encoding="utf-8")
        except Exception as e:
            return TestGenerationResult(
                success=False,
                function_name=function_name or "unknown",
                test_count=0,
                total_test_cases=0,
                error=f"Failed to read file: {str(e)}.",
            )

    if not code:
        return TestGenerationResult(
            success=False,
            function_name=function_name or "unknown",
            test_count=0,
            total_test_cases=0,
            error="Either 'code' or 'file_path' must be provided.",
        )

    valid, error = _validate_code(code)
    if not valid:
        return TestGenerationResult(
            success=False,
            function_name=function_name or "unknown",
            test_count=0,
            total_test_cases=0,
            error=error,
        )

    try:
        from code_scalpel.generators import TestGenerator

        generator = TestGenerator(framework=framework)

        # [20251229_FEATURE] v3.3.0 - Enterprise: Bug reproduction from crash log
        if crash_log:
            result = generator.generate_bug_reproduction_test(
                code=code,
                crash_log=crash_log,
                function_name=function_name,
            )
        else:
            result = generator.generate(code, function_name=function_name)

        total_cases = len(result.test_cases)
        truncated = False
        truncation_warning: str | None = None
        if (
            max_test_cases is not None
            and max_test_cases >= 0
            and total_cases > max_test_cases
        ):
            truncated = True
            truncation_warning = f"Generated {total_cases} test cases; returned {max_test_cases} due to configured limits."
            result.test_cases = result.test_cases[:max_test_cases]

        # [20251230_FIX][unit-tests] Ensure GeneratedTestCase is JSON-serializable.
        # Symbolic execution engines may return solver-specific objects (e.g., Z3 ASTs)
        # that crash MCP transport serialization if returned directly.
        def _json_safe(value: Any) -> Any:
            if value is None or isinstance(value, (str, int, float, bool)):
                return value
            if isinstance(value, (list, tuple)):
                return [_json_safe(v) for v in value]
            if isinstance(value, dict):
                return {str(k): _json_safe(v) for k, v in value.items()}

            # Best-effort Z3 coercion
            as_long = getattr(value, "as_long", None)
            if callable(as_long):
                try:
                    as_long_result = as_long()
                    if isinstance(as_long_result, (int, float, str)):
                        return int(as_long_result)
                except Exception:
                    pass
            as_decimal = getattr(value, "as_decimal", None)
            if callable(as_decimal):
                try:
                    dec = str(as_decimal(20))
                    # Z3 sometimes uses trailing '?' for repeating decimals.
                    return dec.replace("?", "")
                except Exception:
                    pass

            # Fallback: string representation
            try:
                return str(value)
            except Exception:
                return repr(value)

        test_cases = [
            GeneratedTestCase(
                path_id=tc.path_id,
                function_name=tc.function_name,
                inputs=_json_safe(getattr(tc, "inputs", {}) or {}),
                description=tc.description,
                path_conditions=[
                    str(c) for c in (getattr(tc, "path_conditions", []) or [])
                ],
            )
            for tc in result.test_cases
        ]

        # [20251229_FEATURE] v3.3.0 - Pro tier: Data-driven test generation
        # Generate parametrized/data-driven tests if requested
        if data_driven:
            if framework == "pytest":
                pytest_code = result.generate_parametrized_tests()
            else:
                pytest_code = result.pytest_code

            if framework == "unittest":
                unittest_code = result.generate_unittest_subtests()
            else:
                unittest_code = result.unittest_code
        else:
            pytest_code = result.pytest_code
            unittest_code = result.unittest_code

        return TestGenerationResult(
            success=True,
            function_name=result.function_name,
            test_count=len(test_cases),
            test_cases=test_cases,
            total_test_cases=total_cases,
            truncated=truncated,
            truncation_warning=truncation_warning,
            pytest_code=pytest_code,
            unittest_code=unittest_code,
        )

    except Exception as e:
        return TestGenerationResult(
            success=False,
            function_name=function_name or "unknown",
            test_count=0,
            total_test_cases=0,
            error=f"Test generation failed: {str(e)}",
        )


@mcp.tool()
async def generate_unit_tests(
    code: str | None = None,
    file_path: str | None = None,
    function_name: str | None = None,
    framework: str = "pytest",
    data_driven: bool = False,
    crash_log: str | None = None,
) -> TestGenerationResult:
    """
    Generate unit tests from code using symbolic execution.

    Use this tool to automatically create test cases that cover all execution paths
    in a function. Each test case includes concrete input values that trigger a
    specific path through the code.

    [20251220_FIX] v3.0.5 - Added file_path parameter for consistency with other tools.
    [20251229_FEATURE] v3.3.0 - Pro tier: Data-driven/parametrized test generation.

    Example::

        result = await generate_unit_tests(
            code=\"\"\"
            def calculate_discount(price, is_member):
                if price > 100:
                    if is_member:
                        return price * 0.8  # 20% off for members
                    return price * 0.9  # 10% off
                return price  # No discount
            \"\"\",
            framework="pytest"
        )

        # Returns TestGenerationResult:
        # - pytest_code: '''
        #     import pytest
        #
        #     def test_calculate_discount_member_high_price():
        #         assert calculate_discount(150, True) == 120.0
        #
        #     def test_calculate_discount_non_member_high_price():
        #         assert calculate_discount(150, False) == 135.0
        #
        #     def test_calculate_discount_low_price():
        #         assert calculate_discount(50, True) == 50
        #   '''
        # - test_cases: [
        #     GeneratedTestCase(inputs={"price": 150, "is_member": True}, ...),
        #     GeneratedTestCase(inputs={"price": 150, "is_member": False}, ...),
        #     GeneratedTestCase(inputs={"price": 50, "is_member": True}, ...)
        # ]
        # - coverage_paths: 3

    **Pro Tier - Data-Driven Tests Example::**

        result = await generate_unit_tests(
            code='''
            def validate_age(age):
                if age < 0:
                    return "invalid"
                elif age < 18:
                    return "minor"
                else:
                    return "adult"
            ''',
            framework="pytest",
            data_driven=True
        )

        # Returns parametrized test:
        # @pytest.mark.parametrize("age", [
        #     (-1,), (10,), (25,)
        # ], ids=['path_0', 'path_1', 'path_2'])
        # def test_validate_age_parametrized_0(age):
        #     result = validate_age(age=age)
        #     assert result is not None

    **Enterprise Tier - Bug Reproduction Example::**

        result = await generate_unit_tests(
                        code='''
            def divide(a, b):
                return a / b
                        ''',
                        crash_log='''
            Traceback (most recent call last):
              File "test.py", line 3, in <module>
                result = divide(10, 0)
              File "test.py", line 2, in divide
                return a / b
            ZeroDivisionError: division by zero
                        ''',
            framework="pytest"
        )

        # Returns bug reproduction test:
        # def test_divide_path_0():
        #     '''Bug reproduction test for ZeroDivisionError'''
        #     a = 10
        #     b = 0
        #     with pytest.raises(ZeroDivisionError):
        #         divide(a=a, b=b)

    Args:
        code: Source code containing the function to test (provide code or file_path)
        file_path: Path to file containing the function to test (provide code or file_path)
        function_name: Name of function to generate tests for (auto-detected if None)
        framework: Test framework ("pytest" or "unittest")
        data_driven: Generate parametrized/data-driven tests (Pro tier, default False)
        crash_log: Crash log/stack trace for bug reproduction (Enterprise tier, optional)

    Returns:
        TestGenerationResult with pytest_code/unittest_code and generated test_cases
    """
    # [20251225_FEATURE] Tier-based behavior via capability matrix (no upgrade hints).
    tier = _get_current_tier()
    caps = get_tool_capabilities("generate_unit_tests", tier)
    limits = caps.get("limits", {})
    cap_set = caps.get("capabilities", set())

    max_test_cases = limits.get("max_test_cases")
    allowed_frameworks = limits.get("test_frameworks")
    # [20251231_BUGFIX] Fixed capabilities check - it's a set, not dict
    data_driven_supported = "data_driven_tests" in cap_set
    bug_reproduction_supported = "bug_reproduction" in cap_set

    # [20251229_FEATURE] v3.3.0 - Pro tier enforcement for data-driven tests
    if data_driven and not data_driven_supported:
        return TestGenerationResult(
            success=False,
            function_name=function_name or "unknown",
            test_count=0,
            total_test_cases=0,
            error="Data-driven test generation requires Pro tier or higher.",
            tier_applied=tier,
            framework_used=framework,
            max_test_cases_limit=max_test_cases,
            data_driven_enabled=False,
            bug_reproduction_enabled=False,
        )

    # [20251229_FEATURE] v3.3.0 - Enterprise tier enforcement for bug reproduction
    if crash_log and not bug_reproduction_supported:
        return TestGenerationResult(
            success=False,
            function_name=function_name or "unknown",
            test_count=0,
            total_test_cases=0,
            error="Bug reproduction test generation requires Enterprise tier.",
            tier_applied=tier,
            framework_used=framework,
            max_test_cases_limit=max_test_cases,
            data_driven_enabled=data_driven,
            bug_reproduction_enabled=False,
        )

    if isinstance(allowed_frameworks, list) and framework not in allowed_frameworks:
        return TestGenerationResult(
            success=False,
            function_name=function_name or "unknown",
            test_count=0,
            total_test_cases=0,
            error=f"Unsupported framework: {framework}",
            tier_applied=tier,
            framework_used=framework,
            max_test_cases_limit=max_test_cases,
            data_driven_enabled=data_driven,
            bug_reproduction_enabled=crash_log is not None,
        )

    # [20260111_FEATURE] Call sync implementation and add metadata
    result = await asyncio.to_thread(
        _generate_tests_sync,
        code,
        file_path,
        function_name,
        framework,
        max_test_cases,
        data_driven,
        crash_log,
    )

    # Add output metadata for transparency
    result.tier_applied = tier
    result.framework_used = framework
    result.max_test_cases_limit = max_test_cases
    result.data_driven_enabled = data_driven
    result.bug_reproduction_enabled = crash_log is not None

    return result


# ============================================================================
# REFACTOR SIMULATION  # [20251225_FEATURE] Tier-gated handler integration
# ============================================================================


def _simulate_refactor_sync(
    original_code: str,
    new_code: str | None = None,
    patch: str | None = None,
    strict_mode: bool = False,
    *,
    max_file_size_mb: int | None = None,
    analysis_depth: str = "basic",
    compliance_validation: bool = False,
) -> RefactorSimulationResult:
    """Synchronous implementation of simulate_refactor."""
    # [20251225_FEATURE] Tier-aware input size limits (neutral messaging).
    if max_file_size_mb is not None and max_file_size_mb >= 0:
        max_bytes = int(max_file_size_mb * 1024 * 1024)
        for label, text in (
            ("original_code", original_code),
            ("new_code", new_code or ""),
            ("patch", patch or ""),
        ):
            if len(text.encode("utf-8")) > max_bytes:
                return RefactorSimulationResult(
                    success=False,
                    is_safe=False,
                    status="error",
                    error=(
                        f"Input '{label}' exceeds configured size limit of {max_file_size_mb} MB."
                    ),
                )

    valid, error = _validate_code(original_code)
    if not valid:
        return RefactorSimulationResult(
            success=False,
            is_safe=False,
            status="error",
            error=f"Invalid original code: {error}.",
        )

    if new_code is None and patch is None:
        return RefactorSimulationResult(
            success=False,
            is_safe=False,
            status="error",
            error="Must provide either 'new_code' or 'patch'.",
        )

    try:
        from code_scalpel.generators import RefactorSimulator

        # [20251230_FEATURE] Get tier capabilities
        tier = _get_current_tier()
        caps = get_tool_capabilities("simulate_refactor", tier) or {}
        cap_set = set(caps.get("capabilities", set()) or [])

        # Enable Pro tier type checking
        enable_type_checking = "type_checking" in cap_set or "build_check" in cap_set

        # Enable Enterprise tier regression prediction
        enable_regression = "regression_prediction" in cap_set

        simulator = RefactorSimulator(strict_mode=strict_mode)
        result = simulator.simulate(
            original_code=original_code,
            new_code=new_code,
            patch=patch,
            enable_type_checking=enable_type_checking,
            enable_regression_prediction=enable_regression,
            project_root=str(PROJECT_ROOT),
        )

        security_issues = [
            RefactorSecurityIssue(
                type=issue.type,
                severity=issue.severity,
                line=issue.line,
                description=issue.description,
                cwe=issue.cwe,
            )
            for issue in result.security_issues
        ]

        structural_changes = dict(result.structural_changes or {})

        # [20251225_FEATURE] Pro/Enterprise: deeper structural diff (Python only).
        if analysis_depth in {"advanced", "deep"}:
            try:
                import ast

                def _collect_python_functions(code_text: str) -> dict[str, str]:
                    tree = ast.parse(code_text)
                    functions: dict[str, str] = {}

                    class StackFrame:
                        __slots__ = ("class_name",)

                        def __init__(self, class_name: str | None):
                            self.class_name = class_name

                    stack: list[StackFrame] = [StackFrame(None)]

                    def visit(node: ast.AST) -> None:
                        if isinstance(node, ast.ClassDef):
                            stack.append(StackFrame(node.name))
                            for child in node.body:
                                visit(child)
                            stack.pop()
                            return
                        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                            cls = stack[-1].class_name
                            name = f"{cls}.{node.name}" if cls else node.name
                            functions[name] = ast.dump(node, include_attributes=False)
                            return
                        for child in ast.iter_child_nodes(node):
                            visit(child)

                    visit(tree)
                    return functions

                # Prefer the patched code from simulator when available.
                patched_code = getattr(result, "patched_code", None)
                new_text = patched_code or new_code
                if isinstance(new_text, str):
                    old_funcs = _collect_python_functions(original_code)
                    new_funcs = _collect_python_functions(new_text)
                    modified = sorted(
                        name
                        for name in (set(old_funcs) & set(new_funcs))
                        if old_funcs[name] != new_funcs[name]
                    )
                    if modified:
                        structural_changes["functions_modified"] = modified
            except Exception:
                # Best-effort enrichment only.
                pass

        # [20251225_FEATURE] Enterprise: optional compliance validation warnings.
        if compliance_validation:
            removed_funcs = structural_changes.get("functions_removed") or []
            removed_classes = structural_changes.get("classes_removed") or []
            if removed_funcs or removed_classes:
                result.warnings.append(
                    "Compliance validation: detected removed functions/classes."
                )

        return RefactorSimulationResult(
            success=True,
            is_safe=result.is_safe,
            status=result.status.value,
            reason=result.reason,
            security_issues=security_issues,
            structural_changes=structural_changes,
            warnings=result.warnings,
        )

    except Exception as e:
        return RefactorSimulationResult(
            success=False,
            is_safe=False,
            status="error",
            error=f"Simulation failed: {str(e)}",
        )


@mcp.tool()
async def simulate_refactor(
    original_code: str,
    new_code: str | None = None,
    patch: str | None = None,
    strict_mode: bool = False,
) -> RefactorSimulationResult:
    """
    Simulate applying a code change and check for safety issues.

    Use this tool before applying AI-generated code changes to verify they don't
    introduce security vulnerabilities or break existing functionality.

    Provide either the new_code directly OR a unified diff patch.

    Example::

        # Check if a refactor introduces vulnerabilities
        result = await simulate_refactor(
            original_code='''
            def process_data(data):
                return sanitize(data)
            ''',
            new_code='''
            def process_data(data):
                return eval(data)  # Dangerous change!
            '''
        )

        # Returns RefactorSimulationResult:
        # - is_safe: False
        # - status: "unsafe"
        # - security_issues: [
        #     {"type": "Code Injection", "cwe": "CWE-94",
        #      "description": "eval() introduced in refactor"}
        # ]
        # - structural_changes: [
        #     {"type": "function_body_changed", "name": "process_data"}
        # ]

        # Safe refactor example
        safe_result = await simulate_refactor(
            original_code="def add(a, b): return a + b",
            new_code="def add(a: int, b: int) -> int: return a + b"
        )
        # is_safe: True, status: "safe"

    Args:
        original_code: The original source code
        new_code: The modified code to compare against (optional)
        patch: A unified diff patch to apply (optional)
        strict_mode: If True, treat warnings as unsafe

    Returns:
        RefactorSimulationResult with is_safe verdict, security_issues, and structural_changes
    """
    # [20251225_FEATURE] Tier-based behavior via capability matrix (no upgrade hints).
    tier = _get_current_tier()
    caps = get_tool_capabilities("simulate_refactor", tier)
    limits = caps.get("limits", {})
    tool_caps = caps.get("capabilities", set())

    max_file_size_mb = limits.get("max_file_size_mb")
    analysis_depth = limits.get("analysis_depth", "basic")
    compliance_validation = "compliance_validation" in tool_caps

    return await asyncio.to_thread(
        _simulate_refactor_sync,
        original_code,
        new_code,
        patch,
        strict_mode,
        max_file_size_mb=max_file_size_mb,
        analysis_depth=analysis_depth,
        compliance_validation=compliance_validation,
    )


def _crawl_project_discovery(
    root_path: str,
    exclude_dirs: list[str] | None = None,
    *,
    max_files: int | None = None,
    max_depth: int | None = None,
    respect_gitignore: bool = True,
) -> ProjectCrawlResult:
    """
    Discovery-only crawl for Community tier.

    Provides file inventory and structure without deep analysis:
    - Lists Python files and their paths
    - Identifies entrypoint patterns (main blocks, CLI commands)
    - Basic statistics (file count, directory structure)
    - NO complexity analysis
    - NO function/class details
    - NO file contents

    [20251223_FEATURE] v3.2.8 - Community tier discovery crawl.
    """
    import os
    import re
    from datetime import datetime
    from fnmatch import fnmatch
    from pathlib import Path

    try:
        root = Path(root_path)
        if not root.exists():
            raise FileNotFoundError(f"Project root not found: {root_path}")

        # Default excludes
        default_excludes = {
            "__pycache__",
            ".git",
            ".venv",
            "venv",
            "node_modules",
            ".pytest_cache",
            ".mypy_cache",
            "dist",
            "build",
            ".tox",
            "htmlcov",
            ".eggs",
            "*.egg-info",
        }
        if exclude_dirs:
            default_excludes.update(exclude_dirs)

        gitignore_patterns: list[str] = []
        if respect_gitignore:
            gitignore_file = root / ".gitignore"
            if gitignore_file.exists() and gitignore_file.is_file():
                for raw in gitignore_file.read_text(
                    encoding="utf-8", errors="ignore"
                ).splitlines():
                    line = raw.strip()
                    if not line or line.startswith("#"):
                        continue
                    if line.startswith("!"):
                        # Negation patterns are intentionally ignored in this minimal implementation.
                        continue
                    gitignore_patterns.append(line)

        def _gitignore_match(
            rel_posix_path: str, pattern: str, *, is_dir: bool
        ) -> bool:
            if pattern.endswith("/"):
                if not is_dir:
                    return False
                pattern = pattern[:-1]
            if "/" in pattern:
                return fnmatch(rel_posix_path, pattern)
            return fnmatch(Path(rel_posix_path).name, pattern)

        def _is_gitignored(rel_path: Path, *, is_dir: bool) -> bool:
            if not gitignore_patterns:
                return False
            rel_posix = rel_path.as_posix().lstrip("./")
            return any(
                _gitignore_match(rel_posix, pat, is_dir=is_dir)
                for pat in gitignore_patterns
            )

        python_files: list[CrawlFileResult] = []
        entrypoints: list[str] = []
        python_file_count = 0
        ext_counts: dict[str, int] = {}
        framework_hints: set[str] = set()
        reached_limit = False

        # Walk the directory tree
        for dirpath, dirnames, filenames in os.walk(root):
            rel_dir = Path(dirpath).relative_to(root)
            depth = len(rel_dir.parts)
            if max_depth is not None and depth >= max_depth:
                dirnames[:] = []
            else:
                filtered_dirnames: list[str] = []
                for d in dirnames:
                    if d in default_excludes:
                        continue
                    rel_child = rel_dir / d
                    if _is_gitignored(rel_child, is_dir=True):
                        continue
                    filtered_dirnames.append(d)
                dirnames[:] = filtered_dirnames

            for filename in filenames:
                suffix = Path(filename).suffix.lower() or "(no_ext)"
                ext_counts[suffix] = ext_counts.get(suffix, 0) + 1
                rel_file = rel_dir / filename
                if _is_gitignored(rel_file, is_dir=False):
                    continue

                if filename.endswith(".py"):
                    if max_files is not None and python_file_count >= max_files:
                        reached_limit = True
                        dirnames[:] = []
                        break
                    python_file_count += 1
                    file_path = Path(dirpath) / filename
                    rel_path = str(file_path.relative_to(root))

                    # Check for entrypoint patterns without parsing
                    try:
                        content = file_path.read_text(encoding="utf-8", errors="ignore")
                        # [20251230_FIX][crawl] Detect common web/CLI entrypoints in discovery mode.
                        # Previously we only matched Flask '@app.route', missing patterns like '@app.get'.
                        route_decorator = re.search(
                            r"@\w+\.(route|get|post|put|delete|patch)\s*\(",
                            content,
                        )
                        is_entrypoint = (
                            'if __name__ == "__main__"' in content
                            or "if __name__ == '__main__'" in content
                            or "@click.command" in content
                            or bool(route_decorator)
                            or "def main(" in content
                        )

                        if "flask" in content or "@app.route" in content:
                            framework_hints.add("flask")
                        if "django" in content:
                            framework_hints.add("django")
                        if "fastapi" in content:
                            framework_hints.add("fastapi")

                        if is_entrypoint:
                            entrypoints.append(rel_path)

                        # Create minimal file result (discovery mode)
                        python_files.append(
                            CrawlFileResult(
                                path=rel_path,
                                status="success",
                                lines_of_code=len(content.splitlines()),
                                functions=[],
                                classes=[],
                                imports=[],
                                complexity_warnings=[],
                                error=None,
                            )
                        )
                    except Exception as e:
                        python_files.append(
                            CrawlFileResult(
                                path=rel_path,
                                status="error",
                                lines_of_code=0,
                                error=f"Could not read file: {str(e)}",
                            )
                        )

            if reached_limit:
                break

        # Build discovery report
        languages = ", ".join(
            f"{ext}={count}"
            for ext, count in sorted(ext_counts.items(), key=lambda kv: (-kv[1], kv[0]))
        )
        limit_note = "" if not reached_limit else "(limit reached)"
        report = f"""# Project Discovery Report

    ## Summary
    - Python files discovered: {python_file_count} {limit_note}
    - Entrypoints detected: {len(entrypoints)}
    - File extensions observed: {languages if languages else '(none)'}

    ## Entrypoints
    {chr(10).join(f'- {ep}' for ep in entrypoints) if entrypoints else '(none detected)'}

    ## Files
    {chr(10).join(f'- {f.path}' for f in python_files[:50])}
    {'...' if len(python_files) > 50 else ''}
    """

        summary = CrawlSummary(
            total_files=python_file_count,
            successful_files=len([f for f in python_files if f.status == "success"]),
            failed_files=len([f for f in python_files if f.status == "error"]),
            total_lines_of_code=sum(f.lines_of_code for f in python_files),
            total_functions=0,  # Not analyzed in discovery mode
            total_classes=0,  # Not analyzed in discovery mode
            complexity_warnings=0,  # Not analyzed in discovery mode
        )

        # Map extensions to language names for breakdown (best-effort)
        ext_lang_map = {
            ".py": "python",
            ".pyw": "python",
            ".js": "javascript",
            ".mjs": "javascript",
            ".cjs": "javascript",
            ".jsx": "javascript",
            ".ts": "typescript",
            ".tsx": "typescript",
            ".java": "java",
        }
        language_breakdown: dict[str, int] = {}
        for ext, count in ext_counts.items():
            lang = ext_lang_map.get(ext, ext.lstrip("."))
            language_breakdown[lang] = language_breakdown.get(lang, 0) + count

        return ProjectCrawlResult(
            success=True,
            root_path=str(root),
            timestamp=datetime.now().isoformat(),
            summary=summary,
            files=python_files,
            errors=[],
            markdown_report=report,
            language_breakdown=language_breakdown or None,
            cache_hits=None,
            compliance_summary=None,
            framework_hints=sorted(framework_hints) if framework_hints else None,
            entrypoints=entrypoints or None,
        )

    except Exception as e:
        return ProjectCrawlResult(
            success=False,
            root_path=root_path,
            timestamp=datetime.now().isoformat(),
            summary=CrawlSummary(
                total_files=0,
                successful_files=0,
                failed_files=0,
                total_lines_of_code=0,
                total_functions=0,
                total_classes=0,
                complexity_warnings=0,
            ),
            error=f"Discovery crawl failed: {str(e)}",
        )


def _crawl_project_sync(
    root_path: str,
    exclude_dirs: list[str] | None = None,
    complexity_threshold: int = 10,
    include_report: bool = True,
    capabilities: set[str] | None = None,
    max_files: int | None = None,
    max_depth: int | None = None,
    respect_gitignore: bool = True,
) -> ProjectCrawlResult:
    """Synchronous implementation of crawl_project."""
    try:
        import json

        from code_scalpel.analysis.project_crawler import ProjectCrawler

        # [20251229_FEATURE] Enterprise: Incremental indexing with cache
        cache_file: Path | None = None
        cached_results = {}
        cache_hits = 0
        incremental_mode = capabilities and "incremental_indexing" in capabilities

        if incremental_mode:
            cache_dir = Path(root_path) / ".scalpel_cache"
            cache_dir.mkdir(exist_ok=True)
            cache_file = cache_dir / "crawl_cache.json"

            # Ensure cache file exists even on first run
            if cache_file is not None and not cache_file.exists():
                try:
                    cache_file.touch()
                except Exception:
                    cache_file = None

            if cache_file is not None and cache_file.exists():
                try:
                    with open(cache_file, "r", encoding="utf-8") as f:
                        cached_results = json.load(f)
                except Exception:
                    cached_results = {}

        # [20260101_BUGFIX] Enterprise: Load custom crawl rules from config file
        # Fixes test_crawl_project_enterprise_custom_rules_config
        include_extensions: tuple[str, ...] | None = None
        custom_exclude_dirs: list[str] = list(exclude_dirs) if exclude_dirs else []

        if capabilities and "custom_crawl_rules" in capabilities:
            config_file = Path(root_path) / ".code-scalpel" / "crawl_project.json"
            if config_file.exists() and config_file.is_file():
                try:
                    with open(config_file, "r", encoding="utf-8") as f:
                        custom_config = json.load(f)
                    # Load include_extensions from config
                    if "include_extensions" in custom_config:
                        include_extensions = tuple(custom_config["include_extensions"])
                        # [20260102_DEBUG] Log config loading for debugging
                        import logging

                        logger = logging.getLogger(__name__)
                        logger.info(
                            f"Loaded include_extensions from config: {include_extensions}"
                        )
                    # Merge exclude_dirs from config
                    if "exclude_dirs" in custom_config:
                        custom_exclude_dirs.extend(custom_config["exclude_dirs"])
                except Exception as e:
                    # [20260102_BUGFIX] Don't silently ignore errors - log them for debugging
                    import logging

                    logger = logging.getLogger(__name__)
                    logger.warning(
                        f"Failed to load custom crawl config from {config_file}: {e}"
                    )

        crawler = ProjectCrawler(
            root_path,
            exclude_dirs=(
                frozenset(custom_exclude_dirs) if custom_exclude_dirs else None
            ),
            complexity_threshold=complexity_threshold,
            max_files=max_files,
            max_depth=max_depth,
            respect_gitignore=respect_gitignore,
            include_extensions=include_extensions,
        )

        # [20251229_FEATURE] Enterprise: Optimization for 100k+ files
        use_optimization = capabilities and "100k_plus_files_support" in capabilities
        if use_optimization:
            # Enable batching and memory-efficient processing
            import logging

            logger = logging.getLogger(__name__)
            logger.info(
                f"Enterprise mode: Optimizing for large-scale crawl in {root_path}"
            )

        result = crawler.crawl()

        # [20251229_FEATURE] Enterprise: Filter unchanged files if incremental
        if incremental_mode:
            filtered_files = []
            for file_result in result.files_analyzed:
                file_path = str(file_result.path)
                try:
                    mtime = Path(file_result.path).stat().st_mtime
                    cached_mtime = cached_results.get(file_path, {}).get("mtime")
                    if cached_mtime and mtime == cached_mtime:
                        # Use cached result
                        cache_hits += 1
                        continue
                    else:
                        # File changed or new
                        filtered_files.append(file_result)
                        cached_results[file_path] = {"mtime": mtime}
                except Exception:
                    filtered_files.append(file_result)

            # Update result with filtered files
            result.files_analyzed = (
                filtered_files if filtered_files else result.files_analyzed
            )

            # Save cache (always write when incremental is enabled)
            if cache_file:
                try:
                    with open(cache_file, "w", encoding="utf-8") as f:
                        json.dump(cached_results, f)
                except Exception:
                    pass

        # Convert to Pydantic models
        def to_func_info(f) -> CrawlFunctionInfo:
            return CrawlFunctionInfo(
                name=f.qualified_name,
                lineno=f.lineno,
                complexity=f.complexity,
            )

        def to_class_info(c) -> CrawlClassInfo:
            return CrawlClassInfo(
                name=c.name,
                lineno=c.lineno,
                methods=[to_func_info(m) for m in c.methods],
                bases=c.bases,
            )

        def to_file_result(fr, root: str) -> CrawlFileResult:
            import os

            return CrawlFileResult(
                path=os.path.relpath(fr.path, root),
                status=fr.status,
                lines_of_code=fr.lines_of_code,
                functions=[to_func_info(f) for f in fr.functions],
                classes=[to_class_info(c) for c in fr.classes],
                imports=fr.imports,
                complexity_warnings=[to_func_info(f) for f in fr.complexity_warnings],
                error=fr.error,
            )

        summary = CrawlSummary(
            total_files=result.total_files,
            successful_files=len(result.files_analyzed),
            failed_files=len(result.files_with_errors),
            total_lines_of_code=result.total_lines_of_code,
            total_functions=result.total_functions,
            total_classes=result.total_classes,
            complexity_warnings=len(result.all_complexity_warnings),
        )

        files = [to_file_result(f, result.root_path) for f in result.files_analyzed]
        errors = [to_file_result(f, result.root_path) for f in result.files_with_errors]

        # Language breakdown (best-effort by file extension)
        lang_counts: dict[str, int] = {}
        ext_lang_map = {
            ".py": "python",
            ".pyw": "python",
            ".js": "javascript",
            ".mjs": "javascript",
            ".cjs": "javascript",
            ".jsx": "javascript",
            ".ts": "typescript",
            ".tsx": "typescript",
            ".java": "java",
        }
        for f in files:
            suffix = Path(f.path).suffix.lower()
            lang = ext_lang_map.get(suffix, suffix.lstrip("."))
            lang_counts[lang] = lang_counts.get(lang, 0) + 1

        compliance_summary: dict[str, Any] | None = None
        if capabilities and "compliance_scanning" in capabilities:
            # Placeholder best-effort hook; actual compliance analysis is tool-side
            compliance_summary = {
                "status": "not_implemented",
                "files_checked": 0,
                "violations": 0,
                "critical": 0,
                "high": 0,
                "medium": 0,
            }

        report = ""
        if include_report:
            report = crawler.generate_report(result)

            # [20251225_FEATURE] Pro/Enterprise: framework and ecosystem signals.
            if capabilities and "framework_entrypoint_detection" in capabilities:
                framework_signals: set[str] = set()
                nextjs_routes: list[dict] = []
                django_routes: list[dict] = []
                flask_routes: list[dict] = []

                root = Path(root_path)

                # Next.js pages detection
                if capabilities and "nextjs_pages_detection" in capabilities:
                    # Check pages/ directory (Pages Router)
                    pages_dir = root / "pages"
                    if pages_dir.exists():
                        framework_signals.add("Next.js (Pages Router)")
                        for page_file in pages_dir.rglob("*.{js,jsx,ts,tsx}"):
                            rel_path = page_file.relative_to(pages_dir)
                            route = "/" + str(rel_path.with_suffix("")).replace(
                                "\\", "/"
                            )
                            if route.endswith("/index"):
                                route = route[:-6] or "/"
                            # Detect dynamic routes
                            if "[" in route:
                                route_type = "dynamic"
                            elif route.startswith("/api/"):
                                route_type = "api"
                            else:
                                route_type = "page"
                            nextjs_routes.append(
                                {
                                    "path": str(page_file.relative_to(root)),
                                    "route": route,
                                    "type": route_type,
                                }
                            )

                    # Check app/ directory (App Router)
                    app_dir = root / "app"
                    if app_dir.exists():
                        framework_signals.add("Next.js (App Router)")
                        for layout_or_page in app_dir.rglob("*.{js,jsx,ts,tsx}"):
                            filename = layout_or_page.name
                            if filename in (
                                "page.tsx",
                                "page.jsx",
                                "page.js",
                                "page.ts",
                                "layout.tsx",
                                "layout.jsx",
                                "layout.js",
                                "layout.ts",
                                "route.tsx",
                                "route.jsx",
                                "route.js",
                                "route.ts",
                            ):
                                rel_path = layout_or_page.parent.relative_to(app_dir)
                                route = "/" + str(rel_path).replace("\\", "/")
                                if route == "/.":
                                    route = "/"
                                route_type = "app-" + filename.split(".")[0]
                                nextjs_routes.append(
                                    {
                                        "path": str(layout_or_page.relative_to(root)),
                                        "route": route,
                                        "type": route_type,
                                    }
                                )

                # Django views detection
                if capabilities and "django_views_detection" in capabilities:
                    for fr in result.files_analyzed:
                        # Check for Django imports
                        has_django = any(imp.startswith("django") for imp in fr.imports)
                        if has_django:
                            framework_signals.add("Django")
                            # Parse urls.py files
                            if "urls.py" in fr.path:
                                try:
                                    with open(fr.path, "r", encoding="utf-8") as f:
                                        content = f.read()
                                        # Simple regex to find path() calls
                                        import re

                                        patterns = re.findall(
                                            r'path\(["\']([^"\']+)["\'],\s*(\w+)',
                                            content,
                                        )
                                        for route, view in patterns:
                                            django_routes.append(
                                                {
                                                    "route": route,
                                                    "view": view,
                                                    "file": fr.path,
                                                }
                                            )
                                except Exception:
                                    pass

                # Flask routes detection
                if capabilities and "flask_routes_detection" in capabilities:
                    for fr in result.files_analyzed:
                        has_flask = any(imp.startswith("flask") for imp in fr.imports)
                        if has_flask:
                            framework_signals.add("Flask")
                            try:
                                with open(fr.path, "r", encoding="utf-8") as f:
                                    content = f.read()
                                    # Parse @app.route decorators
                                    import re

                                    patterns = re.findall(
                                        r'@(?:app|blueprint)\.route\(["\']([^"\']+)["\'](?:,\s*methods=\[([^\]]+)\])?\)',
                                        content,
                                    )
                                    for route, methods in patterns:
                                        flask_routes.append(
                                            {
                                                "route": route,
                                                "methods": (
                                                    methods.replace('"', "").replace(
                                                        "'", ""
                                                    )
                                                    if methods
                                                    else "GET"
                                                ),
                                                "file": fr.path,
                                            }
                                        )
                            except Exception:
                                pass

                # Basic framework detection via imports (fallback)
                for fr in result.files_analyzed:
                    for imp in fr.imports:
                        if imp.startswith("fastapi"):
                            framework_signals.add("FastAPI")
                        if imp.startswith("click"):
                            framework_signals.add("Click")
                        if imp.startswith("typer"):
                            framework_signals.add("Typer")

                # Generate framework report sections
                if framework_signals:
                    report += "\n\n## Framework Signals\n" + "\n".join(
                        f"- {name}" for name in sorted(framework_signals)
                    )

                if nextjs_routes:
                    report += "\n\n### Next.js Routes\n"
                    for route_info in sorted(nextjs_routes, key=lambda x: x["route"]):
                        report += f"- `{route_info['route']}` ({route_info['type']}) - {route_info['path']}\n"

                if django_routes:
                    report += "\n\n### Django URL Patterns\n"
                    for route_info in sorted(django_routes, key=lambda x: x["route"]):
                        report += f"- `{route_info['route']}` -> {route_info['view']}\n"

                if flask_routes:
                    report += "\n\n### Flask Routes\n"
                    for route_info in sorted(flask_routes, key=lambda x: x["route"]):
                        report += f"- `{route_info['route']}` [{route_info['methods']}] - {route_info['file']}\n"

            if capabilities and "generated_code_detection" in capabilities:
                root = Path(root_path)
                known_generated = [
                    "migrations",
                    "alembic",
                    "dist",
                    "build",
                    "node_modules",
                    "__pycache__",
                    ".venv",
                    "venv",
                ]
                present = [d for d in known_generated if (root / d).exists()]

                # Content-based detection for generated code
                generated_files: list[str] = []
                generation_markers = [
                    "<auto-generated",
                    "@generated",
                    "autogenerated",
                    "do not edit",
                    "generated by",
                    "this file is automatically",
                    "code generator",
                ]

                for fr in result.files_analyzed:
                    try:
                        with open(fr.path, "r", encoding="utf-8", errors="ignore") as f:
                            # Check first 20 lines for generation markers
                            header = "".join(f.readline().lower() for _ in range(20))
                            if any(marker in header for marker in generation_markers):
                                generated_files.append(
                                    str(Path(fr.path).relative_to(root))
                                )
                    except Exception:
                        pass

                if present or generated_files:
                    report += "\n\n## Generated/Third-Party Code\n"
                    if present:
                        report += (
                            "### Folders (Heuristics)\n"
                            + "\n".join(f"- {d}" for d in present)
                            + "\n"
                        )
                    if generated_files:
                        report += "\n### Files (Content Analysis)\n" + "\n".join(
                            f"- {f}" for f in generated_files[:20]  # Limit to 20
                        )
                        if len(generated_files) > 20:
                            report += f"\n- ... and {len(generated_files) - 20} more"

            # [20251229_FEATURE] Enterprise: Monorepo detection and support
            if capabilities and "monorepo_support" in capabilities:
                root = Path(root_path)
                workspaces: list[dict] = []

                # Detect Yarn/npm workspaces
                package_json = root / "package.json"
                if package_json.exists():
                    try:
                        import json as json_lib

                        with open(package_json, "r", encoding="utf-8") as f:
                            pkg_data = json_lib.load(f)
                            if "workspaces" in pkg_data:
                                workspace_patterns = pkg_data["workspaces"]
                                if isinstance(workspace_patterns, dict):
                                    workspace_patterns = workspace_patterns.get(
                                        "packages", []
                                    )

                                for pattern in workspace_patterns:
                                    for workspace_dir in root.glob(pattern):
                                        if workspace_dir.is_dir():
                                            workspace_pkg = (
                                                workspace_dir / "package.json"
                                            )
                                            if workspace_pkg.exists():
                                                try:
                                                    with open(
                                                        workspace_pkg,
                                                        "r",
                                                        encoding="utf-8",
                                                    ) as wf:
                                                        wp_data = json_lib.load(wf)
                                                        workspaces.append(
                                                            {
                                                                "name": wp_data.get(
                                                                    "name",
                                                                    workspace_dir.name,
                                                                ),
                                                                "path": str(
                                                                    workspace_dir.relative_to(
                                                                        root
                                                                    )
                                                                ),
                                                                "type": "npm-workspace",
                                                            }
                                                        )
                                                except Exception:
                                                    pass
                    except Exception:
                        pass

                # Detect Lerna monorepo
                lerna_json = root / "lerna.json"
                if lerna_json.exists():
                    try:
                        import json as json_lib

                        with open(lerna_json, "r", encoding="utf-8") as f:
                            lerna_data = json_lib.load(f)
                            packages = lerna_data.get("packages", ["packages/*"])
                            for pattern in packages:
                                for pkg_dir in root.glob(pattern):
                                    if (
                                        pkg_dir.is_dir()
                                        and (pkg_dir / "package.json").exists()
                                    ):
                                        workspaces.append(
                                            {
                                                "name": pkg_dir.name,
                                                "path": str(pkg_dir.relative_to(root)),
                                                "type": "lerna-package",
                                            }
                                        )
                    except Exception:
                        pass

                # Detect Python monorepos (multiple pyproject.toml or setup.py)
                python_projects = []
                for pyproject in root.rglob("pyproject.toml"):
                    if pyproject.parent != root:  # Sub-projects only
                        try:
                            import toml

                            with open(pyproject, "r", encoding="utf-8") as f:
                                proj_data = toml.load(f)
                                name = proj_data.get("project", {}).get(
                                    "name", pyproject.parent.name
                                )
                                python_projects.append(
                                    {
                                        "name": name,
                                        "path": str(pyproject.parent.relative_to(root)),
                                        "type": "python-package",
                                    }
                                )
                        except Exception:
                            pass

                if workspaces or python_projects:
                    report += "\n\n## Monorepo Structure\n"
                    all_packages = workspaces + python_projects
                    for pkg in sorted(all_packages, key=lambda x: x["path"]):
                        report += (
                            f"- **{pkg['name']}** (`{pkg['path']}`) - {pkg['type']}\n"
                        )

            # [20251229_FEATURE] Enterprise: Cross-repository dependency linking
            if capabilities and "cross_repo_dependency_linking" in capabilities:
                root = Path(root_path)
                external_deps: list[dict] = []

                # Check for git submodules
                gitmodules = root / ".gitmodules"
                if gitmodules.exists():
                    try:
                        content = gitmodules.read_text(encoding="utf-8")
                        import re

                        submodule_pattern = re.compile(
                            r'\[submodule "([^"]+)"\]\s+path\s*=\s*([^\s]+)\s+url\s*=\s*([^\s]+)'
                        )
                        for match in submodule_pattern.finditer(content):
                            name, path, url = match.groups()
                            external_deps.append(
                                {
                                    "name": name,
                                    "path": path,
                                    "url": url,
                                    "type": "git-submodule",
                                }
                            )
                    except Exception:
                        pass

                # Check for workspace dependencies in monorepo
                for pkg_json in root.rglob("package.json"):
                    try:
                        import json as json_lib

                        with open(pkg_json, "r", encoding="utf-8") as f:
                            pkg_data = json_lib.load(f)
                            deps = pkg_data.get("dependencies", {})
                            for dep_name, dep_version in deps.items():
                                # Detect workspace/monorepo references
                                if dep_version.startswith(
                                    "workspace:"
                                ) or dep_version.startswith("link:"):
                                    external_deps.append(
                                        {
                                            "name": dep_name,
                                            "source": str(
                                                pkg_json.parent.relative_to(root)
                                            ),
                                            "version": dep_version,
                                            "type": "workspace-link",
                                        }
                                    )
                    except Exception:
                        pass

                if external_deps:
                    report += "\n\n## Cross-Repository Dependencies\n"
                    for dep in sorted(external_deps, key=lambda x: x.get("name", "")):
                        if dep["type"] == "git-submodule":
                            report += f"- **{dep['name']}** (submodule) - `{dep['path']}` from {dep['url']}\n"
                        elif dep["type"] == "workspace-link":
                            report += f"- **{dep['name']}** (workspace) - {dep['version']} referenced by `{dep['source']}`\n"

        return ProjectCrawlResult(
            success=True,
            root_path=result.root_path,
            timestamp=result.timestamp,
            summary=summary,
            files=files,
            errors=errors,
            markdown_report=report,
            language_breakdown=lang_counts or None,
            cache_hits=cache_hits if incremental_mode else None,
            compliance_summary=compliance_summary,
            framework_hints=None,
            entrypoints=None,
        )

    except Exception as e:
        return ProjectCrawlResult(
            success=False,
            root_path=root_path,
            timestamp="",
            summary=CrawlSummary(
                total_files=0,
                successful_files=0,
                failed_files=0,
                total_lines_of_code=0,
                total_functions=0,
                total_classes=0,
                complexity_warnings=0,
            ),
            error=f"Crawl failed: {str(e)}",
        )


# --- Helper functions for extract_code (refactored for maintainability) ---


def _extraction_error(
    target_name: str,
    error: str,
    tier: str = "community",
    language: str | None = None,
) -> ContextualExtractionResult:
    """Create a standardized error result for extraction failures.

    [20260111_FEATURE] Added tier and language metadata for transparency.
    """
    return ContextualExtractionResult(
        success=False,
        target_name=target_name,
        target_code="",
        context_code="",
        full_code="",
        error=error,
        tier_applied=tier,
        language_detected=language,
        cross_file_deps_enabled=False,
        max_depth_applied=None,
    )


async def _extract_polyglot(
    target_type: str,
    target_name: str,
    file_path: str | None,
    code: str | None,
    language: Any,
    include_token_estimate: bool,
) -> ContextualExtractionResult:
    """
    [20251214_FEATURE] v2.0.0 - Multi-language extraction using PolyglotExtractor.

    Handles extraction for JavaScript, TypeScript, and Java using tree-sitter
    and the Unified IR system.

    Args:
        target_type: "function", "class", or "method"
        target_name: Name of element to extract
        file_path: Path to source file
        code: Source code string (if file_path not provided)
        language: Language enum value
        include_token_estimate: Include token count estimate

    Returns:
        ContextualExtractionResult with extracted code
    """
    # [20251221_FEATURE] v3.1.0 - Use UnifiedExtractor instead of PolyglotExtractor
    # [20251228_BUGFIX] Avoid deprecated shim imports.
    from code_scalpel.mcp.path_resolver import resolve_path
    from code_scalpel.surgery.unified_extractor import Language, UnifiedExtractor

    if file_path is None and code is None:
        return _extraction_error(
            target_name, "Must provide either 'file_path' or 'code' argument"
        )

    try:
        import subprocess
        # Create extractor from file or code
        if file_path is not None:
            resolved_path = resolve_path(file_path, str(PROJECT_ROOT))
            extractor = UnifiedExtractor.from_file(resolved_path, language)
        else:
            # code is guaranteed to be str here (checked earlier in function)
            assert code is not None
            extractor = UnifiedExtractor(code, language=language)

        # Perform extraction
        result = extractor.extract(target_type, target_name)

        if not result.success:
            return _extraction_error(target_name, result.error or "Extraction failed")

        token_estimate = result.token_estimate if include_token_estimate else 0

        # [20260111_FEATURE] Get tier and limits for metadata
        tier = _get_current_tier()
        from code_scalpel.licensing.config_loader import get_tool_limits

        limits = get_tool_limits("extract_code", tier)
        max_depth_limit = limits.get("max_depth")

        # Map Language enum to string
        lang_str_map = {
            Language.PYTHON: "python",
            Language.JAVASCRIPT: "javascript",
            Language.TYPESCRIPT: "typescript",
            Language.JAVA: "java",
        }
        lang_str = lang_str_map.get(language, "unknown")

        # [20251216_FEATURE] v2.0.2 - Include JSX/TSX metadata in result
        return ContextualExtractionResult(
            success=True,
            target_name=target_name,
            target_code=result.code,
            context_code="",  # Cross-file deps not yet supported for non-Python
            full_code=result.code,
            context_items=[],
            total_lines=result.end_line - result.start_line + 1,
            line_start=result.start_line,
            line_end=result.end_line,
            token_estimate=token_estimate,
            # [20260111_FEATURE] Output metadata for transparency
            tier_applied=tier,
            language_detected=lang_str,
            cross_file_deps_enabled=False,  # Not supported for non-Python yet
            max_depth_applied=max_depth_limit,
            jsx_normalized=result.jsx_normalized,
            is_server_component=result.is_server_component,
            is_server_action=result.is_server_action,
            component_type=result.component_type,
        )
    except FileNotFoundError as e:
        return _extraction_error(target_name, str(e))
    except Exception as e:
        return _extraction_error(target_name, f"Extraction failed: {str(e)}")


def _create_extractor(
    file_path: str | None, code: str | None, target_name: str
) -> tuple["SurgicalExtractor | None", ContextualExtractionResult | None]:
    """
    Create a SurgicalExtractor from file_path or code.

    [20251214_FEATURE] v1.5.3 - Integrated PathResolver for intelligent path resolution

    Returns (extractor, None) on success, (None, error_result) on failure.
    """
    from code_scalpel import SurgicalExtractor
    from code_scalpel.mcp.path_resolver import resolve_path

    if file_path is None and code is None:
        return None, _extraction_error(
            target_name, "Must provide either 'file_path' or 'code' argument"
        )

    if file_path is not None:
        try:
            # [20251214_FEATURE] Use PathResolver for intelligent path resolution
            resolved_path = resolve_path(file_path, str(PROJECT_ROOT))
            return SurgicalExtractor.from_file(resolved_path), None
        except FileNotFoundError as e:
            # PathResolver provides helpful error messages
            return None, _extraction_error(target_name, str(e))
        except ValueError as e:
            return None, _extraction_error(target_name, str(e))
    else:
        try:
            # code is guaranteed to be str here (we checked file_path is None and code is not None above)
            assert code is not None
            return SurgicalExtractor(code), None
        except (SyntaxError, ValueError) as e:
            return None, _extraction_error(
                target_name, f"Syntax error in code: {str(e)}"
            )


def _extract_method(extractor: "SurgicalExtractor", target_name: str):
    """Extract a method, handling the ClassName.method_name parsing."""
    if "." not in target_name:
        return None, _extraction_error(
            target_name, "Method name must be 'ClassName.method_name' format"
        )
    class_name, method_name = target_name.rsplit(".", 1)
    return extractor.get_method(class_name, method_name), None


def _perform_extraction(
    extractor: "SurgicalExtractor",
    target_type: str,
    target_name: str,
    include_context: bool,
    include_cross_file_deps: bool,
    context_depth: int,
    file_path: str | None,
):
    """
    Perform the actual extraction based on target type and options.

    Returns (result, cross_file_result, error_result).
    """
    # [20251228_BUGFIX] Avoid deprecated shim imports.
    from code_scalpel.surgery.surgical_extractor import CrossFileResolution

    cross_file_result: CrossFileResolution | None = None

    # CROSS-FILE RESOLUTION PATH
    if include_cross_file_deps and file_path is not None:
        if target_type in ("function", "class"):
            cross_file_result = extractor.resolve_cross_file_dependencies(
                target_name=target_name,
                target_type=target_type,
                max_depth=context_depth,
            )
            return cross_file_result.target, cross_file_result, None
        else:
            # Method - fall back to regular extraction
            result, error = _extract_method(extractor, target_name)
            return result, None, error

    # INTRA-FILE CONTEXT PATH
    if target_type == "function":
        if include_context:
            return (
                extractor.get_function_with_context(
                    target_name, max_depth=context_depth
                ),
                None,
                None,
            )
        return extractor.get_function(target_name), None, None

    if target_type == "class":
        if include_context:
            return (
                extractor.get_class_with_context(target_name, max_depth=context_depth),
                None,
                None,
            )
        return extractor.get_class(target_name), None, None

    if target_type == "method":
        result, error = _extract_method(extractor, target_name)
        return result, None, error

    return (
        None,
        None,
        _extraction_error(
            target_name,
            f"Unknown target_type: {target_type}. Use 'function', 'class', or 'method'.",
        ),
    )


def _process_cross_file_context(cross_file_result) -> tuple[str, list[str]]:
    """Process cross-file resolution results into context_code and context_items."""
    if cross_file_result is None or not cross_file_result.external_symbols:
        return "", []

    external_parts = []
    external_names = []
    for sym in cross_file_result.external_symbols:
        external_parts.append(f"# From {sym.source_file}")
        external_parts.append(sym.code)
        external_names.append(f"{sym.name} ({sym.source_file})")

    context_code = "\n\n".join(external_parts)

    # Add unresolved imports as a comment
    if cross_file_result.unresolved_imports:
        unresolved_comment = "# Unresolved imports: " + ", ".join(
            cross_file_result.unresolved_imports
        )
        context_code = unresolved_comment + "\n\n" + context_code

    return context_code, external_names


def _build_full_code(
    imports_needed: list[str], context_code: str, target_code: str
) -> str:
    """Build the combined full_code for LLM consumption."""
    parts = []
    if imports_needed:
        parts.append("\n".join(imports_needed))
    if context_code:
        parts.append(context_code)
    parts.append(target_code)
    return "\n\n".join(parts)


@mcp.tool()
async def extract_code(
    target_type: str,
    target_name: str,
    file_path: str | None = None,
    code: str | None = None,
    language: str | None = None,
    include_context: bool = False,
    context_depth: int = 1,
    include_cross_file_deps: bool = False,
    include_token_estimate: bool = True,
    variable_promotion: bool = False,
    closure_detection: bool = False,
    dependency_injection_suggestions: bool = False,
    as_microservice: bool = False,
    microservice_host: str = "127.0.0.1",
    microservice_port: int = 8000,
    organization_wide: bool = False,
    workspace_root: str | None = None,
    ctx: Context | None = None,
) -> ContextualExtractionResult:
    """
    Surgically extract specific code elements (functions, classes, methods).

    **TOKEN-EFFICIENT MODE (RECOMMENDED):**
    Provide `file_path` - the server reads the file directly. The Agent
    never sees the full file content, saving potentially thousands of tokens.

    **MULTI-LANGUAGE SUPPORT (v2.0.0):**
    Supports Python, JavaScript, TypeScript, and Java. Language is auto-detected
    from file extension, or specify explicitly with `language` parameter.

    **CROSS-FILE DEPENDENCIES:**
    Set `include_cross_file_deps=True` to automatically resolve imports.
    If your function uses `TaxRate` from `models.py`, this will extract
    `TaxRate` from `models.py` and include it in the response.

    **LEGACY MODE:**
    Provide `code` as a string - for when you already have code in context.

    [20260102_BUGFIX] Default microservice host now binds to loopback to avoid
    unintended exposure on all interfaces.

    Args:
        target_type: Type of element - "function", "class", or "method".
        target_name: Name of the element. For methods, use "ClassName.method_name".
        file_path: Path to the source file (TOKEN SAVER - server reads file).
        code: Source code string (fallback if file_path not provided).
        language: Language override: "python", "javascript", "typescript", "java".
                  If None, auto-detects from file extension.
        include_context: If True, also extract intra-file dependencies.
        context_depth: How deep to traverse dependencies (1=direct, 2=transitive).
        include_cross_file_deps: If True, resolve imports from external files.
        include_token_estimate: If True, include estimated token count.

    Returns:
        ContextualExtractionResult with extracted code and metadata.

    Example (Efficient - Agent sends ~50 tokens, receives ~200):
        extract_code(
            file_path="/project/src/utils.py",
            target_type="function",
            target_name="calculate_tax"
        )

    Example (JavaScript extraction):
        extract_code(
            file_path="/project/src/utils.js",
            target_type="function",
            target_name="calculateTax"
        )

    Example (Java method extraction):
        extract_code(
            file_path="/project/src/Calculator.java",
            target_type="method",
            target_name="Calculator.add"
        )

    Example (With cross-file dependencies):
        extract_code(
            file_path="/project/src/services/order.py",
            target_type="function",
            target_name="process_order",
            include_cross_file_deps=True
        )
    """
    # [20251215_FEATURE] v2.0.0 - Roots capability support
    # Fetch allowed roots from client for security boundary enforcement
    if ctx and file_path:
        await _fetch_and_cache_roots(ctx)

    # [20251228_BUGFIX] Avoid deprecated shim imports.
    from code_scalpel.surgery.surgical_extractor import (
        ContextualExtraction,
        ExtractionResult,
    )
    from code_scalpel.surgery.unified_extractor import Language, detect_language

    # [20251228_FEATURE] Tier-limited option: cross-file dependency resolution.
    tier = _get_current_tier()
    if include_cross_file_deps and not has_capability(
        "extract_code", "cross_file_deps", tier
    ):
        return ContextualExtractionResult(
            success=False,
            target_name=target_name,
            target_code="",
            context_code="",
            full_code="",
            context_items=[],
            total_lines=0,
            line_start=0,
            line_end=0,
            token_estimate=0,
            error="Feature 'cross_file_deps' requires PRO tier.",
            # [20260111_FEATURE] Output metadata for transparency
            tier_applied=tier,
            language_detected=language,
            cross_file_deps_enabled=False,
            max_depth_applied=None,
        )

    # [20251221_FEATURE] v3.1.0 - Unified extractor for all languages
    # Determine language from parameter, file extension, or code content
    detected_lang = Language.AUTO
    if language:
        lang_map = {
            "python": Language.PYTHON,
            "javascript": Language.JAVASCRIPT,
            "js": Language.JAVASCRIPT,
            "jsx": Language.JAVASCRIPT,  # [20251216_FEATURE] JSX is JavaScript with JSX syntax
            "typescript": Language.TYPESCRIPT,
            "ts": Language.TYPESCRIPT,
            "tsx": Language.TYPESCRIPT,  # [20251216_FEATURE] TSX is TypeScript with JSX syntax
            "java": Language.JAVA,
        }
        detected_lang = lang_map.get(language.lower(), Language.AUTO)

    if detected_lang == Language.AUTO:
        detected_lang = detect_language(file_path, code)

    # [20251214_FEATURE] Route to polyglot extractor for non-Python languages
    if detected_lang != Language.PYTHON:
        return await _extract_polyglot(
            target_type=target_type,
            target_name=target_name,
            file_path=file_path,
            code=code,
            language=detected_lang,
            include_token_estimate=include_token_estimate,
        )

    # Python path - use existing SurgicalExtractor with full context support
    # Step 1: Create extractor
    extractor, error = _create_extractor(file_path, code, target_name)
    if error:
        return error

    # extractor is guaranteed to be non-None when error is None
    assert extractor is not None

    try:
        # Step 2: Perform extraction
        result, cross_file_result, error = _perform_extraction(
            extractor,
            target_type,
            target_name,
            include_context,
            include_cross_file_deps,
            context_depth,
            file_path,
        )
        if error:
            return error

        # Step 3: Handle None result
        if result is None:
            return _extraction_error(
                target_name,
                f"{target_type.capitalize()} '{target_name}' not found in code",
            )

        # Step 4: Process result based on type
        if isinstance(result, ExtractionResult):
            if not result.success:
                return _extraction_error(
                    target_name,
                    result.error
                    or f"{target_type.capitalize()} '{target_name}' not found",
                )
            target_code = result.code
            total_lines = (
                result.line_end - result.line_start + 1 if result.line_end > 0 else 0
            )
            line_start = result.line_start
            line_end = result.line_end
            imports_needed = result.imports_needed

            # Handle cross-file context
            context_code, context_items = _process_cross_file_context(cross_file_result)

        elif isinstance(result, ContextualExtraction):
            if not result.target.success:
                return _extraction_error(
                    target_name,
                    result.target.error
                    or f"{target_type.capitalize()} '{target_name}' not found",
                )
            target_code = result.target.code
            context_items = result.context_items
            context_code = result.context_code
            total_lines = result.total_lines
            line_start = result.target.line_start
            line_end = result.target.line_end
            imports_needed = result.target.imports_needed
        else:
            return _extraction_error(
                target_name, f"Unexpected result type: {type(result).__name__}"
            )

        # Step 5: Build final response
        full_code = _build_full_code(imports_needed, context_code, target_code)
        token_estimate = len(full_code) // 4 if include_token_estimate else 0

        advanced: dict[str, Any] = {}

        # Optional advanced extraction features (Python only)
        def _load_source_for_adv() -> str:
            if file_path is not None:
                from code_scalpel.mcp.path_resolver import resolve_path

                resolved = resolve_path(file_path, str(PROJECT_ROOT))
                return Path(resolved).read_text(encoding="utf-8")
            return code or ""

        if variable_promotion:
            if target_type != "function":
                return _extraction_error(
                    target_name, "variable_promotion requires target_type='function'."
                )
            if not has_capability("extract_code", "variable_promotion", tier):
                return _extraction_error(
                    target_name, "variable_promotion requires PRO tier"
                )
            try:
                from code_scalpel.surgery.surgical_extractor import promote_variables

                promoted = promote_variables(_load_source_for_adv(), target_name)
                if getattr(promoted, "success", False):
                    advanced["variable_promotion"] = {
                        "promoted_function": promoted.promoted_function,
                        "promoted_variables": promoted.promoted_variables,
                        "original_function": promoted.original_function,
                        "explanation": promoted.explanation,
                    }
                else:
                    advanced["variable_promotion"] = {
                        "error": getattr(promoted, "error", None)
                        or "Variable promotion failed",
                    }
            except Exception as e:
                advanced["variable_promotion"] = {
                    "error": f"Variable promotion failed: {e}"
                }

        if closure_detection:
            if target_type != "function":
                return _extraction_error(
                    target_name, "closure_detection requires target_type='function'."
                )
            if tier == "community":
                return _extraction_error(
                    target_name, "closure_detection requires PRO tier"
                )
            try:
                from code_scalpel.surgery.surgical_extractor import (
                    detect_closure_variables as _detect_closures,
                )

                clos = _detect_closures(_load_source_for_adv(), target_name)
                if getattr(clos, "success", False):
                    advanced["closure_detection"] = {
                        "function_name": clos.function_name,
                        "has_closures": clos.has_closures,
                        "closure_variables": [
                            {
                                "name": cv.name,
                                "source": cv.source,
                                "line_number": cv.line_number,
                                "risk_level": cv.risk_level,
                                "captured_from": cv.captured_from,
                                "suggestion": cv.suggestion,
                            }
                            for cv in clos.closure_variables
                        ],
                        "explanation": clos.explanation,
                    }
                else:
                    advanced["closure_detection"] = {
                        "error": getattr(clos, "error", None)
                        or "Closure analysis failed",
                    }
            except Exception as e:
                advanced["closure_detection"] = {
                    "error": f"Closure analysis failed: {e}"
                }

        if dependency_injection_suggestions:
            if target_type != "function":
                return _extraction_error(
                    target_name,
                    "dependency_injection_suggestions requires target_type='function'.",
                )
            if tier == "community":
                return _extraction_error(
                    target_name, "dependency injection suggestions require PRO tier"
                )
            try:
                from code_scalpel.surgery.surgical_extractor import (
                    suggest_dependency_injection as _suggest_di,
                )

                di = _suggest_di(_load_source_for_adv(), target_name)
                if getattr(di, "success", False):
                    advanced["dependency_injection_suggestions"] = {
                        "function_name": di.function_name,
                        "has_suggestions": di.has_suggestions,
                        "original_signature": di.original_signature,
                        "refactored_signature": di.refactored_signature,
                        "suggestions": [
                            {
                                "variable_name": s.variable_name,
                                "current_usage": s.current_usage,
                                "suggested_parameter": s.suggested_parameter,
                                "suggested_default": s.suggested_default,
                                "benefit": s.benefit,
                                "refactored_signature": s.refactored_signature,
                            }
                            for s in di.suggestions
                        ],
                        "explanation": di.explanation,
                    }
                else:
                    advanced["dependency_injection_suggestions"] = {
                        "error": getattr(di, "error", None)
                        or "Dependency injection analysis failed",
                    }
            except Exception as e:
                advanced["dependency_injection_suggestions"] = {
                    "error": f"DI analysis failed: {e}"
                }

        if as_microservice:
            if target_type != "function":
                return _extraction_error(
                    target_name, "as_microservice requires target_type='function'."
                )
            if not has_capability("extract_code", "dockerfile_generation", tier):
                return _extraction_error(
                    target_name, "as_microservice requires ENTERPRISE tier"
                )
            try:
                from code_scalpel.surgery.surgical_extractor import (
                    extract_as_microservice as _extract_microservice,
                )

                ms = _extract_microservice(
                    _load_source_for_adv(),
                    target_name,
                    microservice_host,
                    microservice_port,
                )
                if getattr(ms, "success", False):
                    advanced["microservice"] = {
                        "function_code": ms.function_code,
                        "dockerfile": ms.dockerfile,
                        "api_spec": ms.api_spec,
                        "requirements_txt": ms.requirements_txt,
                        "readme": ms.readme,
                    }
                else:
                    advanced["microservice"] = {
                        "error": getattr(ms, "error", None)
                        or "Microservice extraction failed",
                    }
            except Exception as e:
                advanced["microservice"] = {
                    "error": f"Microservice extraction failed: {e}"
                }

        if organization_wide:
            if not has_capability("extract_code", "org_wide_resolution", tier):
                return _extraction_error(
                    target_name, "organization_wide requires ENTERPRISE tier"
                )
            if not code:
                return _extraction_error(
                    target_name, "organization_wide requires 'code' input"
                )
            try:
                from code_scalpel.surgery.surgical_extractor import (
                    resolve_organization_wide as _resolve_org,
                )

                org = _resolve_org(
                    code=code, function_name=target_name, workspace_root=workspace_root
                )
                if getattr(org, "success", False):
                    advanced["organization_wide_resolution"] = {
                        "target_name": org.target_name,
                        "target_code": org.target_code,
                        "cross_repo_imports": [
                            {
                                "repo_name": imp.repo_name,
                                "file_path": imp.file_path,
                                "symbols": imp.symbols,
                                "repo_root": imp.repo_root,
                            }
                            for imp in org.cross_repo_imports
                        ],
                        "resolved_symbols": org.resolved_symbols,
                        "monorepo_structure": org.monorepo_structure,
                        "explanation": org.explanation,
                    }
                else:
                    advanced["organization_wide_resolution"] = {
                        "error": getattr(org, "error", None)
                        or "Organization-wide resolution failed",
                    }
            except Exception as e:
                advanced["organization_wide_resolution"] = {
                    "error": f"Organization-wide resolution failed: {e}"
                }

        # [20260111_FEATURE] Get tier limits for metadata
        from code_scalpel.licensing.config_loader import get_tool_limits

        limits = get_tool_limits("extract_code", tier)
        max_depth_limit = limits.get("max_depth")

        return ContextualExtractionResult(
            success=True,
            target_name=target_name,
            target_code=target_code,
            context_code=context_code,
            full_code=full_code,
            context_items=context_items,
            total_lines=total_lines,
            line_start=line_start,
            line_end=line_end,
            token_estimate=token_estimate,
            # [20260111_FEATURE] Output metadata for transparency
            tier_applied=tier,
            language_detected="python",
            cross_file_deps_enabled=include_cross_file_deps,
            max_depth_applied=max_depth_limit,
            advanced=advanced,
        )

    except Exception as e:
        return _extraction_error(target_name, f"Extraction failed: {str(e)}")


@mcp.tool()
async def rename_symbol(
    file_path: str,
    target_type: str,
    target_name: str,
    new_name: str,
    create_backup: bool = True,
) -> PatchResultModel:
    """
    Rename a function, class, or method in a file.

    This updates the definition of the symbol.

    Args:
        file_path: Path to the Python source file.
        target_type: "function", "class", or "method".
        target_name: Current name (e.g., "my_func" or "MyClass.my_method").
        new_name: New name (e.g., "new_func" or "new_method").
        create_backup: If True (default), create a .bak file before modifying.

    Returns:
        PatchResultModel with success status.
    """
    from code_scalpel.licensing.config_loader import (
        get_cached_limits,
        get_tool_limits,
        merge_limits,
    )
    from code_scalpel.licensing.features import get_tool_capabilities
    from code_scalpel.mcp.path_resolver import resolve_path
    from code_scalpel.surgery.surgical_patcher import UnifiedPatcher

    warnings: list[str] = []

    try:
        resolved = resolve_path(file_path, str(PROJECT_ROOT))
        resolved_path = Path(resolved)
        _validate_path_security(resolved_path)
        file_path = str(resolved_path)

        # [20260103_BUGFIX] Use UnifiedPatcher for automatic language detection
        patcher = UnifiedPatcher.from_file(file_path)
    except FileNotFoundError:
        return PatchResultModel(
            success=False,
            file_path=file_path,
            target_name=target_name,
            target_type=target_type,
            error=f"File not found: {file_path}",
        )
    except ValueError as e:
        return PatchResultModel(
            success=False,
            file_path=file_path,
            target_name=target_name,
            target_type=target_type,
            error=str(e),
        )

    result = patcher.rename_symbol(target_type, target_name, new_name)

    if result.success:
        backup_path = patcher.save(backup=create_backup)

        # V1.0: Pro/Enterprise attempt conservative cross-file refactor.
        tier = _get_current_tier()
        caps = get_tool_capabilities("rename_symbol", tier)
        default_limits = caps.get("limits", {})
        config = get_cached_limits()
        overrides = get_tool_limits("rename_symbol", tier, config=config)
        limits = merge_limits(default_limits, overrides)

        max_files_searched = limits.get("max_files_searched")
        max_files_updated = limits.get("max_files_updated")

        # If configured as definition-only, skip cross-file updates.
        if tier in {"pro", "enterprise"} and not (
            (max_files_searched == 0) and (max_files_updated == 0)
        ):
            try:
                from code_scalpel.surgery.rename_symbol_refactor import (
                    rename_references_across_project,
                )

                xres = rename_references_across_project(
                    project_root=Path(PROJECT_ROOT),
                    target_file=Path(file_path),
                    target_type=target_type,
                    target_name=target_name,
                    new_name=new_name,
                    create_backup=create_backup,
                    max_files_searched=max_files_searched,
                    max_files_updated=max_files_updated,
                )

                if not xres.success:
                    warnings.append(
                        f"Cross-file rename skipped: {xres.error or 'unknown error'}"
                    )
                elif xres.changed_files:
                    warnings.append(
                        f"Updated references/imports in {len(xres.changed_files)} additional file(s)."
                    )

                warnings.extend(xres.warnings)
            except Exception as e:
                warnings.append(f"Cross-file rename skipped due to error: {e}")
        else:
            warnings.append(
                "Definition-only rename (no cross-file updates) at this tier."
            )

        return PatchResultModel(
            success=True,
            file_path=file_path,
            target_name=target_name,
            target_type=target_type,
            lines_before=result.lines_before,
            lines_after=result.lines_after,
            backup_path=backup_path,
            warnings=warnings,
        )
    else:
        return PatchResultModel(
            success=False,
            file_path=file_path,
            target_name=target_name,
            target_type=target_type,
            error=result.error,
            warnings=warnings,
        )


async def _perform_atomic_git_refactor(
    file_path: str, target_name: str, new_code: str
) -> dict[str, Any]:
    """
    Enterprise tier: Atomic refactoring with git branch creation and test execution.

    [20251230_FEATURE] v3.5.0 - Enterprise tier atomic refactoring.

    Creates a git branch, applies changes, runs tests, and reverts if tests fail.

    Args:
        file_path: Path to modified file
        target_name: Name of the modified symbol
        new_code: The new code

    Returns:
        Dictionary with branch_name, tests_passed, and revert status
    """
    import subprocess

    result = {"branch_name": None, "tests_passed": None, "reverted": False}

    try:
        from datetime import datetime
        from pathlib import Path

        # Get project root (look for .git)
        file_path_obj = Path(file_path).resolve()
        project_root = file_path_obj.parent

        while project_root.parent != project_root:
            if (project_root / ".git").exists():
                break
            project_root = project_root.parent

        if not (project_root / ".git").exists():
            # No git repo found
            return result

        # Generate branch name
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        branch_name = f"refactor/{target_name.replace('.', '_')}_{timestamp}"
        result["branch_name"] = branch_name

        # Create and checkout new branch
        subprocess.run(
            ["git", "checkout", "-b", branch_name],
            cwd=project_root,
            capture_output=True,
            check=True,
        )

        # Commit the changes
        subprocess.run(
            ["git", "add", str(file_path_obj.relative_to(project_root))],
            cwd=project_root,
            capture_output=True,
            check=True,
        )

        subprocess.run(
            ["git", "commit", "-m", f"Refactor: Update {target_name}"],
            cwd=project_root,
            capture_output=True,
            check=True,
        )

        # Run tests (look for common test commands)
        test_commands = [
            ["pytest", "-x"],  # Stop on first failure
            ["python", "-m", "pytest", "-x"],
            ["python", "-m", "unittest", "discover"],
            ["npm", "test"],
        ]

        tests_run = False
        tests_passed_flag = False

        for cmd in test_commands:
            try:
                test_result = subprocess.run(
                    cmd,
                    cwd=project_root,
                    capture_output=True,
                    timeout=300,  # 5 minute timeout
                )
                tests_run = True
                tests_passed_flag = test_result.returncode == 0
                break
            except (FileNotFoundError, subprocess.TimeoutExpired):
                continue

        result["tests_passed"] = tests_passed_flag if tests_run else None

        # If tests failed, revert
        if tests_run and not tests_passed_flag:
            # Go back to previous branch
            subprocess.run(
                ["git", "checkout", "-"], cwd=project_root, capture_output=True
            )
            # Delete the failed branch
            subprocess.run(
                ["git", "branch", "-D", branch_name],
                cwd=project_root,
                capture_output=True,
            )
            result["reverted"] = True

        return result

    except Exception as e:
        # If anything fails, try to return to original branch
        try:
            subprocess.run(["git", "checkout", "-"], capture_output=True)
        except Exception:
            # [20260102_BUGFIX] Avoid bare except while preserving best-effort cleanup
            pass
        result["error"] = str(e)
        return result


async def _update_cross_file_references(
    modified_file: str, target_type: str, target_name: str, new_code: str
) -> dict[str, Any]:
    """
    Pro tier: Update cross-file references when a symbol changes.

    [20251230_FEATURE] v3.5.0 - Pro tier cross-file reference updates.

    Scans project for files that reference the modified symbol and updates
    import statements or call sites if needed.

    Args:
        modified_file: Path to the file that was modified
        target_type: Type of symbol (function/class/method)
        target_name: Name of the symbol
        new_code: The new code (to detect signature changes)

    Returns:
        Dictionary with files_updated count and list of updated files
    """
    result = {"files_updated": 0, "updated_files": [], "errors": []}

    try:
        # Get project root
        from pathlib import Path

        modified_path = Path(modified_file).resolve()
        project_root = modified_path.parent

        # Find project root by looking for .git or pyproject.toml
        while project_root.parent != project_root:
            if (project_root / ".git").exists() or (
                project_root / "pyproject.toml"
            ).exists():
                break
            project_root = project_root.parent

        # Use get_symbol_references to find all references
        from code_scalpel.mcp.server import _get_symbol_references_sync

        refs_result = await asyncio.to_thread(
            _get_symbol_references_sync, target_name, str(project_root)
        )

        if not refs_result.success or not refs_result.references:
            return result

        # Check if signature changed
        new_sig = None

        try:
            # Parse new code to get signature
            new_tree = ast.parse(new_code)
            for node in ast.walk(new_tree):
                if isinstance(node, ast.FunctionDef) and node.name == target_name:
                    new_sig = ast.unparse(node.args)
                    break
        except Exception:
            # [20260102_BUGFIX] Avoid bare except while keeping best-effort parsing
            pass

        if not new_sig:
            # No signature change detected, skip updates
            return result

        # Update files with references (excluding the modified file itself)
        for ref in refs_result.references:
            if ref.file == modified_file or ref.is_definition:
                continue

            ref_path = project_root / ref.file
            if not ref_path.exists():
                continue

            try:
                # Read file
                with open(ref_path, "r", encoding="utf-8") as f:
                    content = f.read()

                # Simple heuristic: if the reference line contains a function call,
                # add a comment about potential signature change
                lines = content.splitlines()
                if 0 <= ref.line - 1 < len(lines):
                    line = lines[ref.line - 1]
                    if f"{target_name}(" in line:
                        # Add warning comment
                        indent = len(line) - len(line.lstrip())
                        warning = (
                            " " * indent
                            + f"# WARNING: {target_name} signature may have changed\n"
                        )
                        lines.insert(ref.line - 1, warning)

                        # Write back
                        new_content = "\n".join(lines) + "\n"
                        with open(ref_path, "w", encoding="utf-8") as f:
                            f.write(new_content)

                        result["files_updated"] += 1
                        result["updated_files"].append(str(ref_path))
            except Exception as e:
                result["errors"].append(f"{ref.file}: {str(e)}")

        return result

    except Exception as e:
        result["errors"].append(f"Cross-file update failed: {str(e)}")
        return result


# [20250101_FEATURE] v1.0 roadmap: Enterprise code review approval helper
async def _check_code_review_approval(
    file_path: str,
    target_name: str,
    target_type: str,
    new_code: str | None,
) -> dict[str, Any]:
    """
    Enterprise tier: Check if code review approval is required and granted.

    [20250101_FEATURE] v1.0 roadmap - Enterprise code review approval.

    This checks for approval requirements based on:
    - File sensitivity (e.g., security-critical paths)
    - Symbol visibility (public APIs)
    - Change magnitude

    In a full implementation, this would integrate with code review systems
    like GitHub PRs, GitLab MRs, or enterprise approval workflows.

    Args:
        file_path: Path to the file being modified
        target_name: Name of the symbol being modified
        target_type: Type of symbol
        new_code: The new code

    Returns:
        Dictionary with approved status, reviewer info, and reason
    """
    # Default: approved (for now - full implementation would check approval system)
    result = {
        "approved": True,
        "reviewer": None,
        "reason": None,
        "requires_review": False,
    }

    # Check for sensitive paths that require review
    sensitive_paths = [
        "security",
        "auth",
        "crypto",
        "payment",
        "admin",
    ]

    file_lower = file_path.lower()
    for sensitive in sensitive_paths:
        if sensitive in file_lower:
            result["requires_review"] = True
            # For now, auto-approve with warning
            result["reason"] = f"File in sensitive path: {sensitive}"
            break

    return result


# [20250101_FEATURE] v1.0 roadmap: Enterprise compliance check helper
async def _check_compliance(
    file_path: str,
    target_name: str,
    new_code: str | None,
) -> dict[str, Any]:
    """
    Enterprise tier: Check compliance rules before allowing mutation.

    [20250101_FEATURE] v1.0 roadmap - Enterprise compliance checking.

    Validates code changes against:
    - Security policies (no secrets, no unsafe patterns)
    - Style guidelines (formatting, naming conventions)
    - Architecture rules (layering, dependencies)

    Args:
        file_path: Path to the file being modified
        target_name: Name of the symbol being modified
        new_code: The new code

    Returns:
        Dictionary with compliant status, violations, and warnings
    """
    result = {
        "compliant": True,
        "violations": [],
        "warnings": [],
    }

    if not new_code:
        return result

    # Basic compliance checks
    code_lower = new_code.lower()

    # Check for hardcoded secrets
    secret_patterns = [
        ("password", "Hardcoded password detected"),
        ("secret_key", "Hardcoded secret key detected"),
        ("api_key", "Hardcoded API key detected"),
        ("private_key", "Hardcoded private key detected"),
    ]

    for pattern, message in secret_patterns:
        if f'{pattern} = "' in code_lower or f"{pattern} = '" in code_lower:
            result["warnings"].append(f"Compliance warning: {message}")

    # Check for unsafe patterns
    unsafe_patterns = [
        ("eval(", "Use of eval() detected - security risk"),
        ("exec(", "Use of exec() detected - security risk"),
        ("__import__(", "Dynamic import detected - review required"),
    ]

    for pattern, message in unsafe_patterns:
        if pattern in new_code:
            result["warnings"].append(f"Compliance warning: {message}")

    return result


# [20250101_FEATURE] v1.0 roadmap: Pro pre-update hook helper
async def _run_pre_update_hook(
    file_path: str,
    target_name: str,
    target_type: str,
    new_code: str | None,
) -> dict[str, Any]:
    """
    Pro tier: Run pre-update hook before applying changes.

    [20250101_FEATURE] v1.0 roadmap - Pro pre/post update hooks.

    Pre-update hooks can:
    - Validate custom business rules
    - Check for conflicts
    - Log upcoming changes
    - Block updates based on conditions

    Args:
        file_path: Path to the file being modified
        target_name: Name of the symbol being modified
        target_type: Type of symbol
        new_code: The new code

    Returns:
        Dictionary with continue flag and any warnings
    """
    result = {
        "continue": True,
        "warnings": [],
    }

    # Check for .code-scalpel/hooks/pre_update.py
    from pathlib import Path

    hooks_dir = Path(file_path).parent

    while hooks_dir.parent != hooks_dir:
        hook_path = hooks_dir / ".code-scalpel" / "hooks" / "pre_update.py"
        if hook_path.exists():
            try:
                # Execute the hook (in sandbox)
                import importlib.util

                spec = importlib.util.spec_from_file_location("pre_update", hook_path)
                if spec and spec.loader:
                    hook_module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(hook_module)

                    if hasattr(hook_module, "pre_update"):
                        hook_result = hook_module.pre_update(
                            file_path=file_path,
                            target_name=target_name,
                            target_type=target_type,
                            new_code=new_code,
                        )
                        if isinstance(hook_result, dict):
                            result.update(hook_result)
            except Exception as e:
                result["warnings"].append(f"Pre-update hook warning: {str(e)}")
            break
        hooks_dir = hooks_dir.parent

    return result


# [20250101_FEATURE] v1.0 roadmap: Pro post-update hook helper
async def _run_post_update_hook(
    file_path: str,
    target_name: str,
    target_type: str,
    result: Any,
) -> dict[str, Any]:
    """
    Pro tier: Run post-update hook after applying changes.

    [20250101_FEATURE] v1.0 roadmap - Pro pre/post update hooks.

    Post-update hooks can:
    - Log completed changes
    - Trigger downstream actions
    - Update documentation
    - Notify stakeholders

    Args:
        file_path: Path to the file that was modified
        target_name: Name of the symbol that was modified
        target_type: Type of symbol
        result: The patch result

    Returns:
        Dictionary with any additional warnings
    """
    hook_result = {
        "warnings": [],
    }

    # Check for .code-scalpel/hooks/post_update.py
    from pathlib import Path

    hooks_dir = Path(file_path).parent

    while hooks_dir.parent != hooks_dir:
        hook_path = hooks_dir / ".code-scalpel" / "hooks" / "post_update.py"
        if hook_path.exists():
            try:
                import importlib.util

                spec = importlib.util.spec_from_file_location("post_update", hook_path)
                if spec and spec.loader:
                    hook_module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(hook_module)

                    if hasattr(hook_module, "post_update"):
                        post_result = hook_module.post_update(
                            file_path=file_path,
                            target_name=target_name,
                            target_type=target_type,
                            success=(
                                result.success if hasattr(result, "success") else True
                            ),
                        )
                        if isinstance(post_result, dict):
                            hook_result.update(post_result)
            except Exception as e:
                hook_result["warnings"].append(f"Post-update hook warning: {str(e)}")
            break
        hooks_dir = hooks_dir.parent

    return hook_result


@mcp.tool()
async def update_symbol(
    file_path: str,
    target_type: str,
    target_name: str,
    new_code: str | None = None,
    operation: str = "replace",
    new_name: str | None = None,
    create_backup: bool = True,
) -> PatchResultModel:
    """
        Surgically replace a function, class, or method in a file with new code.

        This is the SAFE way to modify code - you provide only the new symbol,
        and the server handles:
        - Locating the exact symbol boundaries (including decorators)
        - Validating the replacement code syntax
        - Preserving all surrounding code exactly
        - Creating a backup before modification
        - Atomic write (prevents partial writes)

        Args:
            file_path: Path to the Python source file to modify.
            target_type: Type of element - "function", "class", or "method".
            target_name: Name of the element. For methods, use "ClassName.method_name".
            new_code: The complete new definition (including def/class line and body).
            create_backup: If True (default), create a .bak file before modifying.

        Returns:
            PatchResultModel with success status, line changes, and backup path.

        Example (Fix a function):
            update_symbol(
                file_path="/project/src/utils.py",
                target_type="function",
                target_name="calculate_tax",
                new_code='''def calculate_tax(amount, rate=0.1):
        \"\"\"Calculate tax with proper rounding.\"\"\"
        return round(amount * rate, 2)
    '''
            )

        Example (Update a method):
            update_symbol(
                file_path="/project/src/models.py",
                target_type="method",
                target_name="User.validate_email",
                new_code='''def validate_email(self, email):
        import re
        pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'
        return bool(re.match(pattern, email))
    '''
            )

        Safety Features:
            - Backup created at {file_path}.bak (unless create_backup=False)
            - Syntax validation before any file modification
            - Atomic write prevents corruption on crash
            - Original indentation preserved
    """
    # [20251228_BUGFIX] Avoid deprecated shim imports.
    from code_scalpel.mcp.path_resolver import resolve_path
    from code_scalpel.surgery.surgical_patcher import UnifiedPatcher

    # [20251225_FEATURE] Tier-based behavior via capability matrix (no upgrade hints).
    tier = _get_current_tier()
    caps = get_tool_capabilities("update_symbol", tier)
    capabilities = set(caps.get("capabilities", set()))
    limits = caps.get("limits", {})
    warnings: list[str] = []

    # [20250101_FEATURE] v1.0 roadmap: Enforce session update limits for Community tier
    max_updates = limits.get("max_updates_per_session", -1)
    if max_updates > 0:  # -1 means unlimited
        current_count = _get_session_update_count("update_symbol")
        if current_count >= max_updates:
            return PatchResultModel(
                success=False,
                file_path=file_path or "",
                target_name=target_name,
                target_type=target_type,
                error_code="UPDATE_SYMBOL_SESSION_LIMIT_REACHED",
                hint="Start a new session or reduce updates per session.",
                max_updates_per_session=int(max_updates),
                updates_used=int(current_count),
                updates_remaining=0,
                warnings=warnings,
                error=f"Session limit reached: {max_updates} updates per session.",
            )

    # [20251228_BUGFIX] Honor create_backup unless the tier explicitly requires it.
    # 'backup_enabled' means the feature is supported; it is not a mandate.
    backup_required = bool(limits.get("backup_required", False))
    if backup_required and not create_backup:
        create_backup = True
        warnings.append("Backup was required for this operation.")

    validation_level = str(limits.get("validation_level", "syntax"))

    def _semantic_name_check() -> str | None:
        """Best-effort semantic validation that the replacement defines the target."""
        if new_code is None:
            return "Replacement code is required for operation='replace'."
        try:
            tree = ast.parse(new_code)
        except SyntaxError as e:
            return f"Replacement code has syntax error: {e}"

        if not tree.body:
            return "Replacement code is empty."

        first = tree.body[0]
        if target_type == "function":
            if not isinstance(first, (ast.FunctionDef, ast.AsyncFunctionDef)):
                return "Replacement for function must be a function definition."
            if first.name != target_name:
                return f"Replacement function name '{first.name}' does not match target '{target_name}'."
        elif target_type == "class":
            if not isinstance(first, ast.ClassDef):
                return "Replacement for class must be a class definition."
            if first.name != target_name:
                return f"Replacement class name '{first.name}' does not match target '{target_name}'."
        elif target_type == "method":
            if "." not in target_name:
                return "Method name must be 'ClassName.method_name' format."
            class_name, method_name = target_name.rsplit(".", 1)
            if not isinstance(first, (ast.FunctionDef, ast.AsyncFunctionDef)):
                return "Replacement for method must be a function definition."
            if first.name != method_name:
                return f"Replacement method name '{first.name}' does not match target '{class_name}.{method_name}'."

        return None

    operation = (operation or "replace").strip().lower()
    if operation not in {"replace", "rename"}:
        return PatchResultModel(
            success=False,
            file_path=file_path or "",
            target_name=target_name,
            target_type=target_type,
            error_code="UPDATE_SYMBOL_INVALID_OPERATION",
            warnings=warnings,
            error="Invalid operation. Use 'replace' or 'rename'.",
        )

    # Validate inputs
    if not file_path:
        return PatchResultModel(
            success=False,
            file_path="",
            target_name=target_name,
            target_type=target_type,
            error_code="UPDATE_SYMBOL_MISSING_FILE_PATH",
            warnings=warnings,
            error="Parameter 'file_path' is required.",
        )

    if operation == "replace":
        if not new_code or not str(new_code).strip():
            return PatchResultModel(
                success=False,
                file_path=file_path,
                target_name=target_name,
                target_type=target_type,
                error_code="UPDATE_SYMBOL_MISSING_NEW_CODE",
                warnings=warnings,
                error="Parameter 'new_code' cannot be empty for operation='replace'.",
            )
    else:
        if not new_name or not new_name.strip():
            return PatchResultModel(
                success=False,
                file_path=file_path,
                target_name=target_name,
                target_type=target_type,
                error_code="UPDATE_SYMBOL_MISSING_NEW_NAME",
                warnings=warnings,
                error="Parameter 'new_name' is required for operation='rename'.",
            )

    if target_type not in ("function", "class", "method"):
        return PatchResultModel(
            success=False,
            file_path=file_path,
            target_name=target_name,
            target_type=target_type,
            error_code="UPDATE_SYMBOL_INVALID_TARGET_TYPE",
            warnings=warnings,
            error=f"Invalid target_type: {target_type}. Use 'function', 'class', or 'method'.",
        )

    # Community+ (all tiers): semantic validation that the replacement defines the
    # target symbol (prevents accidental renames via replacement code).
    if operation == "replace":
        semantic_error = _semantic_name_check()
        if semantic_error:
            return PatchResultModel(
                success=False,
                file_path=file_path,
                target_name=target_name,
                target_type=target_type,
                error_code="UPDATE_SYMBOL_SEMANTIC_VALIDATION_FAILED",
                warnings=warnings,
                error=semantic_error,
            )

    # Load the file
    try:
        resolved = resolve_path(file_path, str(PROJECT_ROOT))
        resolved_path = Path(resolved)
        _validate_path_security(resolved_path)
        file_path = str(resolved_path)

        # [20260103_BUGFIX] Use UnifiedPatcher for automatic language detection
        # UnifiedPatcher auto-detects language from file extension and routes to
        # appropriate parser (SurgicalPatcher for Python, PolyglotPatcher for JS/TS/Java)
        patcher = UnifiedPatcher.from_file(file_path)
    except FileNotFoundError:
        return PatchResultModel(
            success=False,
            file_path=file_path,
            target_name=target_name,
            target_type=target_type,
            error_code="UPDATE_SYMBOL_FILE_NOT_FOUND",
            warnings=warnings,
            error=f"File not found: {file_path}.",
        )
    except ValueError as e:
        return PatchResultModel(
            success=False,
            file_path=file_path,
            target_name=target_name,
            target_type=target_type,
            error_code="UPDATE_SYMBOL_INVALID_INPUT",
            warnings=warnings,
            error=str(e),
        )

    # [20250101_FEATURE] v1.0 roadmap: Enterprise code review approval
    if "code_review_approval" in capabilities:
        approval_result = await _check_code_review_approval(
            file_path, target_name, target_type, new_code
        )
        if not approval_result.get("approved", True):
            return PatchResultModel(
                success=False,
                file_path=file_path,
                target_name=target_name,
                target_type=target_type,
                error_code="UPDATE_SYMBOL_APPROVAL_REQUIRED",
                warnings=warnings,
                error=f"Code review approval required: {approval_result.get('reason', 'Pending review')}",
            )
        if approval_result.get("reviewer"):
            warnings.append(f"Approved by: {approval_result['reviewer']}")

    # [20250101_FEATURE] v1.0 roadmap: Enterprise compliance check
    if "compliance_check" in capabilities:
        compliance_result = await _check_compliance(file_path, target_name, new_code)
        if not compliance_result.get("compliant", True):
            return PatchResultModel(
                success=False,
                file_path=file_path,
                target_name=target_name,
                target_type=target_type,
                error_code="UPDATE_SYMBOL_COMPLIANCE_FAILED",
                warnings=warnings,
                error=f"Compliance check failed: {compliance_result.get('violations', [])}",
            )
        if compliance_result.get("warnings"):
            warnings.extend(compliance_result["warnings"])

    # [20250101_FEATURE] v1.0 roadmap: Pro pre-update hook
    if "pre_update_hook" in capabilities:
        hook_result = await _run_pre_update_hook(
            file_path, target_name, target_type, new_code
        )
        if not hook_result.get("continue", True):
            return PatchResultModel(
                success=False,
                file_path=file_path,
                target_name=target_name,
                target_type=target_type,
                error_code="UPDATE_SYMBOL_PRE_HOOK_BLOCKED",
                warnings=warnings,
                error=f"Pre-update hook blocked: {hook_result.get('reason', 'Hook returned false')}",
            )
        if hook_result.get("warnings"):
            warnings.extend(hook_result["warnings"])

    # Apply the patch based on target type
    try:
        if operation == "rename":
            # Tier-aware rename support. Delegates to the dedicated tool which
            # enforces tier limits/capabilities (Community definition-only; Pro/Ent
            # may update references depending on configured limits).
            rename_result = await rename_symbol(
                file_path=file_path,
                target_type=target_type,
                target_name=target_name,
                new_name=str(new_name),
                create_backup=create_backup,
            )

            # [20260110_BUGFIX] Prevent session-limit bypass via update_symbol(operation='rename').
            if getattr(rename_result, "success", False):
                new_count = _increment_session_update_count("update_symbol")

                if "audit_trail" in capabilities:
                    _add_audit_entry(
                        tool_name="update_symbol",
                        file_path=file_path,
                        target_name=target_name,
                        operation=operation,
                        success=True,
                        tier=tier,
                        metadata={
                            "rename_delegated": True,
                            "new_name": str(new_name),
                            "backup_path": getattr(rename_result, "backup_path", None),
                        },
                    )

                merged_warnings: list[str] = []
                merged_warnings.extend(warnings)
                merged_warnings.extend(getattr(rename_result, "warnings", []) or [])

                return PatchResultModel(
                    success=True,
                    file_path=getattr(rename_result, "file_path", file_path),
                    target_name=getattr(rename_result, "target_name", target_name),
                    target_type=getattr(rename_result, "target_type", target_type),
                    lines_before=getattr(rename_result, "lines_before", 0),
                    lines_after=getattr(rename_result, "lines_after", 0),
                    lines_delta=getattr(rename_result, "lines_delta", 0),
                    backup_path=getattr(rename_result, "backup_path", None),
                    max_updates_per_session=(
                        int(max_updates) if max_updates > 0 else None
                    ),
                    updates_used=(int(new_count) if max_updates > 0 else None),
                    updates_remaining=(
                        int(max_updates - new_count) if max_updates > 0 else None
                    ),
                    warnings=merged_warnings,
                )

            # Failure case (or no session tracking): return as-is.
            return PatchResultModel(
                success=getattr(rename_result, "success", False),
                file_path=getattr(rename_result, "file_path", file_path),
                target_name=getattr(rename_result, "target_name", target_name),
                target_type=getattr(rename_result, "target_type", target_type),
                lines_before=getattr(rename_result, "lines_before", 0),
                lines_after=getattr(rename_result, "lines_after", 0),
                lines_delta=getattr(rename_result, "lines_delta", 0),
                backup_path=getattr(rename_result, "backup_path", None),
                error=getattr(rename_result, "error", None),
                error_code=getattr(rename_result, "error_code", None),
                hint=getattr(rename_result, "hint", None),
                max_updates_per_session=(int(max_updates) if max_updates > 0 else None),
                updates_used=(
                    int(_get_session_update_count("update_symbol"))
                    if max_updates > 0
                    else None
                ),
                updates_remaining=(
                    int(max_updates - _get_session_update_count("update_symbol"))
                    if max_updates > 0
                    else None
                ),
                warnings=(
                    (getattr(rename_result, "warnings", []) or [])
                    if not warnings
                    else (warnings + (getattr(rename_result, "warnings", []) or []))
                ),
            )

        # At this point we are performing a replacement, which requires concrete code.
        assert new_code is not None

        if target_type == "function":
            result = patcher.update_function(target_name, new_code)
            # [20251229_FEATURE] Auto-insert if not found
            if not result.success and "not found" in (result.error or ""):
                # [20260101_BUGFIX] Verify method exists before calling
                if hasattr(patcher, "insert_function") and callable(
                    getattr(patcher, "insert_function")
                ):
                    result = patcher.insert_function(new_code)  # type: ignore[attr-defined]
                if result.success:
                    warnings.append(
                        f"Function '{target_name}' was not found, so it was inserted."
                    )

        elif target_type == "class":
            result = patcher.update_class(target_name, new_code)
            # [20251229_FEATURE] Auto-insert if not found
            if not result.success and "not found" in (result.error or ""):
                # [20260101_BUGFIX] Verify method exists before calling
                if hasattr(patcher, "insert_class") and callable(
                    getattr(patcher, "insert_class")
                ):
                    result = patcher.insert_class(new_code)  # type: ignore[attr-defined]
                if result.success:
                    warnings.append(
                        f"Class '{target_name}' was not found, so it was inserted."
                    )

        elif target_type == "method":
            if "." not in target_name:
                return PatchResultModel(
                    success=False,
                    file_path=file_path,
                    target_name=target_name,
                    target_type=target_type,
                    error_code="UPDATE_SYMBOL_INVALID_METHOD_NAME",
                    warnings=warnings,
                    error="Method name must be 'ClassName.method_name' format.",
                )
            class_name, method_name = target_name.rsplit(".", 1)
            result = patcher.update_method(class_name, method_name, new_code)
            # [20251229_FEATURE] Auto-insert if not found (but class must exist)
            if (
                not result.success
                and "not found" in (result.error or "")
                and "Class" not in (result.error or "")
            ):
                # [20260101_BUGFIX] Verify method exists before calling
                if hasattr(patcher, "insert_method") and callable(
                    getattr(patcher, "insert_method")
                ):
                    result = patcher.insert_method(class_name, new_code)  # type: ignore[attr-defined]
                if result.success:
                    warnings.append(
                        f"Method '{method_name}' was not found in '{class_name}', so it was inserted."
                    )
        else:
            # Should not reach here due to validation above
            return PatchResultModel(
                success=False,
                file_path=file_path,
                target_name=target_name,
                target_type=target_type,
                warnings=warnings,
                error=f"Unknown target_type: {target_type}.",
            )

        if not result.success:
            return PatchResultModel(
                success=False,
                file_path=file_path,
                target_name=target_name,
                target_type=target_type,
                error_code="UPDATE_SYMBOL_PATCH_FAILED",
                warnings=warnings,
                error=result.error,
            )

        # Save the changes
        backup_path = patcher.save(backup=create_backup)

        # [20251227_BUGFIX] Ensure filesystem sync after atomic write
        import os as os_module

        if hasattr(os_module, "sync"):
            os_module.sync()

        # [20251230_FEATURE] v3.5.0 - Pro tier: Update cross-file references
        if "cross_file_updates" in capabilities:
            try:
                cross_file_updates = await _update_cross_file_references(
                    file_path, target_type, target_name, new_code
                )
                if cross_file_updates["files_updated"] > 0:
                    warnings.append(
                        f"Updated {cross_file_updates['files_updated']} files with reference changes"
                    )
            except Exception as e:
                warnings.append(f"Cross-file update warning: {str(e)}")

        # [20250101_FEATURE] v1.0 roadmap: Pro post-update hook
        if "post_update_hook" in capabilities:
            try:
                post_hook_result = await _run_post_update_hook(
                    file_path, target_name, target_type, result
                )
                if post_hook_result.get("warnings"):
                    warnings.extend(post_hook_result["warnings"])
            except Exception as e:
                warnings.append(f"Post-update hook warning: {str(e)}")

        # [20251230_FEATURE] v3.5.0 - Enterprise tier: Git integration with branch & tests
        git_branch = None
        tests_passed = None
        if "git_integration" in capabilities:
            try:
                git_result = await _perform_atomic_git_refactor(
                    file_path, target_name, new_code
                )
                git_branch = git_result.get("branch_name")
                tests_passed = git_result.get("tests_passed")

                if git_branch:
                    warnings.append(f"Created git branch: {git_branch}")

                if tests_passed is False:
                    # Tests failed - rollback everything
                    if git_result.get("reverted"):
                        return PatchResultModel(
                            success=False,
                            file_path=file_path,
                            target_name=target_name,
                            target_type=target_type,
                            warnings=warnings,
                            error="Tests failed after refactor - changes reverted automatically",
                        )
                elif tests_passed is True:
                    warnings.append("All tests passed ")
            except Exception as e:
                warnings.append(f"Git integration warning: {str(e)}")

        # Enterprise: post-save verification + rollback on failure.
        if ("rollback_support" in capabilities) or (validation_level == "full"):
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    updated_source = f.read()

                # Syntax check on the full file
                ast.parse(updated_source)

                # Verify the symbol is still extractable (guards against boundary mistakes)
                from code_scalpel import SurgicalExtractor

                extractor = SurgicalExtractor(updated_source)
                if target_type == "function":
                    check = extractor.get_function(target_name)
                elif target_type == "class":
                    check = extractor.get_class(target_name)
                else:
                    class_name, method_name = target_name.rsplit(".", 1)
                    check = extractor.get_method(class_name, method_name)

                if not getattr(check, "success", False):
                    raise ValueError(check.error or "Post-save verification failed")
            except Exception as e:
                if backup_path:
                    import shutil

                    shutil.copy2(backup_path, file_path)
                return PatchResultModel(
                    success=False,
                    file_path=file_path,
                    target_name=target_name,
                    target_type=target_type,
                    error_code="UPDATE_SYMBOL_POST_SAVE_VERIFICATION_FAILED",
                    warnings=warnings,
                    error=f"Post-save verification failed: {e}",
                )

        # [20250101_FEATURE] v1.0 roadmap: Increment session counter on success
        new_count = _increment_session_update_count("update_symbol")

        # [20250101_FEATURE] v1.0 roadmap: Enterprise audit trail
        if "audit_trail" in capabilities:
            _add_audit_entry(
                tool_name="update_symbol",
                file_path=file_path,
                target_name=target_name,
                operation=operation,
                success=True,
                tier=tier,
                metadata={
                    "lines_before": result.lines_before,
                    "lines_after": result.lines_after,
                    "backup_path": backup_path,
                },
            )

        return PatchResultModel(
            success=True,
            file_path=file_path,
            target_name=target_name,
            target_type=target_type,
            lines_before=result.lines_before,
            lines_after=result.lines_after,
            lines_delta=result.lines_delta,
            backup_path=backup_path,
            max_updates_per_session=(int(max_updates) if max_updates > 0 else None),
            updates_used=(int(new_count) if max_updates > 0 else None),
            updates_remaining=(
                int(max_updates - new_count) if max_updates > 0 else None
            ),
            warnings=warnings,
        )

    except Exception as e:
        # [20250101_FEATURE] v1.0 roadmap: Enterprise audit trail for failures too
        if "audit_trail" in capabilities:
            _add_audit_entry(
                tool_name="update_symbol",
                file_path=file_path or "",
                target_name=target_name,
                operation=operation,
                success=False,
                tier=tier,
                metadata={"error": str(e)},
            )
        return PatchResultModel(
            success=False,
            file_path=file_path,
            target_name=target_name,
            target_type=target_type,
            warnings=warnings,
            error_code="UPDATE_SYMBOL_INTERNAL_ERROR",
            error=f"Patch failed: {str(e)}",
        )


async def extract_code_with_variable_promotion(
    target_name: str,
    file_path: str | None = None,
    code: str | None = None,
) -> dict:
    """
    Extract a function and promote local variables to parameters (Pro tier).

    [20251229_FEATURE] Pro tier: Smart Extract with variable promotion.

    This tool identifies hardcoded values in functions and promotes them to
    configurable parameters with intelligent defaults, making extracted code
    more reusable.

    Args:
        target_name: Name of the function to extract and analyze
        file_path: Path to source file (token-efficient)
        code: Source code string (fallback)

    Returns:
        Dict with promoted_function, promoted_variables, original_function, explanation

    Example:
        result = await extract_code_with_variable_promotion(
            file_path="/app/utils.py",
            target_name="calculate_tax"
        )
        # Returns function with tax_rate and threshold as parameters
    """
    # Check tier capability
    tier = _get_current_tier()
    if not has_capability("extract_code", "variable_promotion", tier):
        return {
            "success": False,
            "error": "Feature 'variable_promotion' requires PRO tier.",
        }

    # Load code
    if file_path:
        try:
            resolved_path = resolve_file_path(file_path, check_exists=True)
            with open(resolved_path, "r", encoding="utf-8") as f:
                source_code = f.read()
        except Exception as e:
            return {"success": False, "error": f"Failed to read file: {str(e)}"}
    elif code:
        source_code = code
    else:
        return {"success": False, "error": "Must provide either file_path or code"}

    # Import the promotion function
    from code_scalpel.surgery.surgical_extractor import promote_variables

    # Perform variable promotion
    result = promote_variables(source_code, target_name)

    if result.success:
        return {
            "success": True,
            "promoted_function": result.promoted_function,
            "promoted_variables": result.promoted_variables,
            "original_function": result.original_function,
            "explanation": result.explanation,
        }
    else:
        return {
            "success": False,
            "error": result.error or "Variable promotion failed",
        }


async def detect_closure_variables(
    target_name: str,
    file_path: str | None = None,
    code: str | None = None,
) -> dict:
    """
    Detect variables captured from outer scopes (Pro tier).

    [20251229_FEATURE] Pro tier: Closure variable detection.

    Identifies variables that are captured from outer scopes (closures), which can
    cause "works in isolation, breaks in production" bugs when extracting code.

    Args:
        target_name: Name of the function to analyze
        file_path: Path to source file (token-efficient)
        code: Source code string (fallback)

    Returns:
        Dict with closure_variables, has_closures, explanation

    Example:
        result = await detect_closure_variables(
            file_path="/app/utils.py",
            target_name="calculate"
        )
        # Returns detected closure variables with risk levels
    """
    # Check tier capability
    tier = _get_current_tier()
    caps = get_tool_capabilities("extract_code", tier)
    caps.get("capabilities", set())

    # Closure detection is a Pro feature (not explicitly listed but part of Pro smart extract)
    if tier == "community":
        return {
            "success": False,
            "error": "Closure variable detection requires PRO tier.",
        }

    # Load code
    if file_path:
        try:
            resolved_path = resolve_file_path(file_path, check_exists=True)
            with open(resolved_path, "r", encoding="utf-8") as f:
                source_code = f.read()
        except Exception as e:
            return {"success": False, "error": f"Failed to read file: {str(e)}"}
    elif code:
        source_code = code
    else:
        return {"success": False, "error": "Must provide either file_path or code"}

    # Import the closure detection function
    from code_scalpel.surgery.surgical_extractor import (
        detect_closure_variables as detect_closures,
    )

    # Perform closure analysis
    result = detect_closures(source_code, target_name)

    if result.success:
        return {
            "success": True,
            "function_name": result.function_name,
            "closure_variables": [
                {
                    "name": cv.name,
                    "source": cv.source,
                    "line_number": cv.line_number,
                    "risk_level": cv.risk_level,
                    "captured_from": cv.captured_from,
                    "suggestion": cv.suggestion,
                }
                for cv in result.closure_variables
            ],
            "has_closures": result.has_closures,
            "explanation": result.explanation,
        }
    else:
        return {
            "success": False,
            "error": result.error or "Closure analysis failed",
        }


async def suggest_dependency_injection(
    target_name: str,
    file_path: str | None = None,
    code: str | None = None,
) -> dict:
    """
    Suggest dependency injection refactorings for better testability (Pro tier).

    [20251229_FEATURE] Pro tier: Dependency injection suggestions.

    Analyzes function dependencies and recommends converting globals, closures,
    and hard dependencies into injected parameters for better testing and reusability.

    Args:
        target_name: Name of the function to analyze
        file_path: Path to source file (token-efficient)
        code: Source code string (fallback)

    Returns:
        Dict with suggestions, original_signature, refactored_signature, explanation

    Example:
        result = await suggest_dependency_injection(
            file_path="/app/services.py",
            target_name="get_data"
        )
        # Returns DI suggestions with benefits
    """
    # Check tier capability
    tier = _get_current_tier()

    # DI suggestions are a Pro feature
    if tier == "community":
        return {
            "success": False,
            "error": "Dependency injection suggestions require PRO tier.",
        }

    # Load code
    if file_path:
        try:
            resolved_path = resolve_file_path(file_path, check_exists=True)
            with open(resolved_path, "r", encoding="utf-8") as f:
                source_code = f.read()
        except Exception as e:
            return {"success": False, "error": f"Failed to read file: {str(e)}"}
    elif code:
        source_code = code
    else:
        return {"success": False, "error": "Must provide either file_path or code"}

    # Import the DI suggestion function
    from code_scalpel.surgery.surgical_extractor import (
        suggest_dependency_injection as suggest_di,
    )

    # Perform DI analysis
    result = suggest_di(source_code, target_name)

    if result.success:
        return {
            "success": True,
            "function_name": result.function_name,
            "suggestions": [
                {
                    "variable_name": s.variable_name,
                    "current_usage": s.current_usage,
                    "suggested_parameter": s.suggested_parameter,
                    "suggested_default": s.suggested_default,
                    "benefit": s.benefit,
                    "refactored_signature": s.refactored_signature,
                }
                for s in result.suggestions
            ],
            "has_suggestions": result.has_suggestions,
            "original_signature": result.original_signature,
            "refactored_signature": result.refactored_signature,
            "explanation": result.explanation,
        }
    else:
        return {
            "success": False,
            "error": result.error or "Dependency injection analysis failed",
        }


async def extract_as_microservice(
    target_name: str,
    file_path: str | None = None,
    code: str | None = None,
    host: str = "127.0.0.1",
    port: int = 8000,
) -> dict:
    """
    Extract a function as a containerized microservice (Enterprise tier).

    [20251229_FEATURE] Enterprise tier: Microservice Extraction with Dockerfile + API spec.

    [20260102_BUGFIX] Loopback is the default host to avoid binding all interfaces.

    Generates everything needed to deploy a function as a standalone microservice:
    - Dockerfile with dependencies
    - OpenAPI specification
    - FastAPI service wrapper
    - requirements.txt
    - Deployment README

    Args:
        target_name: Name of the function to extract
        file_path: Path to source file (token-efficient)
        code: Source code string (fallback)
        host: Service host (default: "0.0.0.0")
        port: Service port (default: 8000)

    Returns:
        Dict with function_code, dockerfile, api_spec, requirements_txt, readme

    Example:
        result = await extract_as_microservice(
            file_path="/app/services.py",
            target_name="process_data",
            port=8080
        )
        # Returns complete deployment package
    """
    # Check tier capability
    tier = _get_current_tier()
    if not has_capability("extract_code", "dockerfile_generation", tier):
        return {
            "success": False,
            "error": "Feature 'dockerfile_generation' requires ENTERPRISE tier.",
        }

    # Load code
    if file_path:
        try:
            resolved_path = resolve_file_path(file_path, check_exists=True)
            with open(resolved_path, "r", encoding="utf-8") as f:
                source_code = f.read()
        except Exception as e:
            return {"success": False, "error": f"Failed to read file: {str(e)}"}
    elif code:
        source_code = code
    else:
        return {"success": False, "error": "Must provide either file_path or code"}

    # Import the microservice extraction function
    from code_scalpel.surgery.surgical_extractor import (
        extract_as_microservice as extract_microservice,
    )

    # Perform microservice extraction
    result = extract_microservice(source_code, target_name, host, port)

    if result.success:
        return {
            "success": True,
            "function_code": result.function_code,
            "dockerfile": result.dockerfile,
            "api_spec": result.api_spec,
            "requirements_txt": result.requirements_txt,
            "readme": result.readme,
        }
    else:
        return {
            "success": False,
            "error": result.error or "Microservice extraction failed",
        }


async def resolve_organization_wide(
    code: str,
    function_name: str,
    workspace_root: str | None = None,
    ctx: Context | None = None,
) -> dict:
    """
    Resolve imports across multiple repositories in a monorepo.

    [20251229_FEATURE] Enterprise tier: Organization-wide resolution.

    Scans workspace for multiple git repositories and resolves imports across
    repository boundaries, supporting monorepo architectures like Yarn workspaces
    and Lerna configurations.

    Args:
        code: Python source code containing the function
        function_name: Name of the function to extract
        workspace_root: Root directory to scan for repositories (defaults to cwd)

    Returns:
        dict with cross_repo_imports, resolved_symbols, and monorepo_structure

    Example:
        >>> result = await resolve_organization_wide(
        ...     code=frontend_code,
        ...     function_name="PaymentForm",
        ...     workspace_root="/workspace"
        ... )
        >>> result["cross_repo_imports"][0]["repo_name"]
        'backend-service'
    """
    from code_scalpel.surgery.surgical_extractor import resolve_organization_wide

    # Enterprise tier capability check
    tier = _get_current_tier()
    get_tool_capabilities("extract_code", tier)
    if not has_capability("extract_code", "cross_file_deps_unlimited", tier):
        return {
            "success": False,
            "error": "Organization-wide resolution requires Enterprise tier",
        }

    result = resolve_organization_wide(
        code=code,
        function_name=function_name,
        workspace_root=workspace_root,
    )

    if result.success:
        return {
            "success": True,
            "target_name": result.target_name,
            "target_code": result.target_code,
            "cross_repo_imports": [
                {
                    "repo_name": imp.repo_name,
                    "file_path": imp.file_path,
                    "symbols": imp.symbols,
                    "repo_root": imp.repo_root,
                }
                for imp in result.cross_repo_imports
            ],
            "resolved_symbols": result.resolved_symbols,
            "monorepo_structure": result.monorepo_structure,
            "explanation": result.explanation,
        }
    else:
        return {
            "success": False,
            "error": result.error or "Organization-wide resolution failed",
        }


async def extract_with_custom_pattern(
    pattern: str,
    pattern_type: str = "regex",
    project_root: str | None = None,
    include_related: list[str] | None = None,
    ctx: Context | None = None,
) -> dict:
    """
    Extract code using custom patterns (regex or AST).

    [20251229_FEATURE] Enterprise tier: Custom extraction patterns.

    Allows users to define extraction rules for finding code based on patterns,
    useful for refactoring campaigns and architectural analysis.

    Args:
        pattern: Pattern to match (regex or AST query)
        pattern_type: "regex", "function_call", or "import"
        project_root: Root directory to search (defaults to cwd)
        include_related: List of related symbols to include

    Returns:
        dict with matches, total_matches, files_scanned

    Example:
        >>> # Extract all functions that call db.query()
        >>> result = await extract_with_custom_pattern(
        ...     pattern="db.query",
        ...     pattern_type="function_call"
        ... )
        >>> len(result["matches"])
        5
    """
    from code_scalpel.surgery.surgical_extractor import (
        extract_with_custom_pattern as extract_pattern,
    )

    # Enterprise tier capability check
    tier = _get_current_tier()
    get_tool_capabilities("extract_code", tier)
    if not has_capability("extract_code", "custom_patterns", tier):
        return {
            "success": False,
            "error": "Custom extraction patterns require Enterprise tier",
        }

    result = extract_pattern(
        pattern=pattern,
        pattern_type=pattern_type,
        project_root=project_root,
        include_related=include_related,
    )

    if result.success:
        return {
            "success": True,
            "pattern_name": result.pattern_name,
            "matches": [
                {
                    "symbol_name": match.symbol_name,
                    "symbol_type": match.symbol_type,
                    "file_path": match.file_path,
                    "line_number": match.line_number,
                    "code": match.code,
                    "match_reason": match.match_reason,
                }
                for match in result.matches
            ],
            "total_matches": result.total_matches,
            "files_scanned": result.files_scanned,
            "explanation": result.explanation,
        }
    else:
        return {
            "success": False,
            "error": result.error or "Custom pattern extraction failed",
        }


async def detect_service_boundaries(
    project_root: str | None = None,
    min_isolation_score: float = 0.6,
    ctx: Context | None = None,
) -> dict:
    """
    Detect architectural boundaries and suggest microservice splits.

    [20251229_FEATURE] Enterprise tier: Service boundary detection.

    Analyzes dependency graph to identify loosely coupled clusters that could
    be extracted as independent microservices.

    Args:
        project_root: Root directory to analyze (defaults to cwd)
        min_isolation_score: Minimum isolation score (0.0-1.0) for service suggestions

    Returns:
        dict with suggested_services, dependency_graph, total_files_analyzed

    Example:
        >>> result = await detect_service_boundaries(project_root="/app")
        >>> for service in result["suggested_services"]:
        ...     print(f"{service['service_name']}: {len(service['included_files'])} files")
        payment-service: 5 files
        stripe-wrapper: 2 files
    """
    from code_scalpel.surgery.surgical_extractor import (
        detect_service_boundaries as detect_boundaries,
    )

    # Enterprise tier capability check
    tier = _get_current_tier()
    get_tool_capabilities("extract_code", tier)
    if not has_capability("extract_code", "service_boundaries", tier):
        return {
            "success": False,
            "error": "Service boundary detection requires Enterprise tier",
        }

    result = detect_boundaries(
        project_root=project_root,
        min_isolation_score=min_isolation_score,
    )

    if result.success:
        return {
            "success": True,
            "suggested_services": [
                {
                    "service_name": service.service_name,
                    "included_files": service.included_files,
                    "external_dependencies": service.external_dependencies,
                    "internal_dependencies": service.internal_dependencies,
                    "isolation_level": service.isolation_level,
                    "rationale": service.rationale,
                }
                for service in result.suggested_services
            ],
            "dependency_graph": result.dependency_graph,
            "total_files_analyzed": result.total_files_analyzed,
            "explanation": result.explanation,
        }
    else:
        return {
            "success": False,
            "error": result.error or "Service boundary detection failed",
        }


@mcp.tool()
async def crawl_project(
    root_path: str | None = None,
    exclude_dirs: list[str] | None = None,
    complexity_threshold: int = 10,
    include_report: bool = True,
    pattern: str | None = None,
    pattern_type: str = "regex",
    include_related: list[str] | None = None,
    ctx: Context | None = None,
) -> ProjectCrawlResult:
    """
    Crawl an entire project directory and analyze all Python files.

    **Tier Behavior:**
    - Community: Discovery crawl (file inventory, structure, entrypoints)
    - Pro/Enterprise: Deep crawl (full analysis with complexity, dependencies, cross-file)

    Use this tool to get a comprehensive overview of a project's structure,
    complexity hotspots, and code metrics before diving into specific files.

    [20251215_FEATURE] v2.0.0 - Progress reporting for long-running operations.
    Reports progress as files are discovered and analyzed.

    [20251223_FEATURE] v3.2.8 - Tier-based behavior splitting.
    Community tier provides discovery-only crawl for inventory and entrypoints.

    Example::

        result = await crawl_project(
            root_path="/home/user/myproject",
            complexity_threshold=8,
            include_report=True
        )

        # Returns ProjectCrawlResult:
        # - summary: ProjectSummary(
        #     total_files=42,
        #     total_lines=5680,
        #     total_functions=187,
        #     total_classes=23,
        #     average_complexity=4.2
        # )
        # - files: [CrawlFileResult(path="src/main.py", ...), ...]
        # - complexity_hotspots: [
        #     CrawlFunctionInfo(name="parse_config", complexity=15, lineno=42),
        #     CrawlFunctionInfo(name="process_batch", complexity=12, lineno=156)
        # ]
        # - markdown_report: "# Project Analysis Report\n\n## Summary\n..."

        # Find files exceeding complexity threshold
        for hotspot in result.complexity_hotspots:
            print(f"{hotspot.name}: complexity {hotspot.complexity}")

    Args:
        root_path: Path to project root (defaults to current working directory)
        exclude_dirs: Additional directories to exclude (common ones already excluded)
        complexity_threshold: Complexity score that triggers a warning (default: 10)
        include_report: Include a markdown report in the response (default: True)

    Returns:
        ProjectCrawlResult with files, summary stats, complexity_hotspots, and markdown_report
    """
    if root_path is None:
        root_path = str(PROJECT_ROOT)

    # [20251215_FEATURE] v2.0.0 - Progress token support
    # Report initial progress
    if ctx:
        await ctx.report_progress(progress=0, total=100)

    # [20251225_FEATURE] Tier-based behavior via capability matrix (no upgrade hints).
    tier = _get_current_tier()
    caps = get_tool_capabilities("crawl_project", tier)
    capabilities = set(caps.get("capabilities", set()))
    limits = caps.get("limits", {})
    max_files = limits.get("max_files")
    max_depth = limits.get("max_depth")
    respect_gitignore = "gitignore_respect" in capabilities

    # [20260106_FEATURE] v1.0 pre-release - Determine crawl mode for output metadata
    crawl_mode = "discovery" if tier == "community" else "deep"

    if tier == "community":
        # Community: Discovery crawl (inventory + entrypoints)
        result = await asyncio.to_thread(
            _crawl_project_discovery,
            root_path,
            exclude_dirs,
            max_files=max_files,
            max_depth=max_depth,
            respect_gitignore=respect_gitignore,
        )
    else:
        # Pro/Enterprise: Deep crawl with optional feature sections
        result = await asyncio.to_thread(
            _crawl_project_sync,
            root_path,
            exclude_dirs,
            complexity_threshold,
            include_report,
            capabilities,
            max_files,
            max_depth,
            respect_gitignore,
        )

    # [20260106_FEATURE] v1.0 pre-release - Add output transparency metadata
    try:
        result = result.model_copy(
            update={
                "tier_applied": tier,
                "crawl_mode": crawl_mode,
                "files_limit_applied": max_files,
            }
        )
    except Exception:
        # Fallback for older Pydantic or if model_copy fails
        result.tier_applied = tier
        result.crawl_mode = crawl_mode
        result.files_limit_applied = max_files

    # Enterprise feature: project-wide custom pattern extraction (not a standalone MCP tool).
    if pattern:
        if not has_capability("extract_code", "custom_extraction_patterns", tier):
            try:
                result = result.model_copy(
                    update={
                        "pattern_success": False,
                        "pattern_error": "Custom pattern extraction requires ENTERPRISE tier",
                    }
                )
            except Exception:
                pass
        else:
            try:
                from code_scalpel.surgery.surgical_extractor import (
                    extract_with_custom_pattern as _extract_pattern,
                )

                pattern_result = await asyncio.to_thread(
                    _extract_pattern,
                    pattern=pattern,
                    pattern_type=pattern_type,
                    project_root=root_path,
                    include_related=include_related,
                )

                if getattr(pattern_result, "success", False):
                    payload: dict[str, Any] = {
                        "pattern_success": True,
                        "pattern_name": getattr(pattern_result, "pattern_name", None),
                        "pattern_total_matches": getattr(
                            pattern_result, "total_matches", 0
                        ),
                        "pattern_files_scanned": getattr(
                            pattern_result, "files_scanned", 0
                        ),
                        "pattern_matches": [
                            {
                                "symbol_name": m.symbol_name,
                                "symbol_type": m.symbol_type,
                                "file_path": m.file_path,
                                "line_number": m.line_number,
                                "match_reason": m.match_reason,
                            }
                            for m in getattr(pattern_result, "matches", [])
                        ],
                    }
                else:
                    payload = {
                        "pattern_success": False,
                        "pattern_error": getattr(pattern_result, "error", None)
                        or "Custom pattern extraction failed",
                    }

                result = result.model_copy(update=payload)
            except Exception as e:
                try:
                    result = result.model_copy(
                        update={
                            "pattern_success": False,
                            "pattern_error": f"Custom pattern extraction failed: {e}",
                        }
                    )
                except Exception:
                    pass

    # Report completion
    if ctx:
        await ctx.report_progress(progress=100, total=100)

    return result


# ============================================================================
# v1.4.0 MCP TOOLS - Enhanced AI Context
# ============================================================================


def _get_file_context_sync(
    file_path: str, tier: str | None = None, capabilities: dict | None = None
) -> FileContextResult:
    """
    Synchronous implementation of get_file_context.

    [20251214_FEATURE] v1.5.3 - Integrated PathResolver for intelligent path resolution
    [20251220_FEATURE] v3.0.5 - Multi-language support via file extension detection
    [20251225_FEATURE] v3.3.0 - Tier-gated limits, enrichments, and redaction
    """
    from code_scalpel.mcp.path_resolver import resolve_path

    # Language detection by file extension
    LANG_EXTENSIONS = {
        ".py": "python",
        ".js": "javascript",
        ".jsx": "javascript",
        ".ts": "typescript",
        ".tsx": "typescript",
        ".java": "java",
    }

    # Tier + capability detection
    tier = tier or _get_current_tier()

    # [20260104_BUGFIX] Accept capabilities as list overrides for test call sites.
    if isinstance(capabilities, list):
        caps_override: dict[str, Any] | None = {"capabilities": capabilities}
    elif isinstance(capabilities, dict):
        caps_override = capabilities
    else:
        caps_override = None

    caps = caps_override or get_tool_capabilities("get_file_context", tier) or {}
    caps_capabilities = []
    if isinstance(caps, dict):
        caps_capabilities = caps.get("capabilities", []) or []
    cap_set: set[str] = set(caps_capabilities)
    limits = caps.get("limits", {}) if isinstance(caps, dict) else {}
    limits = limits or {}
    max_context_lines = limits.get("max_context_lines", limits.get("context_lines"))

    # [20260111_FEATURE] v1.0 - Calculate output metadata flags for transparency
    PRO_CAPABILITIES = {
        "code_smell_detection",
        "documentation_coverage",
        "maintainability_index",
        "semantic_summarization",
        "intent_extraction",
        "related_imports_inclusion",
        "smart_context_expansion",
    }
    ENTERPRISE_CAPABILITIES = {
        "pii_redaction",
        "secret_masking",
        "api_key_detection",
        "rbac_aware_retrieval",
        "file_access_control",
        "custom_metadata_extraction",
        "compliance_flags",
        "technical_debt_scoring",
        "owner_team_mapping",
        "historical_metrics",
    }
    pro_features_enabled = bool(cap_set & PRO_CAPABILITIES)
    enterprise_features_enabled = bool(cap_set & ENTERPRISE_CAPABILITIES)

    try:
        # [20251214_FEATURE] Use PathResolver for intelligent path resolution
        try:
            resolved_path = resolve_path(file_path, str(PROJECT_ROOT))
            path = Path(resolved_path)
        except FileNotFoundError as e:
            # PathResolver provides helpful error messages
            return FileContextResult(
                success=False,
                file_path=file_path,
                line_count=0,
                tier_applied=tier,
                max_context_lines_applied=max_context_lines,
                pro_features_enabled=pro_features_enabled,
                enterprise_features_enabled=enterprise_features_enabled,
                error=str(e),
            )

        code = path.read_text(encoding="utf-8")
        lines = code.splitlines()
        line_count = len(lines)

        # [20251225_BUGFIX] Enforce tier line limits before heavy work
        if max_context_lines is not None and line_count > int(max_context_lines):
            return FileContextResult(
                success=False,
                file_path=str(path),
                line_count=line_count,
                tier_applied=tier,
                max_context_lines_applied=max_context_lines,
                pro_features_enabled=pro_features_enabled,
                enterprise_features_enabled=enterprise_features_enabled,
                summary="",
                imports_truncated=False,
                total_imports=0,
                error=(
                    f"File has {line_count} lines which exceeds the {max_context_lines} line limit for {tier.title()} tier."
                ),
            )

        # [20251220_FEATURE] Detect language from file extension
        detected_lang = LANG_EXTENSIONS.get(path.suffix.lower(), "unknown")

        # For non-Python files, use analyze_code which handles multi-language
        if detected_lang != "python":
            analysis = _analyze_code_sync(code, detected_lang)
            total_imports = len(analysis.imports)

            semantic_summary = None
            if "semantic_summarization" in cap_set:
                function_count = len(analysis.functions)
                class_count = len(analysis.classes)
                semantic_summary = f"{detected_lang.title()} module with {function_count} function(s) and {class_count} class(es)."

            expanded_context = None
            if "smart_context_expansion" in cap_set:
                preview_len = min(line_count, max_context_lines or 50, 50)
                expanded_context = "\n".join(lines[:preview_len])

            return FileContextResult(
                success=analysis.success if analysis.success is not None else True,
                file_path=str(path),
                tier_applied=tier,
                max_context_lines_applied=max_context_lines,
                pro_features_enabled=pro_features_enabled,
                enterprise_features_enabled=enterprise_features_enabled,
                language=detected_lang,
                line_count=line_count,
                functions=cast(list[FunctionInfo | str], analysis.functions),
                classes=cast(list[ClassInfo | str], analysis.classes),
                imports=analysis.imports[:20],
                exports=[],
                complexity_score=analysis.complexity,
                has_security_issues=False,
                summary=f"{detected_lang.title()} module with {len(analysis.functions)} function(s), {len(analysis.classes)} class(es)",
                imports_truncated=total_imports > 20,
                total_imports=total_imports,
                semantic_summary=semantic_summary,
                expanded_context=expanded_context,
                access_controlled="rbac_aware_retrieval" in cap_set
                or "file_access_control" in cap_set,
                error=analysis.error,
            )

        # Python-specific parsing
        try:
            tree = ast.parse(code)
        except SyntaxError as e:
            return FileContextResult(
                success=False,
                file_path=str(path),
                line_count=line_count,
                tier_applied=tier,
                max_context_lines_applied=max_context_lines,
                pro_features_enabled=pro_features_enabled,
                enterprise_features_enabled=enterprise_features_enabled,
                error=f"Syntax error at line {e.lineno}: {e.msg}.",
            )

        # [20260104_BUGFIX] Return structured symbol info for tiered file context tests.
        functions: list[FunctionInfo] = []
        classes: list[ClassInfo] = []
        function_names: list[str] = []
        class_names: list[str] = []
        imports: list[str] = []
        exports: list[str] = []
        complexity = 0

        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef | ast.AsyncFunctionDef):
                # Only top-level functions
                if hasattr(node, "col_offset") and node.col_offset == 0:
                    functions.append(
                        FunctionInfo(
                            name=node.name,
                            lineno=getattr(node, "lineno", 0) or 0,
                            end_lineno=getattr(node, "end_lineno", None),
                            is_async=isinstance(node, ast.AsyncFunctionDef),
                        )
                    )
                    function_names.append(node.name)
                    complexity += _count_complexity_node(node)
            elif isinstance(node, ast.ClassDef):
                if hasattr(node, "col_offset") and node.col_offset == 0:
                    methods = [
                        n.name
                        for n in node.body
                        if isinstance(n, ast.FunctionDef | ast.AsyncFunctionDef)
                    ]
                    classes.append(
                        ClassInfo(
                            name=node.name,
                            lineno=getattr(node, "lineno", 0) or 0,
                            end_lineno=getattr(node, "end_lineno", None),
                            methods=methods,
                        )
                    )
                    class_names.append(node.name)
            elif isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append(alias.name)
            elif isinstance(node, ast.ImportFrom):
                module = node.module or ""
                for alias in node.names:
                    imports.append(f"{module}.{alias.name}")
            elif isinstance(node, ast.Assign):
                # Check for __all__ exports
                for target in node.targets:
                    if isinstance(target, ast.Name) and target.id == "__all__":
                        if isinstance(node.value, ast.List | ast.Tuple):
                            for elt in node.value.elts:
                                if isinstance(elt, ast.Constant) and isinstance(
                                    elt.value, str
                                ):
                                    exports.append(elt.value)

        # Quick security check
        has_security_issues = False
        # [20260104_BUGFIX] Treat bare except handlers as security findings for Community tier.
        bare_except = any(
            isinstance(node, ast.ExceptHandler) and node.type is None
            for node in ast.walk(tree)
        )
        security_patterns = [
            "eval(",
            "exec(",
            "cursor.execute",
            "os.system(",
            "subprocess.call(",
        ]
        for pattern in security_patterns:
            if pattern in code:
                has_security_issues = True
                break
        if bare_except:
            has_security_issues = True

        # Generate summary based on content
        summary_parts = []
        if classes:
            summary_parts.append(f"{len(classes)} class(es)")
        if functions:
            summary_parts.append(f"{len(functions)} function(s)")
        if "flask" in code.lower() or "app.route" in code:
            summary_parts.append("Flask web application")
        elif "django" in code.lower():
            summary_parts.append("Django module")
        elif "test_" in path.name or "pytest" in code:
            summary_parts.append("Test module")

        summary = ", ".join(summary_parts) if summary_parts else "Python module"

        # [20251220_FEATURE] v3.0.5 - Communicate truncation
        total_imports = len(imports)
        imports_truncated = total_imports > 20

        semantic_summary = None
        if "semantic_summarization" in cap_set:
            doc = ast.get_docstring(tree) or ""
            semantic_summary = summary
            if doc:
                semantic_summary = f"{summary}. Docstring: {doc.strip()}"

        intent_tags: list[str] = []
        if "intent_extraction" in cap_set:
            tag_source = " ".join(
                [path.stem]
                + function_names
                + class_names
                + [ast.get_docstring(tree) or ""]
            )
            for token in re.findall(r"[A-Za-z]{3,}", tag_source):
                lowered = token.lower()
                if lowered not in intent_tags:
                    intent_tags.append(lowered)
            if "flask" in code.lower():
                intent_tags.append("flask")
            if "django" in code.lower():
                intent_tags.append("django")

        related_imports: list[str] = []
        if "related_imports_inclusion" in cap_set:
            for imp in imports:
                candidate = path.parent / (imp.replace(".", "/") + ".py")
                if candidate.exists():
                    related_imports.append(str(candidate))

        expanded_context = None
        if "smart_context_expansion" in cap_set:
            preview_len = min(line_count, max_context_lines or 2000, 2000)
            expanded_context = "\n".join(lines[:preview_len])
            if not expanded_context:
                expanded_context = None

        pii_redacted = False
        secrets_masked = False
        redaction_summary: list[str] = []
        if {"pii_redaction", "secret_masking", "api_key_detection"} & cap_set:
            pii_patterns = {
                "email": re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
            }
            secret_patterns = {
                "aws_access_key": re.compile(r"AKIA[0-9A-Z]{16}"),
                "password_assignment": re.compile(r"(?i)password\s*=\s*['\"]"),
            }

            if "pii_redaction" in cap_set:
                for label, pattern in pii_patterns.items():
                    if pattern.search(code):
                        pii_redacted = True
                        redaction_summary.append(f"Redacted {label}")

            if "secret_masking" in cap_set or "api_key_detection" in cap_set:
                for label, pattern in secret_patterns.items():
                    if pattern.search(code):
                        secrets_masked = True
                        redaction_summary.append(f"Masked {label}")

        access_controlled = (
            "rbac_aware_retrieval" in cap_set or "file_access_control" in cap_set
        )

        # [20251231_FEATURE] v3.3.1 - Pro tier: Code quality metrics
        code_smells: list[dict[str, Any]] = []
        doc_coverage: float | None = None
        maintainability_index: float | None = None

        if "code_smell_detection" in cap_set:
            code_smells = _detect_code_smells(tree, code, lines)

        if "documentation_coverage" in cap_set:
            doc_coverage = _calculate_doc_coverage(tree, function_names, class_names)

        if {"maintainability_index", "maintainability_metrics"} & cap_set:
            maintainability_index = _calculate_maintainability_index(
                line_count, complexity, len(functions) + len(classes)
            )

        # [20251231_FEATURE] v3.3.1 - Enterprise tier: Organizational metadata
        custom_metadata: dict[str, Any] = {}
        compliance_flags: list[str] = []
        technical_debt_score: float | None = None
        owners: list[str] = []
        historical_metrics: dict[str, Any] | None = None

        if "custom_metadata_extraction" in cap_set:
            custom_metadata = _load_custom_metadata(path)

        if "compliance_flags" in cap_set:
            compliance_flags = _detect_compliance_flags(code, path)

        if "technical_debt_scoring" in cap_set:
            technical_debt_score = _calculate_technical_debt(
                code_smells, complexity, doc_coverage, line_count
            )

        if "owner_team_mapping" in cap_set:
            owners = _get_code_owners(path)

        if "historical_metrics" in cap_set:
            historical_metrics = _get_historical_metrics(path)

        return FileContextResult(
            success=True,
            file_path=str(path),
            tier_applied=tier,
            max_context_lines_applied=max_context_lines,
            pro_features_enabled=pro_features_enabled,
            enterprise_features_enabled=enterprise_features_enabled,
            language="python",
            line_count=line_count,
            functions=cast(list[FunctionInfo | str], functions),
            classes=cast(list[ClassInfo | str], classes),
            imports=imports[:20],
            exports=exports,
            complexity_score=complexity,
            has_security_issues=has_security_issues,
            summary=summary,
            imports_truncated=imports_truncated,
            total_imports=total_imports,
            semantic_summary=semantic_summary,
            intent_tags=intent_tags,
            related_imports=related_imports,
            expanded_context=expanded_context,
            pii_redacted=pii_redacted,
            secrets_masked=secrets_masked,
            redaction_summary=redaction_summary,
            access_controlled=access_controlled,
            code_smells=code_smells,
            doc_coverage=doc_coverage,
            maintainability_index=maintainability_index,
            custom_metadata=custom_metadata,
            compliance_flags=compliance_flags,
            technical_debt_score=technical_debt_score,
            owners=owners,
            historical_metrics=historical_metrics,
        )

    except Exception as e:
        return FileContextResult(
            success=False,
            file_path=str(file_path),
            line_count=0,
            tier_applied=tier if "tier" in dir() else "community",
            error=f"Analysis failed: {str(e)}",
        )


def _count_complexity_node(node: ast.AST) -> int:
    """Count cyclomatic complexity for a single node."""
    complexity = 1  # Base complexity
    for child in ast.walk(node):
        if isinstance(child, ast.If | ast.While | ast.For | ast.ExceptHandler):
            complexity += 1
        elif isinstance(child, ast.BoolOp):
            complexity += len(child.values) - 1
    return complexity


# [20251231_FEATURE] v3.3.1 - Pro tier helper functions for code quality metrics


def _detect_code_smells(
    tree: ast.Module, code: str, lines: list[str]
) -> list[dict[str, Any]]:
    """
    Detect common code smells in Python code.

    Returns list of: [{type, line, message, severity}]
    Severity: low, medium, high
    """
    smells: list[dict[str, Any]] = []

    for node in ast.walk(tree):
        # Long function (>50 lines)
        if isinstance(node, ast.FunctionDef | ast.AsyncFunctionDef):
            if node.end_lineno and node.lineno:
                func_lines = node.end_lineno - node.lineno
                if func_lines > 50:
                    smells.append(
                        {
                            "type": "long_function",
                            "line": node.lineno,
                            "message": f"Function '{node.name}' has {func_lines} lines (>50)",
                            "severity": "medium" if func_lines < 100 else "high",
                        }
                    )

            # Too many parameters (>5)
            param_count = len(node.args.args) + len(node.args.kwonlyargs)
            if param_count > 5:
                smells.append(
                    {
                        "type": "too_many_parameters",
                        "line": node.lineno,
                        "message": f"Function '{node.name}' has {param_count} parameters (>5)",
                        "severity": "low" if param_count <= 7 else "medium",
                    }
                )

        # Deeply nested code (>4 levels)
        if isinstance(node, ast.If | ast.For | ast.While | ast.With | ast.Try):
            depth = _get_nesting_depth(node)
            if depth > 4:
                smells.append(
                    {
                        "type": "deep_nesting",
                        "line": node.lineno,
                        "message": f"Code block at line {node.lineno} has nesting depth {depth} (>4)",
                        "severity": "medium" if depth <= 6 else "high",
                    }
                )

        # God class (>20 methods)
        if isinstance(node, ast.ClassDef):
            method_count = sum(
                1
                for n in node.body
                if isinstance(n, ast.FunctionDef | ast.AsyncFunctionDef)
            )
            if method_count > 20:
                smells.append(
                    {
                        "type": "god_class",
                        "line": node.lineno,
                        "message": f"Class '{node.name}' has {method_count} methods (>20)",
                        "severity": "high",
                    }
                )

    # Check for magic numbers (numeric literals not 0, 1, -1)
    magic_numbers_found: set[tuple[int, float | int]] = set()
    for node in ast.walk(tree):
        if isinstance(node, ast.Constant) and isinstance(node.value, (int, float)):
            if node.value not in (0, 1, -1, 0.0, 1.0, -1.0, 2, 10, 100):
                if hasattr(node, "lineno"):
                    key = (node.lineno, node.value)
                    if key not in magic_numbers_found:
                        magic_numbers_found.add(key)

    if len(magic_numbers_found) > 5:
        smells.append(
            {
                "type": "magic_numbers",
                "line": 0,
                "message": f"File contains {len(magic_numbers_found)} magic numbers",
                "severity": "low",
            }
        )

    # Check for bare except
    for node in ast.walk(tree):
        if isinstance(node, ast.ExceptHandler) and node.type is None:
            smells.append(
                {
                    "type": "bare_except",
                    "line": node.lineno,
                    "message": "Bare 'except:' clause catches all exceptions",
                    "severity": "medium",
                }
            )

    return smells


def _get_nesting_depth(node: ast.AST, current_depth: int = 1) -> int:
    """Calculate maximum nesting depth of a code block."""
    max_depth = current_depth
    for child in ast.iter_child_nodes(node):
        if isinstance(child, ast.If | ast.For | ast.While | ast.With | ast.Try):
            child_depth = _get_nesting_depth(child, current_depth + 1)
            max_depth = max(max_depth, child_depth)
    return max_depth


def _calculate_doc_coverage(
    tree: ast.Module, functions: list[str], classes: list[str]
) -> float:
    """
    Calculate documentation coverage percentage.

    Returns: 0.0 to 100.0 (percentage of functions/classes with docstrings)
    """
    total_items = 0
    documented_items = 0

    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef | ast.AsyncFunctionDef):
            if hasattr(node, "col_offset") and node.col_offset == 0:
                total_items += 1
                if ast.get_docstring(node):
                    documented_items += 1
        elif isinstance(node, ast.ClassDef):
            if hasattr(node, "col_offset") and node.col_offset == 0:
                total_items += 1
                if ast.get_docstring(node):
                    documented_items += 1

    # Include module docstring
    if ast.get_docstring(tree):
        documented_items += 1
    total_items += 1  # Count module itself

    if total_items == 0:
        return 100.0

    return round((documented_items / total_items) * 100, 1)


def _calculate_maintainability_index(
    line_count: int, complexity: int, symbol_count: int
) -> float:
    """
    Calculate Maintainability Index (0-100 scale, higher is better).

    Based on simplified Halstead/McCabe formula:
    MI = max(0, (171 - 5.2 * ln(V) - 0.23 * CC - 16.2 * ln(LOC)) * 100 / 171)

    Simplified version for quick calculation:
    MI = 100 - (complexity * 2) - (line_count / 50) - (symbol_count / 10)

    Clamped to 0-100 range.
    """
    import math

    # Simplified maintainability calculation
    if line_count == 0:
        return 100.0

    # Volume approximation (Halstead)
    volume = max(1, line_count * math.log2(max(1, symbol_count + 1)))

    # Simplified MI formula
    mi = (
        171
        - 5.2 * math.log(max(1, volume))
        - 0.23 * complexity
        - 16.2 * math.log(max(1, line_count))
    )
    mi = (mi * 100) / 171

    return round(max(0.0, min(100.0, mi)), 1)


# [20251231_FEATURE] v3.3.1 - Enterprise tier helper functions


def _load_custom_metadata(file_path: Path) -> dict[str, Any]:
    """
    Load custom metadata from .code-scalpel/metadata.yaml if exists.

    Returns empty dict if not found or invalid.
    """
    try:
        # Search for .code-scalpel directory
        search_path = file_path.parent
        for _ in range(10):  # Max 10 levels up
            metadata_file = search_path / ".code-scalpel" / "metadata.yaml"
            if metadata_file.exists():
                import yaml

                with open(metadata_file, encoding="utf-8") as f:
                    data = yaml.safe_load(f) or {}

                # Check for file-specific metadata
                rel_path = str(file_path.relative_to(search_path))
                file_metadata = data.get("files", {}).get(rel_path, {})
                global_metadata = {k: v for k, v in data.items() if k != "files"}
                return {**global_metadata, **file_metadata}

            if search_path.parent == search_path:
                break
            search_path = search_path.parent
    except Exception:
        pass
    return {}


def _detect_compliance_flags(code: str, file_path: Path) -> list[str]:
    """
    Detect compliance-relevant patterns in code.

    Returns list of compliance flags: HIPAA, PCI, SOC2, GDPR, etc.
    """
    flags: list[str] = []
    code_lower = code.lower()

    # HIPAA - Health data patterns
    hipaa_patterns = [
        "patient",
        "health",
        "medical",
        "diagnosis",
        "phi",
        "protected_health",
    ]
    if any(p in code_lower for p in hipaa_patterns):
        flags.append("HIPAA")

    # PCI-DSS - Payment card patterns
    pci_patterns = ["credit_card", "card_number", "cvv", "pan", "cardholder", "payment"]
    if any(p in code_lower for p in pci_patterns):
        flags.append("PCI-DSS")

    # SOC2 - Security/audit patterns
    soc2_patterns = ["audit_log", "access_control", "encryption", "authenticate"]
    if any(p in code_lower for p in soc2_patterns):
        flags.append("SOC2")

    # GDPR - Personal data patterns
    gdpr_patterns = [
        "personal_data",
        "gdpr",
        "consent",
        "data_subject",
        "right_to_erasure",
    ]
    if any(p in code_lower for p in gdpr_patterns):
        flags.append("GDPR")

    return flags


def _calculate_technical_debt(
    code_smells: list[dict[str, Any]],
    complexity: int,
    doc_coverage: float | None,
    line_count: int,
) -> float:
    """
    Calculate technical debt score in estimated hours to fix.

    Based on:
    - Code smells (weighted by severity)
    - Missing documentation
    - High complexity
    """
    debt_hours = 0.0

    # Code smell debt
    severity_weights = {"low": 0.25, "medium": 0.5, "high": 1.0}
    for smell in code_smells:
        debt_hours += severity_weights.get(smell.get("severity", "low"), 0.25)

    # Documentation debt (rough estimate: 0.1 hours per undocumented item per 100 lines)
    if doc_coverage is not None and doc_coverage < 80:
        missing_doc_pct = (80 - doc_coverage) / 100
        debt_hours += (line_count / 100) * missing_doc_pct * 0.5

    # Complexity debt
    if complexity > 20:
        debt_hours += (complexity - 20) * 0.1

    return round(debt_hours, 1)


def _get_code_owners(file_path: Path) -> list[str]:
    """
    Parse CODEOWNERS file to find owners for the given file.

    Returns list of owner identifiers (GitHub usernames, team names, or emails).
    """
    try:
        # Search for CODEOWNERS file
        search_path = file_path.parent
        codeowners_locations = [
            ".github/CODEOWNERS",
            "CODEOWNERS",
            "docs/CODEOWNERS",
        ]

        for _ in range(10):  # Max 10 levels up
            for loc in codeowners_locations:
                codeowners_file = search_path / loc
                if codeowners_file.exists():
                    return _parse_codeowners(codeowners_file, file_path, search_path)

            if search_path.parent == search_path:
                break
            search_path = search_path.parent
    except Exception:
        pass
    return []


def _parse_codeowners(
    codeowners_file: Path, target_file: Path, repo_root: Path
) -> list[str]:
    """Parse CODEOWNERS and return owners matching the target file."""
    try:
        rel_path = str(target_file.relative_to(repo_root))
        owners: list[str] = []

        with open(codeowners_file, encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("#"):
                    continue

                parts = line.split()
                if len(parts) < 2:
                    continue

                pattern = parts[0]
                line_owners = parts[1:]

                # Simple pattern matching (supports * and **)
                if _codeowners_pattern_matches(pattern, rel_path):
                    owners = line_owners  # Last matching pattern wins

        return owners
    except Exception:
        return []


def _codeowners_pattern_matches(pattern: str, file_path: str) -> bool:
    """Check if a CODEOWNERS pattern matches a file path."""
    import fnmatch

    # Normalize paths
    pattern = pattern.lstrip("/")
    file_path = file_path.replace("\\", "/")

    # Handle directory patterns
    if pattern.endswith("/"):
        return file_path.startswith(pattern) or fnmatch.fnmatch(
            file_path, pattern + "*"
        )

    # Handle glob patterns
    if "*" in pattern:
        # Convert ** to match any path depth
        glob_pattern = pattern.replace("**", "*")
        return fnmatch.fnmatch(file_path, glob_pattern)

    # Exact match or prefix match for directories
    return file_path == pattern or file_path.startswith(pattern + "/")


def _get_historical_metrics(file_path: Path) -> dict[str, Any] | None:
    """
    Get historical metrics from git for the file.

    Returns: {churn, age_days, contributors, last_modified}
    or None if git is not available.
    """
    import subprocess

    try:
        # Find git root
        result = subprocess.run(
            ["git", "rev-parse", "--show-toplevel"],
            cwd=file_path.parent,
            capture_output=True,
            text=True,
            timeout=5,
        )
        if result.returncode != 0:
            return None

        git_root = Path(result.stdout.strip())
        rel_path = str(file_path.relative_to(git_root))

        # Get commit count (churn indicator)
        result = subprocess.run(
            ["git", "log", "--oneline", "--follow", "--", rel_path],
            cwd=git_root,
            capture_output=True,
            text=True,
            timeout=10,
        )
        commit_count = (
            len(result.stdout.strip().splitlines()) if result.returncode == 0 else 0
        )

        # Get unique contributors
        result = subprocess.run(
            ["git", "log", "--format=%ae", "--follow", "--", rel_path],
            cwd=git_root,
            capture_output=True,
            text=True,
            timeout=10,
        )
        contributors = (
            list(set(result.stdout.strip().splitlines()))
            if result.returncode == 0
            else []
        )

        # Get file age (first commit date)
        result = subprocess.run(
            [
                "git",
                "log",
                "--follow",
                "--format=%at",
                "--diff-filter=A",
                "--",
                rel_path,
            ],
            cwd=git_root,
            capture_output=True,
            text=True,
            timeout=5,
        )
        first_commit_ts = None
        if result.returncode == 0 and result.stdout.strip():
            try:
                first_commit_ts = int(result.stdout.strip().splitlines()[-1])
            except (ValueError, IndexError):
                pass

        # Get last modified date
        result = subprocess.run(
            ["git", "log", "-1", "--format=%at", "--", rel_path],
            cwd=git_root,
            capture_output=True,
            text=True,
            timeout=5,
        )
        last_modified_ts = None
        if result.returncode == 0 and result.stdout.strip():
            try:
                last_modified_ts = int(result.stdout.strip())
            except ValueError:
                pass

        import time

        now = time.time()

        return {
            "churn": commit_count,
            "age_days": (
                int((now - first_commit_ts) / 86400) if first_commit_ts else None
            ),
            "contributors": contributors[:10],  # Limit to 10
            "contributor_count": len(contributors),
            "last_modified_days_ago": (
                int((now - last_modified_ts) / 86400) if last_modified_ts else None
            ),
        }

    except Exception:
        return None


@mcp.tool()
async def get_file_context(file_path: str) -> FileContextResult:
    """
    Get a file overview without reading full content.

    [v1.4.0] Use this tool to quickly assess if a file is relevant to your task
    without consuming tokens on full content. Returns functions, classes, imports,
    complexity score, and security warnings.

    Why AI agents need this:
    - Quickly assess file relevance before extracting code
    - Understand file structure without token overhead
    - Make informed decisions about which functions to modify

    Example::

        result = await get_file_context("src/services/payment.py")

        # Returns FileContextResult:
        # - file_path: "src/services/payment.py"
        # - functions: ["process_payment", "validate_card", "refund_transaction"]
        # - classes: ["PaymentProcessor", "PaymentError"]
        # - imports: ["stripe", "decimal.Decimal", "datetime"]
        # - complexity_score: 8
        # - line_count: 245
        # - has_security_issues: True
        # - security_warnings: ["Potential SQL injection at line 87"]
        # - docstring: "Payment processing service for Stripe integration."

        # Use to decide if file is relevant
        if "payment" in result.functions or result.has_security_issues:
            # Now extract specific functions
            code = await extract_code(file_path, symbol_name="process_payment")

    Args:
        file_path: Path to the file (absolute or relative to project root)
                   Supports: .py, .js, .ts, .java, .go, .rs, .rb, .php

    Returns:
        FileContextResult with functions, classes, imports, complexity, and security warnings
    """
    try:
        return await asyncio.to_thread(_get_file_context_sync, file_path)
    except Exception as e:
        with open("mcp_error.log", "a") as f:
            f.write(f"Error in get_file_context: {e}\n")
            import traceback

            traceback.print_exc(file=f)
        raise


def _get_symbol_references_sync(
    symbol_name: str,
    project_root: str | None = None,
    max_files: int | None = None,
    max_references: int | None = 100,
    scope_prefix: str | None = None,
    include_tests: bool = True,
    enable_categorization: bool = False,
    enable_codeowners: bool = False,
    enable_impact_analysis: bool = False,
    *,
    tier: str | None = None,
    capabilities: list[str] | None = None,
) -> SymbolReferencesResult:
    """
    Synchronous implementation of get_symbol_references.

    [20251220_FEATURE] v3.0.5 - Optimized single-pass AST walking with deduplication
    [20251220_PERF] v3.0.5 - Uses AST cache to avoid re-parsing unchanged files
    [20251226_FEATURE] Enterprise impact analysis with risk scoring
    [20250112_FEATURE] v3.3.0 - Output metadata fields for tier transparency
    """
    # Resolve tier early for metadata
    tier = tier or _get_current_tier()
    capabilities = capabilities or []

    # Determine Pro and Enterprise feature lists
    pro_features = [
        "usage_categorization",
        "read_write_classification",
        "import_classification",
        "scope_filtering",
        "test_file_filtering",
    ]
    enterprise_features = [
        "codeowners_integration",
        "ownership_attribution",
        "impact_analysis",
        "change_risk_assessment",
    ]

    enabled_pro = [f for f in capabilities if f in pro_features] or None
    enabled_enterprise = [f for f in capabilities if f in enterprise_features] or None

    try:
        root = Path(project_root) if project_root else PROJECT_ROOT

        if not root.exists():
            return SymbolReferencesResult(
                success=False,
                symbol_name=symbol_name,
                error=f"Project root not found: {root}.",
                tier_applied=tier,
                pro_features_enabled=enabled_pro,
                enterprise_features_enabled=enabled_enterprise,
            )

        references: list[SymbolReference] = []
        definition_file = None
        definition_line = None
        # Track seen (file, line, col) triples to avoid duplicates in single pass
        seen: set[tuple[str, int, int]] = set()

        def _is_test_path(rel_path: str) -> bool:
            p = rel_path.replace("\\", "/")
            name = p.rsplit("/", 1)[-1]
            return (
                "/tests/" in f"/{p}/"
                or name.startswith("test_")
                or name.endswith("_test.py")
            )

        # [20251226_FEATURE] Enterprise: Enhanced CODEOWNERS support with validation
        def _find_codeowners_file(search_root: Path) -> Path | None:
            candidates = [
                search_root / "CODEOWNERS",
                search_root / ".github" / "CODEOWNERS",
                search_root / "docs" / "CODEOWNERS",
            ]
            for c in candidates:
                if c.exists() and c.is_file():
                    return c
            return None

        def _parse_codeowners(
            path: Path,
        ) -> tuple[list[tuple[str, list[str], int]], list[str] | None]:
            """Parse CODEOWNERS with validation. Returns (rules, default_owners)."""
            rules: list[tuple[str, list[str], int]] = (
                []
            )  # (pattern, owners, specificity)
            default_owners: list[str] | None = None
            try:
                for raw in path.read_text(encoding="utf-8").splitlines():
                    line = raw.strip()
                    if not line or line.startswith("#"):
                        continue
                    parts = line.split()
                    if len(parts) < 2:
                        continue
                    pattern = parts[0].lstrip("/")
                    owners = [p for p in parts[1:] if p.startswith("@")] or parts[1:]
                    # [20251226_FEATURE] Calculate pattern specificity (more specific wins)
                    specificity = len(pattern.split("/")) + (
                        0 if "*" in pattern else 10
                    )
                    if pattern == "*":
                        default_owners = owners
                    else:
                        rules.append((pattern, owners, specificity))
            except Exception:
                return [], None
            # Sort by specificity descending (most specific first)
            rules.sort(key=lambda x: x[2], reverse=True)
            return rules, default_owners

        def _match_owners(
            rules: list[tuple[str, list[str], int]],
            rel_path: str,
            default_owners: list[str] | None = None,
        ) -> tuple[list[str] | None, float]:
            """Match owners with confidence score based on pattern specificity."""
            if not rules and not default_owners:
                return None, 0.0
            import fnmatch

            normalized = rel_path.replace("\\", "/")
            # [20251226_FEATURE] Enhanced pattern matching with recursive support
            for pattern, owners, specificity in rules:
                # Handle recursive patterns (**)
                if "**" in pattern:
                    base = pattern.replace("**", "*")
                    if fnmatch.fnmatch(normalized, base):
                        confidence = min(1.0, specificity / 20.0)
                        return owners, confidence
                # Standard glob matching
                if fnmatch.fnmatch(normalized, pattern):
                    confidence = min(1.0, specificity / 20.0)
                    return owners, confidence
                # Directory match (pattern applies to all files in directory)
                if fnmatch.fnmatch(normalized, f"{pattern}/*"):
                    confidence = min(1.0, (specificity - 1) / 20.0)
                    return owners, confidence
                # Exact directory match
                if normalized.startswith(pattern + "/"):
                    confidence = min(1.0, specificity / 20.0)
                    return owners, confidence
            # Fall back to default owners
            if default_owners:
                return default_owners, 0.3
            return None, 0.0

        codeowners_rules: list[tuple[str, list[str], int]] = []
        default_owners: list[str] | None = None
        if enable_codeowners or enable_impact_analysis:
            codeowners_file = _find_codeowners_file(root)
            if codeowners_file is not None:
                codeowners_rules, default_owners = _parse_codeowners(codeowners_file)

        # [20251226_FEATURE] Enterprise: Track complexity per file for impact analysis
        file_complexity: dict[str, int] = {}

        # Collect candidate python files first to allow file-level limits.
        candidate_files: list[Path] = []
        scope_norm = (scope_prefix or "").strip().lstrip("/")

        # Walk through all Python files
        for py_file in root.rglob("*.py"):
            # Skip common non-source directories
            if any(
                part.startswith(".")
                or part
                in ("__pycache__", "node_modules", "venv", ".venv", "dist", "build")
                for part in py_file.parts
            ):
                continue

            try:
                rel_path = str(py_file.relative_to(root))
            except Exception:
                continue

            if scope_norm and not rel_path.replace("\\", "/").startswith(
                scope_norm.replace("\\", "/")
            ):
                continue

            if not include_tests and _is_test_path(rel_path):
                continue

            candidate_files.append(py_file)

        total_files = len(candidate_files)
        files_truncated = False
        file_truncation_warning = None
        if max_files is not None and total_files > max_files:
            candidate_files = candidate_files[:max_files]
            files_truncated = True
            file_truncation_warning = (
                f"File scan truncated: scanned {max_files} of {total_files} files."
            )

        category_counts: dict[str, int] = {}
        owner_counts: dict[str, int] = {}

        for py_file in candidate_files:

            try:
                # [20251220_PERF] v3.0.5 - Use cached AST parsing
                tree = _parse_file_cached(py_file)
                if tree is None:
                    continue

                code = py_file.read_text(encoding="utf-8")
                lines = code.splitlines()
                rel_path = str(py_file.relative_to(root))
                is_test_file = _is_test_path(rel_path)

                # [20251226_FEATURE] Enhanced CODEOWNERS with confidence
                owners = None
                owner_confidence = 0.0
                if enable_codeowners or enable_impact_analysis:
                    owners, owner_confidence = _match_owners(
                        codeowners_rules, rel_path, default_owners
                    )

                # [20251226_FEATURE] Enterprise: Track file complexity for impact analysis
                if enable_impact_analysis and rel_path not in file_complexity:
                    # Simple complexity heuristic: count branches and function definitions
                    complexity = 0
                    for n in ast.walk(tree):
                        if isinstance(
                            n, (ast.If, ast.For, ast.While, ast.Try, ast.With)
                        ):
                            complexity += 1
                        elif isinstance(n, (ast.FunctionDef, ast.AsyncFunctionDef)):
                            complexity += 2
                        elif isinstance(n, ast.ClassDef):
                            complexity += 3
                    file_complexity[rel_path] = complexity

                # [20251226_FEATURE] Pro tier: Track first assignments for assignment context
                first_assigned: set[str] = set()

                # Single-pass AST walk with optimized checks
                for node in ast.walk(tree):
                    node_line = getattr(node, "lineno", 0)
                    node_col = getattr(node, "col_offset", 0)

                    # Skip if already seen this location in this file
                    loc_key = (rel_path, node_line, node_col)
                    if loc_key in seen:
                        continue

                    is_def = False
                    matched = False
                    ref_type = "reference"

                    # Check definitions (FunctionDef, AsyncFunctionDef, ClassDef)
                    if isinstance(
                        node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)
                    ):
                        if node.name == symbol_name:
                            matched = True
                            is_def = True
                            ref_type = "definition"
                            if definition_file is None:
                                definition_file = rel_path
                                definition_line = node_line

                        # [20251226_FEATURE] Pro tier: Check decorators on functions/classes
                        if enable_categorization and hasattr(node, "decorator_list"):
                            for decorator in node.decorator_list:
                                dec_line = getattr(decorator, "lineno", 0)
                                dec_col = getattr(decorator, "col_offset", 0)
                                dec_key = (rel_path, dec_line, dec_col)
                                if dec_key in seen:
                                    continue
                                dec_matched = False
                                if (
                                    isinstance(decorator, ast.Name)
                                    and decorator.id == symbol_name
                                ):
                                    dec_matched = True
                                elif (
                                    isinstance(decorator, ast.Attribute)
                                    and decorator.attr == symbol_name
                                ):
                                    dec_matched = True
                                elif isinstance(decorator, ast.Call):
                                    dec_func = decorator.func
                                    if (
                                        isinstance(dec_func, ast.Name)
                                        and dec_func.id == symbol_name
                                    ):
                                        dec_matched = True
                                    elif (
                                        isinstance(dec_func, ast.Attribute)
                                        and dec_func.attr == symbol_name
                                    ):
                                        dec_matched = True
                                if dec_matched:
                                    seen.add(dec_key)
                                    dec_context = (
                                        lines[dec_line - 1]
                                        if 0 < dec_line <= len(lines)
                                        else ""
                                    )
                                    category_counts["decorator"] = (
                                        category_counts.get("decorator", 0) + 1
                                    )
                                    if owners:
                                        for owner in owners:
                                            owner_counts[owner] = (
                                                owner_counts.get(owner, 0) + 1
                                            )
                                    references.append(
                                        SymbolReference(
                                            file=rel_path,
                                            line=dec_line,
                                            column=dec_col,
                                            context=dec_context.strip(),
                                            is_definition=False,
                                            reference_type="decorator",
                                            is_test_file=is_test_file,
                                            owners=owners,
                                        )
                                    )
                                    if (
                                        max_references is not None
                                        and len(references) >= max_references
                                    ):
                                        break

                        # [20251226_FEATURE] Pro tier: Check return type annotation
                        if enable_categorization and isinstance(
                            node, (ast.FunctionDef, ast.AsyncFunctionDef)
                        ):
                            if node.returns is not None:
                                ret_ann = node.returns
                                ret_line = getattr(ret_ann, "lineno", node_line)
                                ret_col = getattr(ret_ann, "col_offset", 0)
                                ret_key = (rel_path, ret_line, ret_col)
                                if ret_key not in seen:
                                    ret_matched = False
                                    if (
                                        isinstance(ret_ann, ast.Name)
                                        and ret_ann.id == symbol_name
                                    ):
                                        ret_matched = True
                                    elif (
                                        isinstance(ret_ann, ast.Attribute)
                                        and ret_ann.attr == symbol_name
                                    ):
                                        ret_matched = True
                                    elif isinstance(ret_ann, ast.Subscript):
                                        val = ret_ann.value
                                        if (
                                            isinstance(val, ast.Name)
                                            and val.id == symbol_name
                                        ):
                                            ret_matched = True
                                        elif (
                                            isinstance(val, ast.Attribute)
                                            and val.attr == symbol_name
                                        ):
                                            ret_matched = True
                                    if ret_matched:
                                        seen.add(ret_key)
                                        ret_context = (
                                            lines[ret_line - 1]
                                            if 0 < ret_line <= len(lines)
                                            else ""
                                        )
                                        category_counts["type_annotation"] = (
                                            category_counts.get("type_annotation", 0)
                                            + 1
                                        )
                                        if owners:
                                            for owner in owners:
                                                owner_counts[owner] = (
                                                    owner_counts.get(owner, 0) + 1
                                                )
                                        references.append(
                                            SymbolReference(
                                                file=rel_path,
                                                line=ret_line,
                                                column=ret_col,
                                                context=ret_context.strip(),
                                                is_definition=False,
                                                reference_type="type_annotation",
                                                is_test_file=is_test_file,
                                                owners=owners,
                                            )
                                        )

                            # [20251226_FEATURE] Pro tier: Check parameter type annotations
                            for arg in (
                                node.args.args
                                + node.args.posonlyargs
                                + node.args.kwonlyargs
                            ):
                                if arg.annotation is not None:
                                    ann = arg.annotation
                                    ann_line = getattr(ann, "lineno", node_line)
                                    ann_col = getattr(ann, "col_offset", 0)
                                    ann_key = (rel_path, ann_line, ann_col)
                                    if ann_key not in seen:
                                        ann_matched = False
                                        if (
                                            isinstance(ann, ast.Name)
                                            and ann.id == symbol_name
                                        ):
                                            ann_matched = True
                                        elif (
                                            isinstance(ann, ast.Attribute)
                                            and ann.attr == symbol_name
                                        ):
                                            ann_matched = True
                                        elif isinstance(ann, ast.Subscript):
                                            val = ann.value
                                            if (
                                                isinstance(val, ast.Name)
                                                and val.id == symbol_name
                                            ):
                                                ann_matched = True
                                            elif (
                                                isinstance(val, ast.Attribute)
                                                and val.attr == symbol_name
                                            ):
                                                ann_matched = True
                                        if ann_matched:
                                            seen.add(ann_key)
                                            ann_context = (
                                                lines[ann_line - 1]
                                                if 0 < ann_line <= len(lines)
                                                else ""
                                            )
                                            category_counts["type_annotation"] = (
                                                category_counts.get(
                                                    "type_annotation", 0
                                                )
                                                + 1
                                            )
                                            if owners:
                                                for owner in owners:
                                                    owner_counts[owner] = (
                                                        owner_counts.get(owner, 0) + 1
                                                    )
                                            references.append(
                                                SymbolReference(
                                                    file=rel_path,
                                                    line=ann_line,
                                                    column=ann_col,
                                                    context=ann_context.strip(),
                                                    is_definition=False,
                                                    reference_type="type_annotation",
                                                    is_test_file=is_test_file,
                                                    owners=owners,
                                                )
                                            )

                    # [20251226_FEATURE] Pro tier: Check annotated variable assignments
                    elif isinstance(node, ast.AnnAssign) and enable_categorization:
                        ann = node.annotation
                        ann_line = getattr(ann, "lineno", node_line)
                        ann_col = getattr(ann, "col_offset", 0)
                        ann_key = (rel_path, ann_line, ann_col)
                        if ann_key not in seen:
                            ann_matched = False
                            if isinstance(ann, ast.Name) and ann.id == symbol_name:
                                ann_matched = True
                            elif (
                                isinstance(ann, ast.Attribute)
                                and ann.attr == symbol_name
                            ):
                                ann_matched = True
                            elif isinstance(ann, ast.Subscript):
                                val = ann.value
                                if isinstance(val, ast.Name) and val.id == symbol_name:
                                    ann_matched = True
                                elif (
                                    isinstance(val, ast.Attribute)
                                    and val.attr == symbol_name
                                ):
                                    ann_matched = True
                            if ann_matched:
                                seen.add(ann_key)
                                ann_context = (
                                    lines[ann_line - 1]
                                    if 0 < ann_line <= len(lines)
                                    else ""
                                )
                                category_counts["type_annotation"] = (
                                    category_counts.get("type_annotation", 0) + 1
                                )
                                if owners:
                                    for owner in owners:
                                        owner_counts[owner] = (
                                            owner_counts.get(owner, 0) + 1
                                        )
                                references.append(
                                    SymbolReference(
                                        file=rel_path,
                                        line=ann_line,
                                        column=ann_col,
                                        context=ann_context.strip(),
                                        is_definition=False,
                                        reference_type="type_annotation",
                                        is_test_file=is_test_file,
                                        owners=owners,
                                    )
                                )

                    # Check imports
                    elif isinstance(node, ast.ImportFrom):
                        for alias in node.names:
                            if alias.name == symbol_name or alias.asname == symbol_name:
                                matched = True
                                ref_type = "import"
                                break
                    elif isinstance(node, ast.Import):
                        for alias in node.names:
                            if alias.asname == symbol_name:
                                matched = True
                                ref_type = "import"
                                break

                    # Check function calls
                    elif isinstance(node, ast.Call):
                        func = node.func
                        if isinstance(func, ast.Name) and func.id == symbol_name:
                            matched = True
                            ref_type = "call"
                        elif (
                            isinstance(func, ast.Attribute) and func.attr == symbol_name
                        ):
                            matched = True
                            # [20251226_FEATURE] Pro tier: Distinguish method call context
                            if enable_categorization:
                                value = func.value
                                if isinstance(value, ast.Name):
                                    if value.id == "self":
                                        ref_type = "instance_method_call"
                                    elif value.id == "cls":
                                        ref_type = "class_method_call"
                                    elif value.id[0].isupper():
                                        # Heuristic: ClassName.method() is static/class method
                                        ref_type = "static_method_call"
                                    else:
                                        ref_type = "method_call"
                                else:
                                    ref_type = "method_call"
                            else:
                                ref_type = "call"

                    # Check name references (but avoid duplicating Call nodes)
                    elif isinstance(node, ast.Name) and node.id == symbol_name:
                        matched = True
                        if enable_categorization:
                            if isinstance(node.ctx, ast.Store):
                                # [20251226_FEATURE] Pro tier: Track first assignment vs reassignment
                                if symbol_name not in first_assigned:
                                    first_assigned.add(symbol_name)
                                    ref_type = "first_assignment"
                                else:
                                    ref_type = "reassignment"
                            elif isinstance(node.ctx, ast.Load):
                                ref_type = "read"
                            else:
                                ref_type = "reference"

                    if matched:
                        seen.add(loc_key)
                        context = (
                            lines[node_line - 1] if 0 < node_line <= len(lines) else ""
                        )
                        if enable_categorization:
                            category_counts[ref_type] = (
                                category_counts.get(ref_type, 0) + 1
                            )
                        if owners:
                            for owner in owners:
                                owner_counts[owner] = owner_counts.get(owner, 0) + 1
                        references.append(
                            SymbolReference(
                                file=rel_path,
                                line=node_line,
                                column=node_col,
                                context=context.strip(),
                                is_definition=is_def,
                                reference_type=(
                                    ref_type if enable_categorization else None
                                ),
                                is_test_file=is_test_file,
                                owners=owners,
                            )
                        )

                        if (
                            max_references is not None
                            and len(references) >= max_references
                        ):
                            break

                if max_references is not None and len(references) >= max_references:
                    break

            except (SyntaxError, UnicodeDecodeError):
                # Skip files that can't be parsed
                continue

        # Sort: definitions first, then by file and line
        references.sort(key=lambda r: (not r.is_definition, r.file, r.line))

        # [20251220_FEATURE] v3.0.5 - Communicate truncation
        total_refs = len(references)
        refs_truncated = max_references is not None and total_refs >= max_references
        truncation_msg = None
        if refs_truncated:
            truncation_msg = "Results truncated: output reached the configured maximum reference limit."

        # [20251226_FEATURE] Enterprise: Advanced risk assessment with weighted factors
        change_risk = None
        risk_score = None
        risk_factors: list[str] | None = None
        blast_radius = None
        test_coverage_ratio = None
        complexity_hotspots_list: list[str] | None = None
        impact_mermaid = None
        codeowners_coverage = None

        unique_files = 0
        if enable_codeowners or enable_impact_analysis:
            unique_files = len({r.file for r in references})
            blast_radius = unique_files

            # Calculate test coverage ratio
            test_refs = sum(1 for r in references if r.is_test_file)
            test_coverage_ratio = test_refs / total_refs if total_refs > 0 else 0.0

            # Calculate CODEOWNERS coverage
            owned_refs = sum(1 for r in references if r.owners)
            codeowners_coverage = owned_refs / total_refs if total_refs > 0 else 0.0

        if enable_impact_analysis:
            # [20251226_FEATURE] Enterprise: Weighted risk score calculation
            risk_factors = []
            base_score = 0

            # Factor 1: Reference count (0-25 points)
            if total_refs >= 100:
                base_score += 25
                risk_factors.append(f"High reference count ({total_refs} references)")
            elif total_refs >= 50:
                base_score += 20
                risk_factors.append(
                    f"Moderate reference count ({total_refs} references)"
                )
            elif total_refs >= 20:
                base_score += 10
                risk_factors.append(f"Multiple references ({total_refs} references)")

            # Factor 2: Blast radius / unique files (0-25 points)
            if unique_files >= 20:
                base_score += 25
                risk_factors.append(f"Wide blast radius ({unique_files} files)")
            elif unique_files >= 10:
                base_score += 15
                risk_factors.append(f"Moderate blast radius ({unique_files} files)")
            elif unique_files >= 5:
                base_score += 8
                risk_factors.append(f"Multiple files affected ({unique_files} files)")

            # Factor 3: Test coverage (0-20 points, higher coverage = lower risk)
            if test_coverage_ratio is not None:
                if test_coverage_ratio < 0.1:
                    base_score += 20
                    risk_factors.append(
                        f"Low test coverage ({test_coverage_ratio:.1%})"
                    )
                elif test_coverage_ratio < 0.3:
                    base_score += 10
                    risk_factors.append(
                        f"Moderate test coverage ({test_coverage_ratio:.1%})"
                    )

            # Factor 4: Complexity hotspots (0-15 points)
            if file_complexity:
                hotspot_threshold = 15
                complexity_hotspots_list = [
                    f
                    for f, c in file_complexity.items()
                    if c >= hotspot_threshold and any(r.file == f for r in references)
                ]
                if len(complexity_hotspots_list) >= 5:
                    base_score += 15
                    risk_factors.append(
                        f"Multiple complexity hotspots ({len(complexity_hotspots_list)} files)"
                    )
                elif len(complexity_hotspots_list) >= 2:
                    base_score += 8
                    risk_factors.append(
                        f"Some complexity hotspots ({len(complexity_hotspots_list)} files)"
                    )

            # Factor 5: Ownership coverage (0-15 points, better coverage = lower risk)
            if codeowners_coverage is not None and codeowners_coverage < 0.5:
                base_score += 15
                risk_factors.append(
                    f"Low ownership coverage ({codeowners_coverage:.1%})"
                )
            elif codeowners_coverage is not None and codeowners_coverage < 0.8:
                base_score += 5
                risk_factors.append(
                    f"Partial ownership coverage ({codeowners_coverage:.1%})"
                )

            risk_score = min(100, base_score)

            # Determine risk level from score
            if risk_score >= 60:
                change_risk = "high"
            elif risk_score >= 30:
                change_risk = "medium"
            else:
                change_risk = "low"

            # [20251226_FEATURE] Enterprise: Generate impact Mermaid diagram
            if unique_files <= 20:  # Only generate for reasonable sizes
                mermaid_lines = ["graph TD"]
                mermaid_lines.append(f'    SYMBOL["{symbol_name}"]')
                file_nodes: dict[str, str] = {}
                for i, file in enumerate(sorted({r.file for r in references})):
                    node_id = f"F{i}"
                    file_nodes[file] = node_id
                    ref_count = sum(1 for r in references if r.file == file)
                    is_test = any(r.is_test_file for r in references if r.file == file)
                    label = file.replace("/", "_").replace(".", "_")
                    if is_test:
                        mermaid_lines.append(
                            f'    {node_id}["{label} ({ref_count})"]:::test'
                        )
                    else:
                        mermaid_lines.append(f'    {node_id}["{label} ({ref_count})"]')
                    mermaid_lines.append(f"    SYMBOL --> {node_id}")
                mermaid_lines.append("    classDef test fill:#90EE90")
                impact_mermaid = "\n".join(mermaid_lines)

        elif enable_codeowners:
            # Simple risk assessment for CODEOWNERS-only (no impact_analysis)
            if total_refs >= 50 or unique_files >= 20:
                change_risk = "high"
            elif total_refs >= 10 or unique_files >= 5:
                change_risk = "medium"
            else:
                change_risk = "low"

        return SymbolReferencesResult(
            success=True,
            symbol_name=symbol_name,
            definition_file=definition_file,
            definition_line=definition_line,
            references=references,
            total_references=total_refs,
            files_scanned=len(candidate_files),
            total_files=total_files,
            files_truncated=files_truncated,
            file_truncation_warning=file_truncation_warning,
            category_counts=category_counts if enable_categorization else None,
            owner_counts=owner_counts if enable_codeowners else None,
            change_risk=change_risk,
            references_truncated=refs_truncated,
            truncation_warning=truncation_msg,
            # [20251226_FEATURE] Enterprise impact analysis fields
            risk_score=risk_score,
            risk_factors=risk_factors,
            blast_radius=blast_radius,
            test_coverage_ratio=test_coverage_ratio,
            complexity_hotspots=complexity_hotspots_list,
            impact_mermaid=impact_mermaid,
            codeowners_coverage=codeowners_coverage,
            # [20250112_FEATURE] Output metadata fields
            tier_applied=tier,
            max_files_applied=max_files,
            max_references_applied=max_references,
            pro_features_enabled=enabled_pro,
            enterprise_features_enabled=enabled_enterprise,
        )

    except Exception as e:
        return SymbolReferencesResult(
            success=False,
            symbol_name=symbol_name,
            error=f"Search failed: {str(e)}",
            tier_applied=tier,
            pro_features_enabled=enabled_pro,
            enterprise_features_enabled=enabled_enterprise,
        )


@mcp.tool()
async def get_symbol_references(
    symbol_name: str,
    project_root: str | None = None,
    scope_prefix: str | None = None,
    include_tests: bool = True,
    ctx: Context | None = None,
) -> SymbolReferencesResult:
    """
    Find all references to a symbol across the project.

    **Tier Behavior:**
    - All tiers: Finds definition and usage references.
    - Community: Applies file/reference limits from the capability matrix.
    - Pro: Adds reference categorization and filtering controls.
    - Enterprise: Adds CODEOWNERS-based ownership attribution and risk metadata.

    [v1.4.0] Use this tool before modifying a function, class, or variable to
    understand its usage across the codebase. Essential for safe refactoring.

    [v3.0.5] Now reports progress as files are scanned.
    [v3.2.8] Tier-based result limiting for Community tier.

    Why AI agents need this:
    - Safe refactoring: know all call sites before changing signatures
    - Impact analysis: understand blast radius of changes
    - No hallucination: real references, not guessed ones

    Example::

        result = await get_symbol_references("process_order")

        # Returns SymbolReferencesResult:
        # - symbol_name: "process_order"
        # - definition_file: "services/order.py"
        # - definition_line: 42
        # - total_references: 7
        # - references: [
        #     SymbolReference(
        #         file="handlers/api.py",
        #         line=156,
        #         column=8,
        #         context="        result = process_order(order_data)",
        #         is_definition=False
        #     ),
        #     SymbolReference(
        #         file="tests/test_order.py",
        #         line=23,
        #         column=4,
        #         context="    process_order(mock_order)",
        #         is_definition=False
        #     ),
        #     ...
        # ]

        # Before changing function signature, check all call sites
        if result.total_references > 0:
            print(f"Warning: {result.total_references} call sites to update")

    Args:
        symbol_name: Name of the function, class, or variable to search for
        project_root: Project root directory (default: server's project root)

    Returns:
        SymbolReferencesResult with definition_file, definition_line, and all references
    """
    # [20251220_FEATURE] v3.0.5 - Progress reporting for file scanning
    if ctx:
        await ctx.report_progress(0, 100, f"Searching for '{symbol_name}'...")

    # [20251225_FEATURE] Capability-driven tier behavior (no upgrade hints)
    # [20251225_BUGFIX] Use the module-local tier helper to match licensing wiring.
    tier = _get_current_tier()
    caps = get_tool_capabilities("get_symbol_references", tier) or {}
    limits = caps.get("limits", {}) if isinstance(caps, dict) else {}
    cap_list = caps.get("capabilities", []) if isinstance(caps, dict) else []
    cap_set = set(cap_list) if isinstance(cap_list, (list, set, tuple)) else set()

    max_files = limits.get("max_files_searched")
    if max_files is None:
        max_files = limits.get("max_files")

    max_references = limits.get("max_references")

    enable_categorization = bool(
        {"usage_categorization", "read_write_classification", "import_classification"}
        & cap_set
    )
    enable_codeowners = bool(
        {"codeowners_integration", "ownership_attribution", "impact_analysis"} & cap_set
    )

    # Only allow Pro+ filtering controls when capability is present.
    effective_scope = scope_prefix if "scope_filtering" in cap_set else None
    effective_include_tests = (
        include_tests if "test_file_filtering" in cap_set else True
    )

    # [20251226_FEATURE] Enterprise: Enable impact analysis for advanced risk assessment
    enable_impact_analysis = bool(
        {"impact_analysis", "change_risk_assessment"} & cap_set
    )

    result = await asyncio.to_thread(
        _get_symbol_references_sync,
        symbol_name,
        project_root,
        max_files,
        max_references,
        effective_scope,
        effective_include_tests,
        enable_categorization,
        enable_codeowners,
        enable_impact_analysis,
        tier=tier,
        capabilities=list(cap_set),
    )

    if ctx:
        await ctx.report_progress(
            100, 100, f"Found {result.total_references} references"
        )

    return result


# ============================================================================
# [20251213_FEATURE] v1.5.0 - get_call_graph MCP Tool
# [20260110_FEATURE] v3.3.0 - Pre-release: confidence, context, paths, focus
# ============================================================================


class CallContextModel(BaseModel):
    """Context information about where a call is made.

    [20260110_FEATURE] v3.3.0 - Call context metadata for security analysis.
    Knowing if a call is conditional or in a try block helps agents assess risk.
    """

    in_loop: bool = Field(default=False, description="Call is inside a loop")
    in_try_block: bool = Field(default=False, description="Call is inside a try block")
    in_conditional: bool = Field(
        default=False, description="Call is inside an if/else block"
    )
    condition_summary: str | None = Field(
        default=None, description="Summary of condition, e.g., 'if user.is_admin'"
    )
    in_async: bool = Field(default=False, description="Call is in an async function")
    in_except_handler: bool = Field(
        default=False, description="Call is in an except handler"
    )


class CallNodeModel(BaseModel):
    """Node in the call graph representing a function."""

    name: str = Field(description="Function name")
    file: str = Field(description="File path (relative) or '<external>'")
    line: int = Field(description="Line number (0 if unknown)")
    end_line: int | None = Field(default=None, description="End line number")
    is_entry_point: bool = Field(
        default=False, description="Whether function is an entry point"
    )
    # [20260110_FEATURE] v3.3.0 - source_uri for IDE click-through
    source_uri: str | None = Field(
        default=None, description="IDE-friendly URI: file:///path#L42"
    )
    # [20251225_FEATURE] Enterprise metrics (best-effort)
    in_degree: int | None = Field(default=None, description="Inbound call count")
    out_degree: int | None = Field(default=None, description="Outbound call count")


class CallEdgeModel(BaseModel):
    """Edge in the call graph representing a function call."""

    caller: str = Field(description="Caller function (file:name)")
    callee: str = Field(description="Callee function (file:name or external name)")
    # [20260110_FEATURE] v3.3.0 - Confidence scoring for call edges
    confidence: float = Field(
        default=1.0,
        ge=0.0,
        le=1.0,
        description="Confidence: 1.0=static, 0.8=type_hint, 0.5=inferred",
    )
    inference_source: str = Field(
        default="static",
        description="How edge was inferred: static, type_hint, class_hierarchy, pattern_match",
    )
    call_line: int | None = Field(default=None, description="Line number of the call")
    # [20260110_FEATURE] v3.3.0 - Call context metadata
    context: CallContextModel | None = Field(
        default=None, description="Context where call is made"
    )


class CallGraphResultModel(BaseModel):
    """Result of call graph analysis."""

    success: bool = Field(default=True, description="Whether analysis succeeded")
    server_version: str = Field(default=__version__, description="Code Scalpel version")
    nodes: list[CallNodeModel] = Field(
        default_factory=list, description="Functions in the graph"
    )
    edges: list[CallEdgeModel] = Field(
        default_factory=list, description="Call relationships"
    )
    entry_point: str | None = Field(
        default=None, description="Entry point used for filtering"
    )
    depth_limit: int | None = Field(default=None, description="Depth limit used")
    mermaid: str = Field(default="", description="Mermaid diagram representation")
    circular_imports: list[list[str]] = Field(
        default_factory=list, description="Detected import cycles"
    )
    # [20260110_FEATURE] v3.3.0 - Path query results
    paths: list[list[str]] = Field(
        default_factory=list,
        description="Paths between source and sink (if paths_from/paths_to specified)",
    )
    # [20260110_FEATURE] v3.3.0 - Subgraph focus mode
    focus_functions: list[str] | None = Field(
        default=None, description="Functions the subgraph is focused on"
    )
    # [20251225_FEATURE] Neutral truncation metadata (tier limits)
    total_nodes: int | None = Field(
        default=None, description="Total nodes before truncation"
    )
    total_edges: int | None = Field(
        default=None, description="Total edges before truncation"
    )
    nodes_truncated: bool | None = Field(
        default=None, description="Whether nodes were truncated"
    )
    edges_truncated: bool | None = Field(
        default=None, description="Whether edges were truncated"
    )
    truncation_warning: str | None = Field(
        default=None, description="Neutral truncation note if applied"
    )
    # [20251225_FEATURE] Enterprise extras (best-effort)
    hot_nodes: list[str] = Field(default_factory=list, description="High-degree nodes")
    dead_code_candidates: list[str] = Field(
        default_factory=list, description="Potentially unreferenced nodes"
    )
    # [20260111_FEATURE] v1.0 validation - Output metadata for transparency
    tier_applied: str = Field(default="community", description="Tier used for analysis")
    max_depth_applied: int | None = Field(
        default=None, description="Max depth limit applied (None = unlimited)"
    )
    max_nodes_applied: int | None = Field(
        default=None, description="Max nodes limit applied (None = unlimited)"
    )
    advanced_resolution_enabled: bool = Field(
        default=False, description="Whether advanced call resolution was enabled"
    )
    enterprise_metrics_enabled: bool = Field(
        default=False, description="Whether enterprise metrics were enabled"
    )
    error: str | None = Field(default=None, description="Error message if failed")


def _get_call_graph_sync(
    project_root: str | None,
    entry_point: str | None,
    depth: int,
    include_circular_import_check: bool,
    max_nodes: int | None = None,
    advanced_resolution: bool = False,
    include_enterprise_metrics: bool = False,
    paths_from: str | None = None,
    paths_to: str | None = None,
    focus_functions: list[str] | None = None,
) -> CallGraphResultModel:
    """Synchronous implementation of get_call_graph."""
    from code_scalpel.ast_tools.call_graph import CallGraphBuilder

    # [20251226_BUGFIX] Ensure deterministic truncation and advanced resolution enrichment.
    def _infer_polymorphic_edges(
        root: Path, graph_nodes: list[CallNodeModel]
    ) -> set[tuple[str, str]]:
        """Best-effort inference of self.* call edges for class methods when advanced_resolution is enabled."""
        import ast

        edges: set[tuple[str, str]] = set()
        files = {n.file for n in graph_nodes if n.file not in {"<external>", ""}}
        for rel_path in files:
            file_path = root / rel_path
            if not file_path.exists():
                continue
            try:
                code = file_path.read_text(encoding="utf-8")
                tree = ast.parse(code)
            except Exception:
                continue

            class_name: str | None = None
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    class_name = node.name
                if (
                    isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef))
                    and class_name
                ):
                    caller = f"{rel_path}:{class_name}.{node.name}"
                    for inner in ast.walk(node):
                        if isinstance(inner, ast.Call) and isinstance(
                            inner.func, ast.Attribute
                        ):
                            if (
                                isinstance(inner.func.value, ast.Name)
                                and inner.func.value.id == "self"
                            ):
                                callee = f"{rel_path}:{class_name}.{inner.func.attr}"
                                edges.add((caller, callee))
        return edges

    root_path = Path(project_root) if project_root else PROJECT_ROOT

    if not root_path.exists():
        return CallGraphResultModel(
            success=False,
            error=f"Project root not found: {root_path}.",
        )

    try:
        builder = CallGraphBuilder(root_path)
        result = builder.build_with_details(
            entry_point=entry_point,
            depth=depth,
            max_nodes=max_nodes,
            advanced_resolution=advanced_resolution,
        )

        # [20260110_FEATURE] v3.3.0 - Convert dataclasses to Pydantic models with new fields
        nodes = [
            CallNodeModel(
                name=n.name,
                file=n.file,
                line=n.line,
                end_line=n.end_line,
                is_entry_point=n.is_entry_point,
                # [20260110_FEATURE] v3.3.0 - Source URI for IDE click-through
                source_uri=n.source_uri,
            )
            for n in result.nodes
        ]

        # [20260110_FEATURE] v3.3.0 - Convert edges with context/confidence metadata
        edges = []
        for e in result.edges:
            context_model = None
            if e.context is not None:
                context_model = CallContextModel(
                    in_loop=e.context.in_loop,
                    in_try_block=e.context.in_try_block,
                    in_conditional=e.context.in_conditional,
                    condition_summary=e.context.condition_summary,
                    in_async=e.context.in_async,
                    in_except_handler=e.context.in_except_handler,
                )
            edges.append(
                CallEdgeModel(
                    caller=e.caller,
                    callee=e.callee,
                    call_line=e.call_line,
                    confidence=e.confidence,
                    inference_source=e.inference_source,
                    context=context_model,
                )
            )

        # [20251226_FEATURE] Add heuristic polymorphism edges when enabled.
        if advanced_resolution:
            extra_edges = _infer_polymorphic_edges(root_path, nodes)
            existing = {(e.caller, e.callee) for e in edges}
            for caller, callee in extra_edges:
                if (caller, callee) not in existing:
                    edges.append(CallEdgeModel(caller=caller, callee=callee))
                    existing.add((caller, callee))

        # [20251226_BUGFIX] Apply explicit truncation to honor limits even if builder is permissive.
        total_nodes = len(nodes)
        total_edges = len(edges)
        nodes_truncated = bool(getattr(result, "nodes_truncated", False))
        edges_truncated = bool(getattr(result, "edges_truncated", False))
        if max_nodes is not None and total_nodes > max_nodes:
            nodes = nodes[:max_nodes]
            nodes_truncated = True
            # [20251230_BUGFIX] Normalize node identity format for consistent edge filtering
            # Handle both "file:name" and "file:Class.method" formats
            kept_endpoints = set()
            for n in nodes:
                # Add both the full name and simple name to handle format variations
                if n.file != "<external>":
                    kept_endpoints.add(f"{n.file}:{n.name}")
                    # If name contains class qualification, also add without file prefix
                    if "." in n.name:
                        kept_endpoints.add(n.name)
                else:
                    kept_endpoints.add(n.name)

            filtered_edges = [
                e
                for e in edges
                if (e.caller in kept_endpoints and e.callee in kept_endpoints)
            ]
            edges_truncated = edges_truncated or len(filtered_edges) < len(edges)
            edges = filtered_edges

        truncation_warning = None
        if nodes_truncated or edges_truncated:
            parts: list[str] = []
            if max_nodes is not None:
                parts.append(f"max_nodes={max_nodes}")
            parts.append(f"max_depth={depth}")
            truncation_warning = "Results truncated by limits: " + ", ".join(parts)

        hot_nodes: list[str] = []
        dead_code_candidates: list[str] = []
        if include_enterprise_metrics:
            # [20251225_FEATURE] Best-effort degree metrics and dead-code candidates.
            in_deg: dict[str, int] = {}
            out_deg: dict[str, int] = {}

            for edge in edges:
                out_deg[edge.caller] = out_deg.get(edge.caller, 0) + 1
                in_deg[edge.callee] = in_deg.get(edge.callee, 0) + 1

            for node in nodes:
                full_name = (
                    f"{node.file}:{node.name}"
                    if node.file != "<external>"
                    else node.name
                )
                node.in_degree = in_deg.get(full_name, 0)
                node.out_degree = out_deg.get(full_name, 0)

            def _total_degree(n: CallNodeModel) -> tuple[int, str]:
                full = f"{n.file}:{n.name}" if n.file != "<external>" else n.name
                return ((n.in_degree or 0) + (n.out_degree or 0), full)

            hot_nodes = [
                (f"{n.file}:{n.name}" if n.file != "<external>" else n.name)
                for n in sorted(nodes, key=_total_degree, reverse=True)[:10]
                if _total_degree(n)[0] > 0
            ]

            # [20251230_BUGFIX] Optimize dead-code detection: use existing graph instead of rebuilding
            # Only identify dead code from the nodes we already have (don't rebuild full graph)
            # This is more efficient and respects the user's max_nodes limit
            dead_code_candidates = [
                f"{n.file}:{n.name}" if n.file != "<external>" else n.name
                for n in nodes
                if not n.is_entry_point
                and in_deg.get(
                    f"{n.file}:{n.name}" if n.file != "<external>" else n.name, 0
                )
                == 0
            ]

        # Optionally check for circular imports
        circular_imports = []
        if include_circular_import_check:
            circular_imports = builder.detect_circular_imports()

        # [20260110_FEATURE] v3.3.0 - Path query API
        paths: list[list[str]] = []
        if paths_from and paths_to:
            # Build adjacency list from edges for path finding
            adj_list: dict[str, list[str]] = {}
            for e in edges:
                adj_list.setdefault(e.caller, []).append(e.callee)
            find_paths = getattr(builder, "find_paths", None)
            if callable(find_paths):
                found_paths = find_paths(
                    paths_from, paths_to, max_depth=depth, graph=adj_list
                )
                paths = (
                    cast(list[list[str]], found_paths)
                    if isinstance(found_paths, list)
                    else []
                )
            else:
                # Fallback: simple DFS path search within max depth
                def _dfs_paths(start: str, goal: str, max_depth: int) -> list[list[str]]:
                    results: list[list[str]] = []
                    stack: list[tuple[str, list[str]]] = [(start, [start])]
                    while stack:
                        node, path = stack.pop()
                        if len(path) - 1 > max_depth:
                            continue
                        if node == goal:
                            results.append(path)
                            continue
                        for nxt in adj_list.get(node, []):
                            if nxt in path:
                                continue
                            stack.append((nxt, path + [nxt]))
                    return results

                paths = _dfs_paths(paths_from, paths_to, depth)

        # [20260110_FEATURE] v3.3.0 - Focus mode: filter to subgraph around focus_functions
        actual_focus_functions: list[str] | None = None
        if focus_functions:
            # Find k-hop neighborhood around focus functions
            focus_set = set(focus_functions)
            related: set[str] = set(focus_functions)

            # Add all nodes that call or are called by focus functions (1-hop)
            for e in edges:
                if e.caller in focus_set or e.callee in focus_set:
                    related.add(e.caller)
                    related.add(e.callee)

            # Filter nodes and edges to only those in the related set
            nodes = [
                n for n in nodes if f"{n.file}:{n.name}" in related or n.name in related
            ]
            edges = [e for e in edges if e.caller in related or e.callee in related]
            actual_focus_functions = focus_functions

        return CallGraphResultModel(
            nodes=nodes,
            edges=edges,
            entry_point=result.entry_point,
            depth_limit=result.depth_limit,
            mermaid=result.mermaid,
            circular_imports=circular_imports,
            paths=paths,
            focus_functions=actual_focus_functions,
            total_nodes=total_nodes,
            total_edges=total_edges,
            nodes_truncated=nodes_truncated,
            edges_truncated=edges_truncated,
            truncation_warning=truncation_warning,
            hot_nodes=hot_nodes,
            dead_code_candidates=dead_code_candidates,
        )

    except Exception as e:
        return CallGraphResultModel(
            success=False,
            error=f"Call graph analysis failed: {str(e)}",
        )


@mcp.tool()
async def get_call_graph(
    project_root: str | None = None,
    entry_point: str | None = None,
    depth: int = 10,
    include_circular_import_check: bool = True,
    paths_from: str | None = None,
    paths_to: str | None = None,
    focus_functions: list[str] | None = None,
    ctx: Context | None = None,
) -> CallGraphResultModel:
    """
    Build a call graph showing function relationships in the project.

    **Tier Behavior:**
    - All tiers: Tool is available.
    - Limits and optional enhancements are applied based on tool capabilities.

    [v1.5.0] Use this tool to understand code flow and function dependencies.
    Analyzes Python source files to build a static call graph with:
    - Line number tracking for each function
    - Entry point detection (main, CLI commands, routes)
    - Depth-limited traversal from any starting function
    - Mermaid diagram generation for visualization
    - Circular import detection

    [v3.0.5] Now reports progress during graph construction.
    [v3.2.8] Tier-based depth limiting for Community tier.
    [v3.3.0] Added path queries, focus mode, call context, and confidence scoring.

    **v3.3.0 Features:**
    - **Path Queries:** Use paths_from and paths_to to find all call paths between functions
    - **Focus Mode:** Use focus_functions to extract a subgraph centered on specific functions
    - **Call Context:** Each edge includes context (in_loop, in_try_block, in_conditional)
    - **Confidence Scoring:** Each edge includes confidence (1.0=static, 0.8=type_hint, 0.5=inferred)
    - **Source URIs:** Each node includes source_uri for IDE click-through (file:///path#L42)

    Why AI agents need this:
    - Navigation: Quickly understand how functions connect
    - Impact analysis: See what breaks if you change a function
    - Refactoring: Identify tightly coupled code
    - Documentation: Generate visual diagrams of code flow
    - Security: Find call paths from user input to dangerous sinks

    Args:
        project_root: Project root directory (default: server's project root)
        entry_point: Starting function name (e.g., "main" or "app.py:main")
                    If None, includes all functions
        depth: Maximum depth to traverse from entry point (default: 10)
        include_circular_import_check: Check for circular imports (default: True)
        paths_from: Source function for path query (e.g., "routes.py:handle_request")
        paths_to: Sink function for path query (e.g., "db.py:execute_query")
        focus_functions: List of functions to focus the subgraph on

    Returns:
        CallGraphResultModel with nodes, edges, Mermaid diagram, paths, and any circular imports
    """
    # [20251220_FEATURE] v3.0.5 - Progress reporting
    if ctx:
        await ctx.report_progress(0, 100, "Building call graph...")

    # [20251225_FEATURE] Capability-driven tier behavior (no upgrade hints)
    tier = _get_current_tier()
    caps = get_tool_capabilities("get_call_graph", tier) or {}
    limits = caps.get("limits", {}) or {}
    cap_list = caps.get("capabilities", []) or []

    capabilities = set(cap_list) if not isinstance(cap_list, set) else cap_list

    max_depth = limits.get("max_depth")
    max_nodes = limits.get("max_nodes")

    actual_depth = depth
    if max_depth is not None and depth > max_depth:
        actual_depth = max_depth

    advanced_resolution = (
        "polymorphism_resolution" in capabilities
        or "advanced_call_graph" in capabilities
    )
    include_enterprise_metrics = (
        "hot_path_identification" in capabilities
        or "dead_code_detection" in capabilities
        or "custom_graph_analysis" in capabilities
    )

    result = await asyncio.to_thread(
        _get_call_graph_sync,
        project_root,
        entry_point,
        actual_depth,
        include_circular_import_check,
        max_nodes,
        advanced_resolution,
        include_enterprise_metrics,
        paths_from,
        paths_to,
        focus_functions,
    )

    # [20260111_FEATURE] v1.0 validation - Populate output metadata
    result.tier_applied = tier
    result.max_depth_applied = max_depth
    result.max_nodes_applied = max_nodes
    result.advanced_resolution_enabled = advanced_resolution
    result.enterprise_metrics_enabled = include_enterprise_metrics

    if ctx:
        node_count = len(result.nodes) if result.nodes else 0
        edge_count = len(result.edges) if result.edges else 0
        await ctx.report_progress(
            100, 100, f"Call graph complete: {node_count} functions, {edge_count} calls"
        )

    return result


# ============================================================================
# [20251216_FEATURE] v2.5.0 - get_graph_neighborhood MCP Tool
# ============================================================================


class NeighborhoodNodeModel(BaseModel):
    """A node in the neighborhood subgraph."""

    id: str = Field(description="Node ID (language::module::type::name)")
    depth: int = Field(description="Distance from center node (0 = center)")
    metadata: dict = Field(default_factory=dict, description="Additional node metadata")
    # [20251226_FEATURE] Optional higher-tier metadata (best-effort)
    in_degree: int | None = Field(default=None, description="Inbound edge count")
    out_degree: int | None = Field(default=None, description="Outbound edge count")


class NeighborhoodEdgeModel(BaseModel):
    """An edge in the neighborhood subgraph."""

    from_id: str = Field(description="Source node ID")
    to_id: str = Field(description="Target node ID")
    edge_type: str = Field(description="Type of relationship")
    confidence: float = Field(description="Confidence score (0.0-1.0)")


class GraphNeighborhoodResult(BaseModel):
    """Result of k-hop neighborhood extraction."""

    success: bool = Field(description="Whether extraction succeeded")
    server_version: str = Field(default=__version__, description="Code Scalpel version")

    # Center node info
    center_node_id: str = Field(default="", description="ID of the center node")
    k: int = Field(default=0, description="Number of hops used")

    # Subgraph
    nodes: list[NeighborhoodNodeModel] = Field(
        default_factory=list, description="Nodes in the neighborhood"
    )
    edges: list[NeighborhoodEdgeModel] = Field(
        default_factory=list, description="Edges in the neighborhood"
    )
    total_nodes: int = Field(default=0, description="Number of nodes in subgraph")
    total_edges: int = Field(default=0, description="Number of edges in subgraph")

    # Truncation info
    max_depth_reached: int = Field(
        default=0, description="Maximum depth actually reached"
    )
    truncated: bool = Field(default=False, description="Whether graph was truncated")
    truncation_warning: str | None = Field(
        default=None, description="Warning if truncated"
    )

    # Mermaid diagram
    mermaid: str = Field(default="", description="Mermaid diagram of neighborhood")

    # [20251230_BUGFIX] Ensure tier-gated extras exist on the response model.
    # Some clients/tests rely on these fields being present even when empty.
    semantic_neighbors: list[str] = Field(
        default_factory=list,
        description="Pro: semantic neighbor node IDs (best-effort)",
    )
    logical_relationships: list[dict] = Field(
        default_factory=list,
        description="Pro: detected logical relationships (best-effort)",
    )

    query_supported: bool = Field(
        default=False, description="Enterprise: graph query language supported"
    )
    traversal_rules_available: bool = Field(
        default=False,
        description="Enterprise: custom traversal rules supported",
    )
    path_constraints_supported: bool = Field(
        default=False,
        description="Enterprise: path constraint queries supported",
    )

    # [20251226_FEATURE] Optional higher-tier metadata
    hot_nodes: list[str] = Field(
        default_factory=list, description="High-degree nodes in the returned subgraph"
    )

    error: str | None = Field(default=None, description="Error message if failed")


def _generate_neighborhood_mermaid(
    nodes: list[NeighborhoodNodeModel],
    edges: list[NeighborhoodEdgeModel],
    center_node_id: str,
) -> str:
    """Generate Mermaid diagram for neighborhood."""
    lines = ["graph TD"]

    # Add nodes with depth-based styling
    for node in nodes:
        # Sanitize node ID for Mermaid
        safe_id = node.id.replace("::", "_").replace(".", "_").replace("-", "_")
        label = node.id.split("::")[-1] if "::" in node.id else node.id

        if node.depth == 0:
            # Center node - special styling
            lines.append(f'    {safe_id}["{label}"]:::center')
        elif node.depth == 1:
            lines.append(f'    {safe_id}["{label}"]:::depth1')
        else:
            lines.append(f'    {safe_id}["{label}"]:::depth2plus')

    # Add edges
    for edge in edges:
        from_safe = edge.from_id.replace("::", "_").replace(".", "_").replace("-", "_")
        to_safe = edge.to_id.replace("::", "_").replace(".", "_").replace("-", "_")
        lines.append(f"    {from_safe} --> {to_safe}")

    # Add style definitions
    lines.append("    classDef center fill:#f9f,stroke:#333,stroke-width:3px")
    lines.append("    classDef depth1 fill:#bbf,stroke:#333,stroke-width:2px")
    lines.append("    classDef depth2plus fill:#ddd,stroke:#333,stroke-width:1px")

    return "\n".join(lines)


def _normalize_graph_center_node_id(center_node_id: str) -> str:
    """Normalize common legacy node-id formats into canonical IDs.

    Canonical format: python::<module>::function::<name>

    Accepted legacy inputs:
    - routes.py:search_route
    - path/to/routes.py:search_route
    - routes:search_route
    """
    raw = (center_node_id or "").strip()
    if not raw:
        return raw

    if raw.startswith("python::") and "::function::" in raw:
        return raw

    # Common legacy format: <file>:<symbol>
    if ":" in raw and "::" not in raw:
        left, right = raw.rsplit(":", 1)
        file_part = left.strip()
        name = right.strip()
        if not name:
            return raw

        # If this looks like a path, convert to module.
        if file_part.endswith(".py"):
            module = file_part.replace("/", ".").replace("\\", ".")
            if module.endswith(".py"):
                module = module[: -len(".py")]
            # Drop leading dots that can happen with absolute-ish inputs.
            module = module.strip(".")
            if module:
                return f"python::{module}::function::{name}"

        # If this looks like a bare module name.
        module = file_part.replace("/", ".").replace("\\", ".").strip(".")
        if module:
            return f"python::{module}::function::{name}"

    return raw


def _fast_validate_python_function_node_exists(
    root_path: Path, center_node_id: str
) -> tuple[bool, str | None]:
    """Best-effort fast validation for python::<module>::function::<name>.

    This avoids building the full call graph when the node ID points to a module
    file that doesn't exist or a function name that doesn't exist in that file.

    Returns:
        (ok, error_message)
    """
    import re

    m = re.match(
        r"^(?P<lang>[^:]+)::(?P<module>[^:]+)::(?P<kind>[^:]+)::(?P<name>.+)$",
        center_node_id.strip(),
    )
    if not m:
        return (
            False,
            "Invalid center_node_id format; expected language::module::type::name",
        )

    lang = m.group("lang")
    module = m.group("module")
    kind = m.group("kind")
    name = m.group("name")

    if lang != "python" or kind != "function":
        return True, None

    if module in ("external", "unknown"):
        return False, f"Center node module '{module}' is not a local module"

    # Map module -> file path
    candidate = root_path / (module.replace(".", "/") + ".py")
    if not candidate.exists():
        return False, f"Center node file not found for module '{module}': {candidate}"

    # Quick AST scan for a matching function name in that single file.
    try:
        import ast

        code = candidate.read_text(encoding="utf-8")
        tree = ast.parse(code)
        for node in ast.walk(tree):
            if (
                isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef))
                and node.name == name
            ):
                return True, None
        return False, f"Center node function '{name}' not found in {candidate}"
    except Exception:
        # If parsing fails, fall back to the slow path (graph build)
        return True, None


@mcp.tool()
async def get_graph_neighborhood(
    center_node_id: str,
    k: int = 2,
    max_nodes: int = 100,
    direction: str = "both",
    min_confidence: float = 0.0,
    project_root: str | None = None,
    query: str | None = None,
) -> GraphNeighborhoodResult:
    """
    Extract k-hop neighborhood subgraph around a center node.

    **Tier Behavior:**
    - All tiers: Tool is available.
    - Limits and optional enhancements are applied based on tool capabilities.

    [v2.5.0] Use this tool to prevent graph explosion when analyzing large
    codebases. Instead of loading the entire graph, extract only the nodes
    within k hops of a specific node.

    [v3.2.8] Tier-based hop limiting for Community tier.
    [v3.5.0] Enterprise: Graph query language support.

    **Graph Pruning Formula:** N(v, k) = {u  V : d(v, u)  k}

    This extracts all nodes u where the shortest path from center v to u
    is at most k hops.

    **Truncation Protection:**
    If the neighborhood exceeds max_nodes, the graph is truncated and
    a warning is returned. This prevents memory exhaustion on dense graphs.

    Key capabilities:
    - Extract focused subgraph around any node
    - Control traversal depth with k parameter
    - Limit graph size with max_nodes
    - Filter by edge direction (incoming, outgoing, both)
    - Filter by minimum confidence score
    - Generate Mermaid visualization
    - Pro: Semantic neighbors and logical relationships
    - Enterprise: Query language for custom graph traversals

    Why AI agents need this:
    - **Focused Analysis:** Analyze only relevant code, not entire codebase
    - **Memory Safety:** Prevent OOM on large graphs
    - **Honest Uncertainty:** Know when graph is incomplete

    Example:
        # Get 2-hop neighborhood around a function
        result = get_graph_neighborhood(
            center_node_id="python::services::function::process_order",
            k=2,
            max_nodes=50
        )
        if result.truncated:
            print(f"Warning: {result.truncation_warning}")

        # Enterprise: Use query language
        result = get_graph_neighborhood(
            center_node_id="python::controllers::function::handle_request",
            query="MATCH (n)-[:calls]->(m:function) WHERE m.name CONTAINS 'DB' RETURN n, m"
        )

    Args:
        center_node_id: ID of the center node (format: language::module::type::name)
        k: Maximum hops from center (default: 2)
        max_nodes: Maximum nodes to include (default: 100)
        direction: "outgoing", "incoming", or "both" (default: "both")
        min_confidence: Minimum edge confidence to follow (default: 0.0)
        project_root: Project root directory (default: server's project root)
        query: Graph query language (Enterprise tier only)

    Returns:
        GraphNeighborhoodResult with subgraph, truncation info, and Mermaid diagram
    """
    from code_scalpel.graph_engine import UniversalGraph

    root_path = Path(project_root) if project_root else PROJECT_ROOT

    if not root_path.exists():
        return GraphNeighborhoodResult(
            success=False,
            error=f"Project root not found: {root_path}.",
        )

    # [20251225_FEATURE] Capability-driven tier behavior (no upgrade hints)
    tier = _get_current_tier()
    caps = get_tool_capabilities("get_graph_neighborhood", tier) or {}
    limits = caps.get("limits", {}) or {}
    cap_list = caps.get("capabilities", []) or []
    cap_set = set(cap_list) if not isinstance(cap_list, set) else cap_list

    # Enterprise capability flags (returned even when empty)
    query_supported = bool("graph_query_language" in cap_set)
    traversal_rules_available = bool("custom_traversal_rules" in cap_set)
    path_constraints_supported = bool("path_constraint_queries" in cap_set)

    # [20251226_BUGFIX] Support both legacy and current limit keys.
    max_k_hops = limits.get("max_k_hops", limits.get("max_k"))
    max_nodes_limit = limits.get("max_nodes")

    actual_k = k
    k_limited = False
    if max_k_hops is not None and k > max_k_hops:
        actual_k = int(max_k_hops)
        k_limited = True
        k = actual_k

    if max_nodes_limit is not None and max_nodes > max_nodes_limit:
        max_nodes = int(max_nodes_limit)

    # Validate parameters
    if k < 1:
        return GraphNeighborhoodResult(
            success=False,
            error="Parameter 'k' must be at least 1.",
        )

    if max_nodes < 1:
        return GraphNeighborhoodResult(
            success=False,
            error="Parameter 'max_nodes' must be at least 1.",
        )

    if direction not in ("outgoing", "incoming", "both"):
        return GraphNeighborhoodResult(
            success=False,
            error=f"Parameter 'direction' must be 'outgoing', 'incoming', or 'both', got '{direction}'.",
        )

    try:
        center_node_id = _normalize_graph_center_node_id(center_node_id)

        ok, fast_err = _fast_validate_python_function_node_exists(
            root_path, center_node_id
        )
        if not ok:
            return GraphNeighborhoodResult(
                success=False,
                error=fast_err or "Center node not found",
            )

        # [v3.0.4] Try to use cached graph first
        from code_scalpel.graph_engine import UniversalGraph

        advanced_resolution = bool(
            {
                "advanced_neighborhood",
                "semantic_neighbors",
                "logical_relationship_detection",
                "polymorphism_resolution",
                "virtual_call_tracking",
            }
            & cap_set
        )

        include_enterprise_metrics = bool(
            {
                "custom_traversal",
                "graph_query_language",
                "custom_traversal_rules",
                "path_constraint_queries",
            }
            & cap_set
        )

        cache_variant = "advanced" if advanced_resolution else "basic"
        graph = _get_cached_graph(root_path, cache_variant=cache_variant)

        if graph is None:
            # Cache miss - build the graph
            from code_scalpel.ast_tools.call_graph import CallGraphBuilder

            builder = CallGraphBuilder(root_path)
            call_graph_result = builder.build_with_details(
                entry_point=None,
                depth=10,
                max_nodes=None,
                advanced_resolution=advanced_resolution,
            )

            # Convert call graph to UniversalGraph
            from code_scalpel.graph_engine import (
                EdgeType,
                GraphEdge,
                GraphNode,
                NodeType,
                UniversalNodeID,
            )

            graph = UniversalGraph()

            # Add nodes
            for node in call_graph_result.nodes:
                node_id = UniversalNodeID(
                    language="python",
                    module=(
                        node.file.replace("/", ".").replace(".py", "")
                        if node.file != "<external>"
                        else "external"
                    ),
                    node_type=NodeType.FUNCTION,
                    name=node.name,
                    line=node.line,
                )
                graph.add_node(
                    GraphNode(
                        id=node_id,
                        metadata={
                            "file": node.file,
                            "line": node.line,
                            "is_entry_point": node.is_entry_point,
                        },
                    )
                )

            # Add edges
            for edge in call_graph_result.edges:
                # Parse caller/callee into node IDs
                caller_parts = edge.caller.split(":")
                callee_parts = edge.callee.split(":")

                caller_file = caller_parts[0] if len(caller_parts) > 1 else ""
                caller_name = caller_parts[-1]
                callee_file = callee_parts[0] if len(callee_parts) > 1 else ""
                callee_name = callee_parts[-1]

                caller_module = (
                    caller_file.replace("/", ".").replace(".py", "")
                    if caller_file
                    else "unknown"
                )
                callee_module = (
                    callee_file.replace("/", ".").replace(".py", "")
                    if callee_file
                    else "external"
                )

                caller_id = f"python::{caller_module}::function::{caller_name}"
                callee_id = f"python::{callee_module}::function::{callee_name}"

                graph.add_edge(
                    GraphEdge(
                        from_id=caller_id,
                        to_id=callee_id,
                        edge_type=EdgeType.DIRECT_CALL,
                        confidence=0.9,
                        evidence="Direct function call",
                    )
                )

            # Cache the built graph for subsequent calls
            _cache_graph(root_path, graph, cache_variant=cache_variant)

        # [20251229_FEATURE] Enterprise tier: Query language support
        if query and "graph_query_language" in cap_set:
            from code_scalpel.graph.graph_query import GraphQueryEngine
            from code_scalpel.graph_engine import NeighborhoodResult, UniversalGraph

            try:
                engine = GraphQueryEngine(graph)
                query_result = engine.execute(query)

                if not query_result.success:
                    return GraphNeighborhoodResult(
                        success=False,
                        error=f"Query execution failed: {query_result.error}",
                    )

                # Build a subgraph from query results (bounded by max_nodes)
                subgraph = UniversalGraph()
                included_node_ids: set[str] = set()

                for node_data in query_result.nodes:
                    if len(included_node_ids) >= max_nodes:
                        break
                    node_id = str(node_data.get("id", "") or "").strip()
                    if not node_id:
                        continue
                    original_node = graph.get_node(node_id)
                    if original_node:
                        subgraph.add_node(original_node)
                        included_node_ids.add(node_id)

                # Add edges from query result (only if both endpoints are included)
                for edge_data in query_result.edges:
                    from_id = str(edge_data.get("from_id", "") or "").strip()
                    to_id = str(edge_data.get("to_id", "") or "").strip()
                    if not from_id or not to_id:
                        continue
                    if (
                        from_id not in included_node_ids
                        or to_id not in included_node_ids
                    ):
                        continue
                    # Find original edge in the graph
                    for edge in graph.edges:
                        if edge.from_id == from_id and edge.to_id == to_id:
                            subgraph.add_edge(edge)
                            break

                node_depths = {node_id: 0 for node_id in included_node_ids}
                truncated = len(query_result.nodes) > max_nodes

                result = NeighborhoodResult(
                    success=True,
                    subgraph=subgraph,
                    center_node_id=center_node_id,
                    k=0,
                    total_nodes=len(subgraph.nodes),
                    total_edges=len(subgraph.edges),
                    max_depth_reached=0,
                    truncated=truncated,
                    truncation_warning=(
                        f"Query result truncated at {max_nodes} nodes due to max_nodes limit."
                        if truncated
                        else None
                    ),
                    node_depths=node_depths,
                )

            except Exception as e:
                return GraphNeighborhoodResult(
                    success=False,
                    error=f"Query language error: {str(e)}",
                )
        else:
            # Standard k-hop extraction
            result = graph.get_neighborhood(
                center_node_id=center_node_id,
                k=k,
                max_nodes=max_nodes,
                direction=direction,
                min_confidence=min_confidence,
            )

        if not result.success:
            return GraphNeighborhoodResult(
                success=False,
                error=result.error,
            )

        # [20251229_FEATURE] Pro tier: Add semantic neighbors
        semantic_neighbor_ids: Set[str] = set()
        if "semantic_neighbors" in cap_set:
            from code_scalpel.graph.semantic_neighbors import SemanticNeighborFinder

            try:
                # Extract center function name from node_id
                center_name = center_node_id.split("::")[-1]
                finder = SemanticNeighborFinder(root_path)
                semantic_result = finder.find_semantic_neighbors(
                    center_name=center_name,
                    k=min(10, max_nodes // 2),
                    min_similarity=0.3,
                )

                if semantic_result.success:
                    for neighbor in semantic_result.neighbors:
                        # Add semantic neighbor to the graph if not already present
                        if neighbor.node_id not in result.node_depths:
                            semantic_neighbor_ids.add(neighbor.node_id)
                            # Add to result's node_depths at depth k+1 (beyond normal k-hop)
                            result.node_depths[neighbor.node_id] = k + 1

                            # Add edge from center to semantic neighbor
                            if result.subgraph:
                                from code_scalpel.graph_engine import (
                                    EdgeType,
                                    GraphEdge,
                                )

                                result.subgraph.add_edge(
                                    GraphEdge(
                                        from_id=center_node_id,
                                        to_id=neighbor.node_id,
                                        edge_type=EdgeType.SEMANTIC_SIMILAR,
                                        confidence=neighbor.similarity_score,
                                        evidence=f"Semantic: {', '.join(neighbor.relationship_types)}",
                                    )
                                )
            except Exception:
                # Semantic neighbor discovery is best-effort, don't fail the whole query
                pass

        # [20251229_FEATURE] Pro tier: Add logical relationships
        if "logical_relationship_detection" in cap_set:
            from code_scalpel.graph.logical_relationships import (
                LogicalRelationshipDetector,
            )

            try:
                center_name = center_node_id.split("::")[-1]
                detector = LogicalRelationshipDetector(root_path)
                relationship_result = detector.find_relationships(
                    center_name=center_name, max_relationships=20
                )

                if relationship_result.success:
                    for rel in relationship_result.relationships:
                        # Add logical relationship as an edge
                        if result.subgraph and rel.source_node in result.node_depths:
                            from code_scalpel.graph_engine import EdgeType, GraphEdge

                            # Ensure target node exists in the graph
                            if rel.target_node not in result.node_depths:
                                result.node_depths[rel.target_node] = k + 1

                            result.subgraph.add_edge(
                                GraphEdge(
                                    from_id=rel.source_node,
                                    to_id=rel.target_node,
                                    edge_type=EdgeType.LOGICAL_RELATED,
                                    confidence=rel.confidence,
                                    evidence=f"Logical: {rel.relationship_type} - {rel.evidence}",
                                )
                            )
            except Exception:
                # Logical relationship detection is best-effort, don't fail the whole query
                pass

        # Convert to response models
        nodes = []
        for node_id, depth in result.node_depths.items():
            node = result.subgraph.get_node(node_id) if result.subgraph else None
            nodes.append(
                NeighborhoodNodeModel(
                    id=node_id,
                    depth=depth,
                    metadata=node.metadata if node else {},
                )
            )

        edges = []
        if result.subgraph:
            for edge in result.subgraph.edges:
                edges.append(
                    NeighborhoodEdgeModel(
                        from_id=edge.from_id,
                        to_id=edge.to_id,
                        edge_type=edge.edge_type.value,
                        confidence=edge.confidence,
                    )
                )

        # Generate Mermaid diagram
        mermaid = _generate_neighborhood_mermaid(nodes, edges, center_node_id)

        hot_nodes: list[str] = []
        if include_enterprise_metrics:
            # [20251225_FEATURE] Best-effort degree metrics within the returned subgraph.
            in_deg: dict[str, int] = {}
            out_deg: dict[str, int] = {}
            for edge in edges:
                out_deg[edge.from_id] = out_deg.get(edge.from_id, 0) + 1
                in_deg[edge.to_id] = in_deg.get(edge.to_id, 0) + 1

            for n in nodes:
                n.in_degree = in_deg.get(n.id, 0)
                n.out_degree = out_deg.get(n.id, 0)

            def _degree_key(n: NeighborhoodNodeModel) -> tuple[int, str]:
                return (((n.in_degree or 0) + (n.out_degree or 0)), n.id)

            hot_nodes = [
                n.id
                for n in sorted(nodes, key=_degree_key, reverse=True)[:10]
                if _degree_key(n)[0] > 0
            ]

        return GraphNeighborhoodResult(
            success=True,
            center_node_id=center_node_id,
            k=actual_k,
            nodes=nodes,
            edges=edges,
            total_nodes=result.total_nodes,
            total_edges=result.total_edges,
            max_depth_reached=result.max_depth_reached,
            truncated=result.truncated or k_limited,
            truncation_warning=result.truncation_warning,
            mermaid=mermaid,
            hot_nodes=hot_nodes,
            query_supported=query_supported,
            traversal_rules_available=traversal_rules_available,
            path_constraints_supported=path_constraints_supported,
        )

    except Exception as e:
        return GraphNeighborhoodResult(
            success=False,
            error=f"Graph neighborhood extraction failed: {str(e)}",
        )


# ============================================================================
# [20251213_FEATURE] v1.5.0 - get_project_map MCP Tool
# ============================================================================


class ModuleInfo(BaseModel):
    """Information about a Python module/file."""

    path: str = Field(description="Relative file path")
    functions: list[str] = Field(
        default_factory=list, description="Function names in the module"
    )
    classes: list[str] = Field(
        default_factory=list, description="Class names in the module"
    )
    imports: list[str] = Field(default_factory=list, description="Import statements")
    entry_points: list[str] = Field(
        default_factory=list, description="Detected entry points"
    )
    line_count: int = Field(default=0, description="Number of lines in file")
    complexity_score: int = Field(default=0, description="Cyclomatic complexity score")


class PackageInfo(BaseModel):
    """Information about a Python package (directory with __init__.py)."""

    name: str = Field(description="Package name")
    path: str = Field(description="Relative path to package")
    modules: list[str] = Field(
        default_factory=list, description="Module names in package"
    )
    subpackages: list[str] = Field(default_factory=list, description="Subpackage names")


class ProjectMapResult(BaseModel):
    """Result of project map analysis."""

    # [20251226_BUGFIX] Ensure tier-gated extras are accessible when schema differs
    try:
        from pydantic import ConfigDict as _ConfigDict  # type: ignore

        model_config = _ConfigDict(extra="allow")
    except Exception:

        class Config:  # type: ignore
            extra = "allow"

    success: bool = Field(default=True, description="Whether analysis succeeded")
    server_version: str = Field(default=__version__, description="Code Scalpel version")
    project_root: str = Field(description="Absolute path to project root")
    total_files: int = Field(default=0, description="Total Python files")
    total_lines: int = Field(default=0, description="Total lines of code")
    languages: dict[str, int] = Field(
        default_factory=dict, description="Language breakdown by file count"
    )
    packages: list[PackageInfo] = Field(
        default_factory=list, description="Detected packages"
    )
    modules: list[ModuleInfo] = Field(
        default_factory=list, description="Modules analyzed (max 50 in Mermaid diagram)"
    )
    entry_points: list[str] = Field(
        default_factory=list, description="All detected entry points"
    )
    circular_imports: list[list[str]] = Field(
        default_factory=list, description="Circular import cycles"
    )
    complexity_hotspots: list[str] = Field(
        default_factory=list, description="Files with high complexity"
    )
    mermaid: str = Field(default="", description="Mermaid diagram of package structure")

    # [20260111_FEATURE] Output metadata fields for tier transparency
    tier_applied: str = Field(
        default="community",
        description="The tier that was applied to this request (community, pro, enterprise)",
    )
    max_files_applied: int | None = Field(
        default=None,
        description="The max_files limit applied (100 for Community, 1000 for Pro, None for Enterprise)",
    )
    max_modules_applied: int | None = Field(
        default=None,
        description="The max_modules limit applied (50 for Community, 200 for Pro, 1000 for Enterprise)",
    )
    pro_features_enabled: bool = Field(
        default=False,
        description="Whether Pro tier features were enabled (coupling_metrics, architectural_layers, etc.)",
    )
    enterprise_features_enabled: bool = Field(
        default=False,
        description="Whether Enterprise tier features were enabled (city_map, force_graph, etc.)",
    )

    # [20251226_BUGFIX] Provide tier-gated attributes via properties when model schema differs
    def _get_extra_value(self, name: str):  # type: ignore[override]
        import subprocess

        try:
            extra = object.__getattribute__(self, "__pydantic_extra__")
            if extra:
                return extra.get(name)
        except Exception:
            return None
        return None

    @property
    def module_relationships(self):  # type: ignore[override]
        return self._get_extra_value("module_relationships")

    @property
    def architectural_layers(self):  # type: ignore[override]
        return self._get_extra_value("architectural_layers")

    @property
    def coupling_metrics(self):  # type: ignore[override]
        return self._get_extra_value("coupling_metrics")

    @property
    def dependency_diagram(self):  # type: ignore[override]
        return self._get_extra_value("dependency_diagram")

    @property
    def city_map_data(self):  # type: ignore[override]
        return self._get_extra_value("city_map_data")

    @property
    def force_graph(self):  # type: ignore[override]
        return self._get_extra_value("force_graph")

    @property
    def churn_heatmap(self):  # type: ignore[override]
        return self._get_extra_value("churn_heatmap")

    @property
    def bug_hotspots(self):  # type: ignore[override]
        return self._get_extra_value("bug_hotspots")

    @property
    def git_ownership(self):  # type: ignore[override]
        return self._get_extra_value("git_ownership")

    # [20251231_FEATURE] v3.3.1 - Property getters for new Enterprise fields
    @property
    def multi_repo_summary(self):  # type: ignore[override]
        return self._get_extra_value("multi_repo_summary")

    @property
    def historical_trends(self):  # type: ignore[override]
        return self._get_extra_value("historical_trends")

    @property
    def custom_metrics(self):  # type: ignore[override]
        return self._get_extra_value("custom_metrics")

    @property
    def compliance_overlay(self):  # type: ignore[override]
        return self._get_extra_value("compliance_overlay")

    # [20251226_FEATURE] Tier-aware rich metadata fields
    # [20260102_REFACTOR] Pydantic Fields intentionally mirror property names; silence F811 redefinition.
    module_relationships: list[dict[str, Any]] | None = Field(  # noqa: F811
        default=None,
        description="Import relationship edges between modules (Pro/Enterprise)",
    )
    architectural_layers: list[dict[str, str]] | None = Field(  # noqa: F811
        default=None,
        description="Layer detection results per module (Pro/Enterprise)",
    )
    coupling_metrics: list[dict[str, Any]] | None = Field(  # noqa: F811
        default=None,
        description="Coupling metrics (afferent/efferent/instability) per module",
    )
    dependency_diagram: str | None = Field(  # noqa: F811
        default=None,
        description="Mermaid diagram of dependency graph when enabled",
    )
    city_map_data: dict[str, Any] | None = Field(  # noqa: F811
        default=None,
        description="Abstract city-map payload for Enterprise visualization",
    )
    force_graph: dict[str, Any] | None = Field(  # noqa: F811
        default=None,
        description="Force-directed graph payload for Enterprise",
    )
    churn_heatmap: list[dict[str, Any]] | None = Field(  # noqa: F811
        default=None,
        description="Code churn summary for Enterprise",
    )
    bug_hotspots: list[dict[str, Any]] | None = Field(  # noqa: F811
        default=None,
        description="Bug hotspot heuristics for Enterprise",
    )
    git_ownership: list[dict[str, Any]] | None = Field(  # noqa: F811
        default=None,
        description="Lightweight ownership attribution (Pro/Enterprise)",
    )
    # [20251231_FEATURE] v3.3.1 - New Enterprise fields per roadmap v1.0
    multi_repo_summary: dict[str, Any] | None = Field(  # noqa: F811
        default=None,
        description="Multi-repository aggregation summary (Enterprise)",
    )
    historical_trends: list[dict[str, Any]] | None = Field(  # noqa: F811
        default=None,
        description="Historical architecture trends from git log analysis (Enterprise)",
    )
    custom_metrics: dict[str, Any] | None = Field(  # noqa: F811
        default=None,
        description="Custom map metrics defined in configuration (Enterprise)",
    )
    compliance_overlay: dict[str, Any] | None = Field(  # noqa: F811
        default=None,
        description="Compliance/architecture rule violations overlay (Enterprise)",
    )
    # [20251220_FEATURE] v3.0.5 - Truncation communication
    modules_in_diagram: int = Field(
        default=0, description="Number of modules shown in Mermaid diagram"
    )
    diagram_truncated: bool = Field(
        default=False, description="Whether Mermaid diagram was truncated"
    )
    error: str | None = Field(default=None, description="Error message if failed")


def _get_project_map_sync(
    project_root: str | None,
    include_complexity: bool,
    complexity_threshold: int,
    include_circular_check: bool,
    *,
    tier: str | None = None,
    capabilities: dict | None = None,
    max_files_limit: int | None = None,
    max_modules_limit: int | None = None,
) -> ProjectMapResult:
    import subprocess
    """Synchronous implementation of get_project_map.

    The function is tier-aware; limits and capabilities are computed in the async wrapper
    and passed through to avoid re-resolving tier in worker threads.
    """
    import ast

    from code_scalpel.ast_tools.call_graph import CallGraphBuilder

    root_path = Path(project_root) if project_root else PROJECT_ROOT

    # [20250112_FIX] Resolve tier before try block so it's available in except
    tier = tier or _get_current_tier()

    if not root_path.exists():
        return ProjectMapResult(
            success=False,
            project_root=str(root_path),
            error=f"Project root not found: {root_path}.",
            tier_applied=tier,
            pro_features_enabled=tier in ("pro", "enterprise"),
            enterprise_features_enabled=tier == "enterprise",
        )

    try:
        caps = capabilities or get_tool_capabilities("get_project_map", tier) or {}
        caps_set = set(caps.get("capabilities", set()) or [])
        limits = caps.get("limits", {}) or {}

        effective_max_files = max_files_limit
        if effective_max_files is None:
            effective_max_files = limits.get("max_files")

        effective_max_modules = max_modules_limit
        if effective_max_modules is None:
            effective_max_modules = limits.get("max_modules")

        if effective_max_modules is None:
            effective_max_modules = 50

        modules: list[ModuleInfo] = []
        packages: dict[str, PackageInfo] = {}
        all_entry_points: list[str] = []
        complexity_hotspots: list[str] = []
        total_lines = 0

        # Entry point detection patterns
        entry_decorators = {
            "command",
            "main",
            "cli",
            "app",
            "route",
            "get",
            "post",
            "put",
            "delete",
        }

        def is_entry_point(func_node: ast.AST) -> bool:
            """Check if function is an entry point."""
            # Type guard: must be FunctionDef or AsyncFunctionDef
            if not isinstance(func_node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                return False
            if func_node.name == "main":
                return True
            for dec in getattr(func_node, "decorator_list", []):
                dec_name = ""
                if isinstance(dec, ast.Name):
                    dec_name = dec.id
                elif isinstance(dec, ast.Attribute):
                    dec_name = dec.attr
                elif isinstance(dec, ast.Call):
                    if isinstance(dec.func, ast.Attribute):
                        dec_name = dec.func.attr
                    elif isinstance(dec.func, ast.Name):
                        dec_name = dec.func.id
                if dec_name in entry_decorators:
                    return True
            return False

        def calculate_complexity(tree: ast.AST) -> int:
            """Calculate cyclomatic complexity of a module."""
            complexity = 1  # Base complexity
            for node in ast.walk(tree):
                if isinstance(
                    node,
                    (
                        ast.If,
                        ast.While,
                        ast.For,
                        ast.AsyncFor,
                        ast.ExceptHandler,
                        ast.With,
                        ast.AsyncWith,
                        ast.Assert,
                        ast.comprehension,
                    ),
                ):
                    complexity += 1
                elif isinstance(node, (ast.And, ast.Or)):
                    complexity += 1
                elif isinstance(node, ast.BoolOp):
                    complexity += len(node.values) - 1
            return complexity

        # Collect all Python files
        python_files = list(root_path.rglob("*.py"))

        # [20251229_BUGFIX] Filter exclusions BEFORE applying file limit
        # Previously: files were sorted/limited first, then filtered, causing
        # .venv files to dominate the limited set and then be filtered out,
        # resulting in zero modules detected.
        exclude_patterns = {
            "__pycache__",
            ".git",
            "venv",
            ".venv",
            "env",
            ".env",
            "node_modules",
            "dist",
            "build",
            ".tox",
            ".pytest_cache",
            "htmlcov",
            ".mypy_cache",
        }

        # Filter out excluded directories FIRST
        # Check both exact matches and startswith for patterns like .venv-*
        def should_exclude(file_path: Path) -> bool:
            for part in file_path.parts:
                # Exact match
                if part in exclude_patterns:
                    return True
                # Startswith match for patterns like .venv-mcp-smoke
                for pattern in exclude_patterns:
                    if part.startswith(pattern):
                        return True
            return False

        python_files = [f for f in python_files if not should_exclude(f)]

        # [20251226_FEATURE] Tier-aware file cap - AFTER filtering
        python_files = sorted(python_files)
        if effective_max_files is not None and len(python_files) > effective_max_files:
            python_files = python_files[:effective_max_files]

        for file_path in python_files:
            rel_path = str(file_path.relative_to(root_path))

            try:
                with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                    code = f.read()

                lines = code.count("\n") + 1
                total_lines += lines

                tree = ast.parse(code)

                # Extract module info
                functions = []
                classes = []
                imports = []
                entry_points = []

                for node in ast.walk(tree):
                    if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                        functions.append(node.name)
                        if is_entry_point(node):
                            entry_points.append(f"{rel_path}:{node.name}")
                    elif isinstance(node, ast.ClassDef):
                        classes.append(node.name)
                    elif isinstance(node, ast.Import):
                        for alias in node.names:
                            imports.append(alias.name)
                    elif isinstance(node, ast.ImportFrom):
                        if node.module:
                            imports.append(node.module)

                # Calculate complexity if requested
                complexity = 0
                if include_complexity:
                    complexity = calculate_complexity(tree)
                    if complexity >= complexity_threshold:
                        complexity_hotspots.append(
                            f"{rel_path} (complexity: {complexity})"
                        )

                all_entry_points.extend(entry_points)

                modules.append(
                    ModuleInfo(
                        path=rel_path,
                        functions=functions,
                        classes=classes,
                        imports=list(set(imports)),  # Dedupe
                        entry_points=entry_points,
                        line_count=lines,
                        complexity_score=complexity,
                    )
                )

                # Track packages
                parent = file_path.parent
                while parent != root_path and parent.exists():
                    init_file = parent / "__init__.py"
                    if init_file.exists():
                        pkg_path = str(parent.relative_to(root_path))
                        pkg_name = parent.name
                        if pkg_path not in packages:
                            packages[pkg_path] = PackageInfo(
                                name=pkg_name,
                                path=pkg_path,
                                modules=[],
                                subpackages=[],
                            )
                        # Add module to package
                        if rel_path not in packages[pkg_path].modules:
                            packages[pkg_path].modules.append(rel_path)
                    parent = parent.parent

            except Exception:
                # Skip files with errors
                continue

        # Organize package hierarchy
        pkg_list = list(packages.values())
        for pkg in pkg_list:
            for other_pkg in pkg_list:
                if (
                    other_pkg.path.startswith(pkg.path + "/")
                    and other_pkg.name not in pkg.subpackages
                ):
                    pkg.subpackages.append(other_pkg.name)

        # Check for circular imports
        circular_imports = []
        if include_circular_check:
            builder = CallGraphBuilder(root_path)
            circular_imports = builder.detect_circular_imports()

        # [20251213_FEATURE] Calculate language breakdown
        languages: dict[str, int] = {"python": len(modules)}
        # Also count other common file types
        for ext, lang in [
            (".js", "javascript"),
            (".ts", "typescript"),
            (".java", "java"),
            (".json", "json"),
            (".yaml", "yaml"),
            (".yml", "yaml"),
            (".md", "markdown"),
            (".html", "html"),
            (".css", "css"),
        ]:
            len(list(root_path.rglob(f"*{ext}")))
            # Exclude common ignored dirs
            actual_count = sum(
                1
                for f in root_path.rglob(f"*{ext}")
                if not any(p in exclude_patterns for p in f.parts)
            )
            if actual_count > 0:
                languages[lang] = languages.get(lang, 0) + actual_count

        modules_in_diagram = (
            len(modules)
            if effective_max_modules is None
            else min(len(modules), int(effective_max_modules))
        )
        diagram_limit = modules_in_diagram

        # [20251226_FEATURE] Tier-aware relationship + analytics construction
        dotted_to_path: dict[str, str] = {}
        for mod in modules:
            dotted = mod.path[:-3] if mod.path.endswith(".py") else mod.path
            dotted = dotted.replace("/", ".")
            dotted_to_path[dotted] = mod.path

        def resolve_import_target(import_name: str) -> str | None:
            """Map an import string to a known module path if possible."""
            if import_name in dotted_to_path:
                return dotted_to_path[import_name]
            for dotted_key, path_value in dotted_to_path.items():
                if import_name.startswith(dotted_key):
                    return path_value
            return None

        module_relationships: list[dict[str, str]] | None = None
        dependency_diagram: str | None = None
        architectural_layers: list[dict[str, str]] | None = None
        coupling_metrics: list[dict[str, Any]] | None = None
        force_graph: dict[str, Any] | None = None
        city_map_data: dict[str, Any] | None = None
        churn_heatmap: list[dict[str, Any]] | None = None
        bug_hotspots: list[dict[str, Any]] | None = None
        git_ownership: list[dict[str, Any]] | None = None
        # [20251231_FEATURE] v3.3.1 - New Enterprise feature variables
        multi_repo_summary: dict[str, Any] | None = None
        historical_trends: list[dict[str, Any]] | None = None
        custom_metrics: dict[str, Any] | None = None
        compliance_overlay: dict[str, Any] | None = None

        # [20251226_BUGFIX] Align capability flags with tier when feature map differs
        enable_relationships = (
            "module_relationship_visualization" in caps_set
            or "dependency_tracking" in caps_set
            or (tier and tier.lower() in {"pro", "enterprise"})
        )
        enable_dependency_diagram = "import_dependency_diagram" in caps_set or (
            tier and tier.lower() in {"pro", "enterprise"}
        )
        enable_layers = "architectural_layer_detection" in caps_set or (
            tier and tier.lower() in {"pro", "enterprise"}
        )
        enable_coupling = "coupling_analysis" in caps_set or (
            tier and tier.lower() in {"pro", "enterprise"}
        )
        enable_force_graph = "force_directed_graph" in caps_set or (
            tier and tier.lower() == "enterprise"
        )
        enable_city = "interactive_city_map" in caps_set or (
            tier and tier.lower() == "enterprise"
        )
        enable_churn = "code_churn_visualization" in caps_set or (
            tier and tier.lower() == "enterprise"
        )
        enable_bug_hotspots = "bug_hotspot_heatmap" in caps_set or (
            tier and tier.lower() == "enterprise"
        )
        enable_git_blame = "git_blame_integration" in caps_set or (
            tier and tier.lower() in {"pro", "enterprise"}
        )
        # [20251231_FEATURE] v3.3.1 - New Enterprise feature flags
        enable_multi_repo = "multi_repository_maps" in caps_set or (
            tier and tier.lower() == "enterprise"
        )
        enable_historical_trends = "historical_architecture_trends" in caps_set or (
            tier and tier.lower() == "enterprise"
        )
        enable_custom_metrics = "custom_map_metrics" in caps_set or (
            tier and tier.lower() == "enterprise"
        )
        enable_compliance_overlay = "compliance_overlay" in caps_set or (
            tier and tier.lower() == "enterprise"
        )

        edges: list[tuple[str, str]] = []
        if (
            enable_relationships
            or enable_dependency_diagram
            or enable_coupling
            or enable_layers
            or enable_force_graph
            or enable_city
        ):
            for mod in modules:
                for imp in mod.imports:
                    target_path = resolve_import_target(imp)
                    if target_path:
                        edges.append((mod.path, target_path))

        if enable_relationships:
            module_relationships = [
                {"source": src, "target": dst, "type": "import"} for src, dst in edges
            ]

        if enable_dependency_diagram and edges:
            diagram_lines = ["graph TD"]
            for idx, mod in enumerate(modules[:modules_in_diagram]):
                node_id = f"N{idx}"
                label = mod.path.replace("/", "_").replace(".", "_")
                diagram_lines.append(f'    {node_id}["{label}"]')
            path_to_id = {
                mod.path: f"N{idx}"
                for idx, mod in enumerate(modules[:modules_in_diagram])
            }
            for src, dst in edges:
                if src in path_to_id and dst in path_to_id:
                    diagram_lines.append(f"    {path_to_id[src]} --> {path_to_id[dst]}")
            dependency_diagram = "\n".join(diagram_lines)

        if enable_layers:

            def classify_layer(path: str) -> tuple[str, str]:
                lowered = path.lower()
                if any(k in lowered for k in ["controller", "view", "handler", "api"]):
                    return "controller", "Matched controller/view keywords"
                if any(k in lowered for k in ["service", "logic", "manager"]):
                    return "service", "Matched service/logic keywords"
                if any(
                    k in lowered for k in ["repo", "repository", "model", "dao", "db"]
                ):
                    return "repository", "Matched repository/model keywords"
                if any(k in lowered for k in ["util", "helper", "common", "shared"]):
                    return "utility", "Matched utility keywords"
                return "other", "No heuristic match"

            architectural_layers = []
            for mod in modules:
                layer, reason = classify_layer(mod.path)
                architectural_layers.append(
                    {"module": mod.path, "layer": layer, "reason": reason}
                )

        if enable_coupling:
            outgoing: dict[str, set[str]] = {mod.path: set() for mod in modules}
            incoming: dict[str, set[str]] = {mod.path: set() for mod in modules}
            for src, dst in edges:
                if src in outgoing:
                    outgoing[src].add(dst)
                if dst in incoming:
                    incoming[dst].add(src)

            coupling_metrics = []
            for mod in modules:
                ca = len(incoming.get(mod.path, set()))
                ce = len(outgoing.get(mod.path, set()))
                denom = ca + ce
                instability = ce / denom if denom else 0.0
                coupling_metrics.append(
                    {
                        "module": mod.path,
                        "afferent": ca,
                        "efferent": ce,
                        "instability": round(instability, 3),
                    }
                )

        if enable_force_graph and edges:
            force_graph = {
                "nodes": [
                    {
                        "id": mod.path,
                        "group": (
                            (architectural_layers or [{}])[idx].get("layer", "other")
                            if architectural_layers
                            else "other"
                        ),
                    }
                    for idx, mod in enumerate(modules)
                ],
                "links": [
                    {"source": src, "target": dst, "value": 1} for src, dst in edges
                ],
            }

        if enable_city:
            city_map_data = {
                "buildings": [
                    {
                        "id": mod.path,
                        "height": max(mod.complexity_score, 1),
                        "footprint": max(mod.line_count // 10, 1),
                        "layer": next(
                            (
                                layer_info["layer"]
                                for layer_info in (architectural_layers or [])
                                if layer_info.get("module") == mod.path
                            ),
                            "other",
                        ),
                    }
                    for mod in modules
                ]
            }

        if enable_churn:
            churn_heatmap = []
            for mod in modules:
                churn_score = mod.complexity_score + len(mod.imports)
                level = "low"
                if churn_score > 20:
                    level = "high"
                elif churn_score > 10:
                    level = "medium"
                churn_heatmap.append(
                    {"module": mod.path, "churn_score": churn_score, "level": level}
                )

        if enable_bug_hotspots:
            bug_hotspots = []
            for mod in modules:
                if mod.complexity_score >= complexity_threshold or len(mod.imports) > 5:
                    bug_hotspots.append(
                        {
                            "module": mod.path,
                            "reason": "High complexity or import fan-in",
                        }
                    )
            if not bug_hotspots:
                bug_hotspots = [
                    {"module": mod.path, "reason": "No hotspots detected"}
                    for mod in modules[:1]
                ]

        if enable_git_blame:
            # [20251229_FEATURE] Enterprise: Git blame integration
            git_ownership = []
            try:
                import subprocess

                # Check if we're in a git repository
                result = subprocess.run(
                    ["git", "rev-parse", "--is-inside-work-tree"],
                    cwd=root_path,
                    capture_output=True,
                    text=True,
                    timeout=5,
                )

                if result.returncode == 0:
                    # We're in a git repo, analyze file ownership
                    for mod in modules:
                        try:
                            file_path = root_path / mod.path
                            # Run git blame to get line-by-line authorship
                            blame_result = subprocess.run(
                                ["git", "blame", "--line-porcelain", str(file_path)],
                                cwd=root_path,
                                capture_output=True,
                                text=True,
                                timeout=10,
                            )

                            if blame_result.returncode == 0:
                                # Parse git blame output
                                authors: dict[str, int] = {}
                                for line in blame_result.stdout.split("\n"):
                                    if line.startswith("author "):
                                        author = line[
                                            7:
                                        ].strip()  # Remove "author " prefix
                                        authors[author] = authors.get(author, 0) + 1

                                if authors:
                                    # Find primary owner (most lines authored)
                                    total_lines = sum(authors.values())
                                    primary_owner = max(
                                        authors.items(), key=lambda x: x[1]
                                    )
                                    owner_name = primary_owner[0]
                                    owner_lines = primary_owner[1]
                                    confidence = round(owner_lines / total_lines, 2)

                                    git_ownership.append(
                                        {
                                            "module": mod.path,
                                            "owner": owner_name,
                                            "confidence": confidence,
                                            "contributors": len(authors),
                                            "ownership_breakdown": {
                                                k: round(v / total_lines, 2)
                                                for k, v in sorted(
                                                    authors.items(),
                                                    key=lambda x: x[1],
                                                    reverse=True,
                                                )[:5]
                                            },
                                        }
                                    )
                                else:
                                    git_ownership.append(
                                        {
                                            "module": mod.path,
                                            "owner": "unknown",
                                            "confidence": 0.0,
                                            "reason": "No git blame data",
                                        }
                                    )
                            else:
                                git_ownership.append(
                                    {
                                        "module": mod.path,
                                        "owner": "unknown",
                                        "confidence": 0.0,
                                        "reason": "File not tracked in git",
                                    }
                                )
                        except subprocess.TimeoutExpired:
                            git_ownership.append(
                                {
                                    "module": mod.path,
                                    "owner": "unknown",
                                    "confidence": 0.0,
                                    "reason": "Git blame timeout",
                                }
                            )
                        except Exception as e:
                            git_ownership.append(
                                {
                                    "module": mod.path,
                                    "owner": "unknown",
                                    "confidence": 0.0,
                                    "reason": f"Error: {str(e)}",
                                }
                            )
                else:
                    # Not a git repository
                    git_ownership = [
                        {
                            "module": mod.path,
                            "owner": "unknown",
                            "confidence": 0.0,
                            "reason": "Not a git repository",
                        }
                        for mod in modules
                    ]
            except FileNotFoundError:
                # Git not installed
                git_ownership = [
                    {
                        "module": mod.path,
                        "owner": "unknown",
                        "confidence": 0.0,
                        "reason": "Git not installed",
                    }
                    for mod in modules
                ]
            except Exception as e:
                # Other error
                git_ownership = [
                    {
                        "module": mod.path,
                        "owner": "unknown",
                        "confidence": 0.0,
                        "reason": f"Error: {str(e)}",
                    }
                    for mod in modules
                ]

        # [20251231_FEATURE] v3.3.1 - Enterprise: Historical architecture trends
        if enable_historical_trends:
            historical_trends = []
            try:
                import subprocess

                # Check if we're in a git repository
                result = subprocess.run(
                    ["git", "rev-parse", "--is-inside-work-tree"],
                    cwd=root_path,
                    capture_output=True,
                    text=True,
                    timeout=5,
                )

                if result.returncode == 0:
                    # Get file changes over last 30 days for trend analysis
                    log_result = subprocess.run(
                        [
                            "git",
                            "log",
                            "--since=30 days ago",
                            "--name-only",
                            "--pretty=format:%H|%ad|%an",
                            "--date=short",
                        ],
                        cwd=root_path,
                        capture_output=True,
                        text=True,
                        timeout=30,
                    )

                    if log_result.returncode == 0:
                        # Parse git log output into trends
                        file_change_counts: dict[str, int] = {}
                        date_activity: dict[str, int] = {}

                        current_date = ""
                        for line in log_result.stdout.split("\n"):
                            line = line.strip()
                            if not line:
                                continue
                            if "|" in line:
                                # This is a commit header line
                                parts = line.split("|")
                                if len(parts) >= 2:
                                    current_date = parts[1]
                                    date_activity[current_date] = (
                                        date_activity.get(current_date, 0) + 1
                                    )
                            elif line.endswith(".py"):
                                # This is a Python file that was changed
                                file_change_counts[line] = (
                                    file_change_counts.get(line, 0) + 1
                                )

                        # Calculate architecture stability metrics
                        total_changes = sum(file_change_counts.values())
                        hot_files = sorted(
                            file_change_counts.items(), key=lambda x: x[1], reverse=True
                        )[:10]

                        historical_trends = [
                            {
                                "period": "last_30_days",
                                "total_commits": len(date_activity),
                                "total_file_changes": total_changes,
                                "most_changed_files": [
                                    {"file": f, "changes": c} for f, c in hot_files
                                ],
                                "activity_by_date": date_activity,
                                "stability_score": round(
                                    1.0
                                    - min(
                                        total_changes / max(len(modules) * 3, 1), 1.0
                                    ),
                                    2,
                                ),
                            }
                        ]
                    else:
                        historical_trends = [{"error": "Could not retrieve git log"}]
                else:
                    historical_trends = [{"error": "Not a git repository"}]
            except subprocess.TimeoutExpired:
                historical_trends = [{"error": "Git log timeout"}]
            except FileNotFoundError:
                historical_trends = [{"error": "Git not installed"}]
            except Exception as e:
                historical_trends = [{"error": f"Historical trends failed: {str(e)}"}]

        # [20251231_FEATURE] v3.3.1 - Enterprise: Custom map metrics
        if enable_custom_metrics:
            # Load custom metrics from architecture.toml if present
            custom_metrics = {
                "configured": False,
                "metrics": {},
            }
            try:
                arch_config_path = root_path / ".code-scalpel" / "architecture.toml"
                if arch_config_path.exists():
                    try:
                        import tomllib
                    except ImportError:
                        import tomli as tomllib  # type: ignore

                    with open(arch_config_path, "rb") as f:
                        arch_config = tomllib.load(f)

                    if "custom_metrics" in arch_config:
                        custom_metrics = {
                            "configured": True,
                            "source": str(arch_config_path),
                            "metrics": arch_config["custom_metrics"],
                        }
                    else:
                        custom_metrics["note"] = (
                            "No custom_metrics section in architecture.toml"
                        )
                else:
                    custom_metrics["note"] = "No .code-scalpel/architecture.toml found"
            except Exception as e:
                custom_metrics["error"] = f"Failed to load custom metrics: {str(e)}"

        # [20251231_FEATURE] v3.3.1 - Enterprise: Compliance overlay
        if enable_compliance_overlay:
            compliance_overlay = {
                "violations": [],
                "status": "unknown",
            }
            try:
                # Try to use the architectural rules engine if available
                arch_config_path = root_path / ".code-scalpel" / "architecture.toml"
                if arch_config_path.exists():
                    try:
                        # [20260101_BUGFIX] Dynamic import to avoid Pyright errors
                        from code_scalpel.ast_tools import architectural_rules

                        ArchitecturalRulesEngine = getattr(
                            architectural_rules, "ArchitecturalRulesEngine", None
                        )
                        if ArchitecturalRulesEngine is None:
                            raise ImportError("ArchitecturalRulesEngine not found")

                        engine = ArchitecturalRulesEngine(str(arch_config_path))

                        # Analyze each module for architectural violations
                        violations: list[dict[str, Any]] = []
                        for mod in modules:
                            for imp in mod.imports:
                                violation = engine.check_import(mod.path, imp)
                                if violation:
                                    violations.append(
                                        {
                                            "file": mod.path,
                                            "import": imp,
                                            "rule": violation.rule_name,
                                            "severity": violation.severity,
                                            "message": violation.message,
                                        }
                                    )

                        compliance_overlay = {
                            "violations": violations,
                            "violation_count": len(violations),
                            "status": (
                                "compliant" if len(violations) == 0 else "non_compliant"
                            ),
                            "rules_source": str(arch_config_path),
                        }
                    except ImportError:
                        compliance_overlay = {
                            "violations": [],
                            "status": "unavailable",
                            "note": "Architectural rules engine not available",
                        }
                else:
                    compliance_overlay = {
                        "violations": [],
                        "status": "unconfigured",
                        "note": "No .code-scalpel/architecture.toml found",
                    }
            except Exception as e:
                compliance_overlay = {
                    "violations": [],
                    "status": "error",
                    "error": f"Compliance check failed: {str(e)}",
                }

        # [20251231_FEATURE] v3.3.1 - Enterprise: Multi-repository summary placeholder
        # This feature requires additional_roots parameter; for now provide metadata
        if enable_multi_repo:
            multi_repo_summary = {
                "enabled": True,
                "primary_root": str(root_path),
                "additional_roots": [],  # Future: pass via parameter
                "note": "Pass additional_roots parameter for multi-repo analysis",
                "total_repositories": 1,
            }

        # Generate Mermaid package diagram
        mermaid_lines = ["graph TD"]
        mermaid_lines.append("    subgraph Project")
        for i, mod in enumerate(modules[:diagram_limit]):
            mod_id = f"M{i}"
            label = mod.path.replace("/", "_").replace(".", "_")
            if mod.entry_points:
                mermaid_lines.append(
                    f'        {mod_id}[["{label}"]]'
                )  # Stadium for entry
            else:
                mermaid_lines.append(f'        {mod_id}["{label}"]')
        mermaid_lines.append("    end")

        # [20251220_FEATURE] v3.0.5 - Communicate truncation
        diagram_truncated = (
            effective_max_modules is not None and len(modules) > effective_max_modules
        )
        if diagram_truncated and effective_max_modules is not None:
            mermaid_lines.append(
                f"    Note[... and {len(modules) - int(effective_max_modules)} more modules]"
            )

        return ProjectMapResult(
            project_root=str(root_path),
            total_files=len(modules),
            total_lines=total_lines,
            languages=languages,
            packages=pkg_list,
            modules=modules,
            entry_points=all_entry_points,
            circular_imports=circular_imports,
            complexity_hotspots=complexity_hotspots,
            mermaid="\n".join(mermaid_lines),
            module_relationships=module_relationships,
            architectural_layers=architectural_layers,
            coupling_metrics=coupling_metrics,
            dependency_diagram=dependency_diagram,
            city_map_data=city_map_data,
            force_graph=force_graph,
            churn_heatmap=churn_heatmap,
            bug_hotspots=bug_hotspots,
            git_ownership=git_ownership,
            # [20251231_FEATURE] v3.3.1 - New Enterprise fields
            multi_repo_summary=multi_repo_summary,
            historical_trends=historical_trends,
            custom_metrics=custom_metrics,
            compliance_overlay=compliance_overlay,
            modules_in_diagram=modules_in_diagram,
            diagram_truncated=diagram_truncated,
            # [20250112_FIX] v3.3.0 - Include tier metadata fields
            tier_applied=tier,
            max_files_applied=effective_max_files,
            max_modules_applied=effective_max_modules,
            pro_features_enabled=tier in ("pro", "enterprise"),
            enterprise_features_enabled=tier == "enterprise",
        )

    except Exception as e:
        # tier is available since we resolve it before the try block
        return ProjectMapResult(
            success=False,
            project_root=str(root_path),
            error=f"Project map analysis failed: {str(e)}",
            tier_applied=tier,
            pro_features_enabled=tier in ("pro", "enterprise"),
            enterprise_features_enabled=tier == "enterprise",
        )


@mcp.tool()
async def get_project_map(
    project_root: str | None = None,
    include_complexity: bool = True,
    complexity_threshold: int = 10,
    include_circular_check: bool = True,
    detect_service_boundaries: bool = False,
    min_isolation_score: float = 0.6,
    ctx: Context | None = None,
) -> ProjectMapResult:
    """
    Generate a comprehensive map of the project structure.

    [v1.5.0] Use this tool to get a high-level overview of a codebase before diving in.
    Analyzes all Python files to provide:
    - Package and module structure
    - Function and class inventory per file
    - Entry point detection (main, CLI commands, routes)
    - Complexity hotspots (files that need attention)
    - Circular import detection
    - Mermaid diagram of project structure

    [v3.0.5] Now reports progress during analysis.

    Why AI agents need this:
    - Orientation: Understand project structure before making changes
    - Navigation: Know where to find specific functionality
    - Risk assessment: Identify complex areas that need careful handling
    - Architecture: See how packages and modules are organized

    Args:
        project_root: Project root directory (default: server's project root)
        include_complexity: Calculate cyclomatic complexity (default: True)
        complexity_threshold: Threshold for flagging hotspots (default: 10)
        include_circular_check: Check for circular imports (default: True)

    Returns:
        ProjectMapResult with comprehensive project overview
    """
    # [20251220_FEATURE] v3.0.5 - Progress reporting
    if ctx:
        await ctx.report_progress(0, 100, "Scanning project structure...")

    tier = _get_current_tier()
    caps = get_tool_capabilities("get_project_map", tier) or {}
    limits = caps.get("limits", {}) or {}

    result = await asyncio.to_thread(
        _get_project_map_sync,
        project_root,
        include_complexity,
        complexity_threshold,
        include_circular_check,
        tier=tier,
        capabilities=caps,
        max_files_limit=limits.get("max_files"),
        max_modules_limit=limits.get("max_modules"),
    )

    # Enterprise feature: suggest service boundaries (not a standalone MCP tool).
    if detect_service_boundaries:
        if not has_capability("extract_code", "service_boundaries", tier):
            try:
                result = result.model_copy(
                    update={
                        "service_boundaries_success": False,
                        "service_boundaries_error": "Service boundary detection requires ENTERPRISE tier",
                    }
                )
            except Exception:
                pass
        else:
            try:
                from code_scalpel.surgery.surgical_extractor import (
                    detect_service_boundaries as _detect_boundaries,
                )

                boundaries = await asyncio.to_thread(
                    _detect_boundaries,
                    project_root=project_root,
                    min_isolation_score=min_isolation_score,
                )

                if getattr(boundaries, "success", False):
                    payload: dict[str, Any] = {
                        "service_boundaries_success": True,
                        "suggested_services": [
                            {
                                "service_name": s.service_name,
                                "included_files": s.included_files,
                                "external_dependencies": s.external_dependencies,
                                "internal_dependencies": s.internal_dependencies,
                                "isolation_level": s.isolation_level,
                                "rationale": s.rationale,
                            }
                            for s in boundaries.suggested_services
                        ],
                        "service_dependency_graph": boundaries.dependency_graph,
                        "service_total_files_analyzed": boundaries.total_files_analyzed,
                        "service_boundaries_explanation": boundaries.explanation,
                    }
                else:
                    payload = {
                        "service_boundaries_success": False,
                        "service_boundaries_error": getattr(boundaries, "error", None)
                        or "Service boundary detection failed",
                    }

                result = result.model_copy(update=payload)
            except Exception as e:
                try:
                    result = result.model_copy(
                        update={
                            "service_boundaries_success": False,
                            "service_boundaries_error": f"Service boundary detection failed: {e}",
                        }
                    )
                except Exception:
                    pass

    if ctx:
        msg = f"Analyzed {result.total_files} files, {result.total_lines} lines"
        await ctx.report_progress(100, 100, msg)

    return result


# ============================================================================
# [20251213_FEATURE] v1.5.1 - get_cross_file_dependencies MCP Tool
# ============================================================================


class ImportNodeModel(BaseModel):
    """Information about an import in the import graph."""

    module: str = Field(description="Module name (e.g., 'os', 'mypackage.utils')")
    import_type: str = Field(description="Import type: 'direct', 'from', or 'star'")
    names: list[str] = Field(
        default_factory=list, description="Imported names (for 'from' imports)"
    )
    alias: str | None = Field(default=None, description="Alias if import uses 'as'")
    line: int = Field(default=0, description="Line number of import")


class SymbolDefinitionModel(BaseModel):
    """Information about a symbol defined in a file."""

    name: str = Field(description="Symbol name")
    file: str = Field(description="File where symbol is defined (relative path)")
    line: int = Field(default=0, description="Line number of definition")
    symbol_type: str = Field(description="Type: 'function', 'class', or 'variable'")
    is_exported: bool = Field(default=False, description="Whether symbol is in __all__")


class ExtractedSymbolModel(BaseModel):
    """An extracted symbol with its code and dependencies."""

    name: str = Field(description="Symbol name")
    code: str = Field(description="Full source code of the symbol")
    file: str = Field(description="Source file (relative path)")
    line_start: int = Field(default=0, description="Starting line number")
    line_end: int = Field(default=0, description="Ending line number")
    dependencies: list[str] = Field(
        default_factory=list, description="Names of symbols this depends on"
    )
    # [20251216_FEATURE] v2.5.0 - Confidence decay for deep dependency chains
    depth: int = Field(default=0, description="Depth from original target (0 = target)")
    confidence: float = Field(
        default=1.0,
        description="Confidence score with decay applied (0.0-1.0). Formula: C_base  0.9^depth",
    )
    low_confidence: bool = Field(
        default=False, description="True if confidence is below threshold (0.5)"
    )


class AliasResolutionModel(BaseModel):
    """Import alias resolution details (Pro tier)."""

    alias: str = Field(description="Alias name as used in the importing module")
    original_module: str = Field(
        description="Module where the symbol originates (pre-alias)"
    )
    original_name: str | None = Field(
        default=None, description="Original symbol name before aliasing"
    )
    file: str | None = Field(
        default=None, description="File containing the aliasing import (relative)"
    )
    line: int | None = Field(default=None, description="Line number of the import")


class WildcardExpansionModel(BaseModel):
    """Wildcard import expansion details (Pro tier)."""

    from_module: str = Field(description="Module imported with a wildcard")
    expanded_symbols: list[str] = Field(
        default_factory=list,
        description="Symbols expanded from __all__ or public definitions",
    )


class ReexportChainModel(BaseModel):
    """Re-export chain information (Pro tier)."""

    symbol: str = Field(description="Symbol name exposed by re-export")
    apparent_source: str = Field(description="Module that appears to export the symbol")
    actual_source: str = Field(description="Module where the symbol truly originates")


class ChainedAliasResolutionModel(BaseModel):
    """Multi-hop alias resolution details (Pro tier)."""

    symbol: str = Field(description="Alias as referenced in the target module")
    chain: list[str] = Field(
        default_factory=list,
        description="Modules traversed while resolving the alias chain",
    )
    resolved_module: str | None = Field(
        default=None, description="Module where the symbol ultimately resides"
    )
    resolved_name: str | None = Field(
        default=None, description="Original symbol name after resolving aliases"
    )


class CouplingViolationModel(BaseModel):
    """Coupling metric violation (Enterprise tier)."""

    metric: str = Field(
        description="Metric name, e.g., fan_in/fan_out/dependency_depth"
    )
    value: int | float = Field(description="Observed metric value")
    limit: int | float = Field(description="Configured limit for the metric")
    module: str | None = Field(
        default=None, description="Module evaluated for coupling"
    )
    severity: str | None = Field(
        default=None, description="Severity level for violation"
    )
    description: str | None = Field(default=None, description="Human-readable summary")


class ArchitecturalViolationModel(BaseModel):
    """Architectural rule violation (Enterprise tier)."""

    type: str = Field(description="Rule name/type that was violated")
    severity: str = Field(description="Severity classification")
    source: str | None = Field(default=None, description="Source module/file")
    target: str | None = Field(default=None, description="Target module/file")
    from_layer: str | None = Field(default=None, description="Layer of source module")
    to_layer: str | None = Field(default=None, description="Layer of target module")
    description: str | None = Field(default=None, description="Violation description")
    recommendation: str | None = Field(
        default=None, description="Suggested remediation for the violation"
    )


class BoundaryAlertModel(BaseModel):
    """Layer boundary alert (Enterprise tier)."""

    rule: str | None = Field(default=None, description="Rule producing the alert")
    from_layer: str | None = Field(default=None, description="Origin layer")
    to_layer: str | None = Field(default=None, description="Destination layer")
    source: str | None = Field(default=None, description="Source module/file")
    target: str | None = Field(default=None, description="Target module/file")


class CrossFileDependenciesResult(BaseModel):
    """Result of cross-file dependency analysis."""

    success: bool = Field(description="Whether analysis succeeded")
    server_version: str = Field(default=__version__, description="Code Scalpel version")

    # Target symbol info
    target_name: str = Field(default="", description="Name of the analyzed symbol")
    target_file: str = Field(
        default="", description="File containing the target symbol"
    )

    # Dependency info
    extracted_symbols: list[ExtractedSymbolModel] = Field(
        default_factory=list,
        description="All symbols extracted (target + dependencies)",
    )
    total_dependencies: int = Field(
        default=0, description="Number of dependencies resolved"
    )
    unresolved_imports: list[str] = Field(
        default_factory=list, description="External imports that could not be resolved"
    )

    # Import graph info
    import_graph: dict[str, list[str]] = Field(
        default_factory=dict, description="Import graph: file -> list of imported files"
    )
    circular_imports: list[list[str]] = Field(
        default_factory=list, description="Detected circular import cycles"
    )

    # Combined code for AI consumption
    combined_code: str = Field(
        default="", description="All extracted code combined, ready for AI consumption"
    )
    token_estimate: int = Field(
        default=0, description="Estimated token count for combined code"
    )

    # [20251226_FEATURE] Tier-aware metadata
    transitive_depth: int = Field(
        default=0,
        description="Max transitive depth actually analyzed after tier limits",
    )
    # Pro tier dependency insights
    alias_resolutions: list[AliasResolutionModel] = Field(
        default_factory=list,
        description="Resolved import aliases with original modules (Pro)",
    )
    wildcard_expansions: list[WildcardExpansionModel] = Field(
        default_factory=list,
        description="Expanded symbols from wildcard imports (Pro)",
    )
    reexport_chains: list[ReexportChainModel] = Field(
        default_factory=list,
        description="Re-export chains tracing apparent vs actual sources (Pro)",
    )
    chained_alias_resolutions: list[ChainedAliasResolutionModel] = Field(
        default_factory=list,
        description="Multi-hop alias resolution chains (Pro)",
    )

    coupling_score: float | None = Field(
        default=None,
        description="Coupling heuristic (dependencies / unique files) when enabled",
    )
    coupling_violations: list[CouplingViolationModel] = Field(
        default_factory=list,
        description="Coupling metrics exceeding configured limits (Enterprise)",
    )
    dependency_chains: list[list[str]] = Field(
        default_factory=list,
        description="Dependency chains traced when transitive mapping is enabled",
    )
    boundary_violations: list[dict[str, Any]] = Field(
        default_factory=list,
        description="Architectural boundary violations detected (Enterprise)",
    )
    layer_violations: list[dict[str, Any]] = Field(
        default_factory=list,
        description="Layering issues detected (Enterprise)",
    )
    architectural_alerts: list[dict[str, Any]] = Field(
        default_factory=list,
        description="Aggregated alerts for detected violations",
    )
    architectural_violations: list[ArchitecturalViolationModel] = Field(
        default_factory=list,
        description="Architectural rule engine violations (Enterprise)",
    )
    boundary_alerts: list[BoundaryAlertModel] = Field(
        default_factory=list,
        description="Layer boundary alerts with from/to layer context (Enterprise)",
    )
    layer_mapping: dict[str, list[str]] = Field(
        default_factory=dict,
        description="Layer name to file list mapping from architecture rules",
    )
    rules_applied: list[str] = Field(
        default_factory=list,
        description="Architectural rules evaluated by the rule engine",
    )
    architectural_rules_applied: list[str] = Field(
        default_factory=list,
        description="Explicit list of architectural rules applied (Enterprise)",
    )
    exempted_files: list[str] = Field(
        default_factory=list,
        description="Files/modules exempted from rule checks via configuration",
    )

    files_scanned: int = Field(
        default=0,
        description="Unique files observed before truncation",
    )
    files_truncated: int = Field(
        default=0,
        description="Number of files dropped due to tier limits",
    )
    truncation_warning: str | None = Field(
        default=None,
        description="Warning describing any applied truncation",
    )
    truncated: bool = Field(
        default=False,
        description="True if analysis truncated files due to limits",
    )
    files_analyzed: int = Field(
        default=0,
        description="Count of files analyzed (post-truncation, effective)",
    )
    max_depth_reached: int = Field(
        default=0,
        description="Actual maximum dependency depth reached during traversal",
    )

    # Mermaid diagram
    mermaid: str = Field(
        default="", description="Mermaid diagram of import relationships"
    )

    # [20251216_FEATURE] v2.5.0 - Confidence decay tracking
    confidence_decay_factor: float = Field(
        default=0.9,
        description="Decay factor used: C_effective = C_base  decay_factor^depth",
    )
    low_confidence_count: int = Field(
        default=0, description="Number of symbols below confidence threshold (0.5)"
    )
    low_confidence_warning: str | None = Field(
        default=None, description="Warning message if low-confidence symbols detected"
    )

    error: str | None = Field(default=None, description="Error message if failed")


def _get_cross_file_dependencies_sync(
    target_file: str,
    target_symbol: str,
    project_root: str | None,
    max_depth: int,
    include_code: bool,
    include_diagram: bool,
    confidence_decay_factor: float = 0.9,
    tier: str | None = None,
    capabilities: dict | None = None,
    max_files_limit: int | None = None,
    timeout_seconds: float | None = None,
) -> CrossFileDependenciesResult:
    """
    Synchronous implementation of get_cross_file_dependencies.

    [20251220_BUGFIX] v3.0.5 - Parameter order matches async function for consistency.
    """
    from code_scalpel.ast_tools.cross_file_extractor import CrossFileExtractor
    from code_scalpel.licensing.features import get_tool_capabilities
    import os

    root_path = Path(project_root) if project_root else PROJECT_ROOT

    # [20260127_FIX] Heuristic Auto-Scoping for Community Tier
    # If project_root is not provided by user, check if default usage would cover too many files.
    # If so, scope down to the parent directory of the target file instead of the entire project root.
    if project_root is None and not tier == "enterprise":
        # Check volume roughly
        try:
             # Fast check using git if available, or os.walk (limited)
             # Here we assume a simple heuristic: if we are deeper than root, use parent dir.
             target_path_obj = Path(target_file)
             if not target_path_obj.is_absolute():
                 target_path_obj = root_path / target_file
             
             # If target is inside root_path but deeper
             if target_path_obj.exists() and len(target_path_obj.parts) > len(root_path.parts) + 1:
                 # Re-scope root to the parent of the target file
                 # e.g. /project/src/module/file.py -> /project/src/module
                 root_path = target_path_obj.parent
        except Exception:
            pass # Fallback to default behavior on error

    if not root_path.exists():
        return CrossFileDependenciesResult(
            success=False,
            error=f"Project root not found: {root_path}.",
        )

    # Resolve target file path
    target_path = Path(target_file)
    if not target_path.is_absolute():
        target_path = root_path / target_file

    if not target_path.exists():
        return CrossFileDependenciesResult(
            success=False,
            error=f"Target file not found: {target_path}.",
        )

    tier = tier or _get_current_tier()
    caps = capabilities or get_tool_capabilities("get_cross_file_dependencies", tier)
    caps_set = set(caps.get("capabilities", set()) or [])
    limits = caps.get("limits", {}) or {}

    effective_max_depth = max_depth
    depth_limit = limits.get("max_depth")
    if depth_limit is not None and effective_max_depth > depth_limit:
        effective_max_depth = int(depth_limit)

    if max_files_limit is None:
        max_files_limit = limits.get("max_files")

    # Allow caller override but never exceed tier-imposed limit
    if limits.get("max_files") is not None and max_files_limit is not None:
        max_files_limit = min(max_files_limit, limits["max_files"])

    # [20251227_REFACTOR] Uniform generous timeout for all tiers
    # Timeout is a safeguard, not a tier feature. The depth/file limits
    # naturally bound execution time. Timeout only triggers for pathological cases.
    build_timeout = int(timeout_seconds) if timeout_seconds else 120

    allow_transitive = "transitive_dependency_mapping" in caps_set
    coupling_enabled = "deep_coupling_analysis" in caps_set
    firewall_enabled = (
        "architectural_firewall" in caps_set or "boundary_violation_alerts" in caps_set
    )

    # [20260104_BUGFIX] Initialize all variables that may be referenced in return statement
    # to avoid UnboundLocalError if exception occurs before assignment
    files_analyzed = 0
    max_depth_reached = 0
    files_scanned = 0
    files_truncated = 0
    truncation_warning = None

    try:
        from concurrent.futures import ThreadPoolExecutor
        from concurrent.futures import TimeoutError as FuturesTimeoutError

        def run_with_timeout(func, timeout_seconds, *args, **kwargs):
            """
            Cross-platform timeout wrapper using ThreadPoolExecutor.

            Works on both Unix/Linux and Windows by running the function
            in a thread pool with a timeout.

            Args:
                func: Function to execute
                timeout_seconds: Maximum execution time in seconds
                *args, **kwargs: Arguments to pass to func

            Returns:
                Result from func

            Raises:
                TimeoutError: If execution exceeds timeout_seconds
            """
            # NOTE: Avoid `with ThreadPoolExecutor(...)` here.
            # If `future.result(timeout=...)` times out, the context manager's
            # shutdown(wait=True) can block forever waiting for a hung worker.
            executor = ThreadPoolExecutor(max_workers=1)
            future = executor.submit(func, *args, **kwargs)
            try:
                return future.result(timeout=timeout_seconds)
            except FuturesTimeoutError:
                future.cancel()
                raise TimeoutError(
                    f"Operation timed out after {timeout_seconds} seconds"
                )
            finally:
                # Do not wait for a potentially hung worker thread.
                executor.shutdown(wait=False, cancel_futures=True)

        # Build CrossFileExtractor (includes ImportResolver.build()) with timeout protection.
        def build_extractor():
            extractor = CrossFileExtractor(root_path)
            extractor.build()
            return extractor

        try:
            # [20251227_REFACTOR] Generous timeout safeguard (not tier-limited)
            # [20260127_FIX] Add volume check before build to prevent timeout on large repos
            # Community tier limit is 500 files. If scoping is wide (root_path), verify count first.
            if tier == "community" or (tier is None and not caps):
                file_count = 0
                too_many = False
                for _, _, files in os.walk(str(root_path)):
                    file_count += len(files)
                    if file_count > 500:
                        too_many = True
                        break
                if too_many:
                     return CrossFileDependenciesResult(
                        success=False,
                        error=f"Scope too large (>500 files). Community Tier is limited to 500 files per scan. Please verify a specific subdirectory using the 'project_root' parameter (Current: {root_path}).",
                    )

            extractor = run_with_timeout(build_extractor, build_timeout)
        except TimeoutError:
            # [20251227_FEATURE] Context-window aware error messaging for AI agents
            return CrossFileDependenciesResult(
                success=False,
                error=(
                    f"TIMEOUT ({build_timeout}s): CrossFileExtractor.build() exceeded safeguard limit. "
                    f"FIX: Use a smaller project_root scope. Current: {root_path}. "
                    f"EXAMPLE: Instead of project root, target a specific subdirectory like 'src/module/'. "
                    f"LIMITS: max_depth={effective_max_depth}, max_files={max_files_limit or 'unlimited'}."
                ),
            )

        # [20251216_FEATURE] v2.5.0 - Pass confidence_decay_factor to extractor
        def extract_dependencies():
            return extractor.extract(
                str(target_path),
                target_symbol,
                depth=effective_max_depth,
                confidence_decay_factor=confidence_decay_factor,
            )

        # [20251227_REFACTOR] Extraction timeout is 50% of build timeout
        extraction_timeout = build_timeout // 2
        try:
            extraction_result = run_with_timeout(
                extract_dependencies, extraction_timeout
            )
        except TimeoutError:
            # [20251227_FEATURE] Context-window aware error messaging for AI agents
            return CrossFileDependenciesResult(
                success=False,
                error=(
                    f"TIMEOUT ({extraction_timeout}s): Extracting '{target_symbol}' exceeded safeguard limit. "
                    f"FIX: Reduce max_depth (current: {effective_max_depth}) or target a simpler symbol. "
                    f"EXAMPLE: Try max_depth=1 for direct dependencies only. "
                    f"FILE: {target_path}"
                ),
            )

        # Check for extraction errors
        if not extraction_result.success:
            return CrossFileDependenciesResult(
                success=False,
                error=f"Extraction failed: {'; '.join(extraction_result.errors)}.",
            )

        # Build the list of all symbols (target + dependencies)
        all_symbols = []
        if extraction_result.target:
            all_symbols.append(extraction_result.target)
        all_symbols.extend(extraction_result.dependencies)

        # Convert extracted symbols to models
        extracted_symbols = []
        combined_parts = []

        # [20251216_FEATURE] v2.5.0 - Low confidence threshold
        LOW_CONFIDENCE_THRESHOLD = 0.5

        for sym in all_symbols:
            rel_file = (
                str(Path(sym.file).relative_to(root_path))
                if Path(sym.file).is_absolute()
                else sym.file
            )
            # [20251216_FEATURE] v2.5.0 - Include depth and confidence in symbol model
            extracted_symbols.append(
                ExtractedSymbolModel(
                    name=sym.name,
                    code=sym.code if include_code else "",
                    file=rel_file,
                    line_start=sym.line,  # ExtractedSymbol uses 'line' not 'line_start'
                    line_end=sym.end_line or 0,  # ExtractedSymbol uses 'end_line'
                    dependencies=list(sym.dependencies),
                    depth=sym.depth,
                    confidence=sym.confidence,
                    low_confidence=sym.confidence < LOW_CONFIDENCE_THRESHOLD,
                )
            )
            if include_code:
                combined_parts.append(f"# From {rel_file}")
                combined_parts.append(sym.code)

        combined_code = "\n\n".join(combined_parts) if include_code else ""

        # Use the extractor's combined code if available (includes proper ordering)
        if include_code and extraction_result.combined_code:
            combined_code = extraction_result.combined_code

        # Get unresolved imports from extraction result
        unresolved_imports = (
            extraction_result.module_imports
        )  # These are imports that couldn't be resolved

        # Build import graph dict (file -> list of imported files)
        # Use the extractor's resolver (avoid double-building) and keep this focused.
        resolver = extractor.resolver
        import_graph: dict[str, list[str]] = {}

        # [20251226_FEATURE] Tier-aware truncation and metrics
        file_order: list[str] = []
        for sym in extracted_symbols:
            if sym.file not in file_order:
                file_order.append(sym.file)

        files_scanned = len(file_order)
        files_truncated = 0
        truncation_warning = None
        if max_files_limit is not None and files_scanned > max_files_limit:
            files_truncated = files_scanned - max_files_limit
            allowed_files = set(file_order[:max_files_limit])
            truncation_warning = f"Truncated to {max_files_limit} files (of {files_scanned}) due to tier limits."
            extracted_symbols = [
                sym for sym in extracted_symbols if sym.file in allowed_files
            ]
            file_order = file_order[:max_files_limit]

            if include_code:
                combined_parts = []
                for sym in extracted_symbols:
                    combined_parts.append(f"# From {sym.file}")
                    combined_parts.append(sym.code)
                combined_code = "\n\n".join(combined_parts)

        # Limit graph construction to files actually involved in the extraction.
        files_of_interest: set[str] = set()
        for sym in extracted_symbols:
            try:
                p = Path(sym.file)
                if not p.is_absolute():
                    p = root_path / p
                p = p.resolve()
                files_of_interest.add(str(p))
            except Exception:
                continue
        files_of_interest.add(str(target_path.resolve()))

        for abs_file in files_of_interest:
            module_name = resolver.file_to_module.get(abs_file)
            if not module_name:
                continue

            rel_path = str(Path(abs_file).relative_to(root_path))
            imported_files: list[str] = []
            for imp in resolver.imports.get(module_name, []):
                resolved_file = resolver.module_to_file.get(imp.module)
                if not resolved_file:
                    continue
                try:
                    resolved_abs = str(Path(resolved_file).resolve())
                    if resolved_abs not in files_of_interest:
                        continue
                    resolved_rel = str(Path(resolved_abs).relative_to(root_path))
                except Exception:
                    continue
                if resolved_rel not in imported_files:
                    imported_files.append(resolved_rel)

            if imported_files:
                import_graph[rel_path] = imported_files

        # Make target file relative (used by diagram + returned fields)
        target_rel = (
            str(target_path.relative_to(root_path))
            if target_path.is_absolute()
            else target_file
        )

        transitive_depth = effective_max_depth

        # [20251226_FEATURE] Enforce tier depth clamp: ensure we never exceed tier's configured max_depth.
        # This applies even if user requested higher. For community, max_depth_limit is 1.
        max_depth_limit = limits.get("max_depth")
        if max_depth_limit is not None and transitive_depth > max_depth_limit:
            transitive_depth = int(max_depth_limit)

        coupling_score: float | None = None
        if coupling_enabled:
            unique_files = len(file_order) if file_order else 0
            deps_count = max(0, len(extracted_symbols) - 1)
            if unique_files > 0:
                coupling_score = round(deps_count / unique_files, 3)

        dependency_chains: list[list[str]] = []
        if allow_transitive and import_graph:
            from collections import deque

            max_chains = 25
            queue = deque([(target_rel, [target_rel], 0)])
            seen_paths: set[tuple[str, ...]] = set()
            while queue and len(dependency_chains) < max_chains:
                current, path, depth = queue.popleft()
                if depth >= transitive_depth:
                    continue
                for dep in import_graph.get(current, []):
                    new_path = path + [dep]
                    path_key = tuple(new_path)
                    if path_key in seen_paths:
                        continue
                    seen_paths.add(path_key)
                    dependency_chains.append(new_path)
                    queue.append((dep, new_path, depth + 1))

            # Update max_depth_reached: longest chain (edges = len-1), bounded by transitive_depth limit.
            # The test expects max_depth_reached <= transitive_depth (which is tier-clamped).
            if dependency_chains:
                chain_depths = [max(0, len(c) - 1) for c in dependency_chains]
                if chain_depths:
                    observed = max(chain_depths)
                    # But we enforce transitive_depth as the hard limit; if chains are longer, cap it.
                    max_depth_reached = min(
                        max(max_depth_reached, observed), transitive_depth
                    )

        # [20251226_FEATURE] Enterprise architectural firewall outputs
        boundary_violations: list[dict[str, Any]] = []
        layer_violations: list[dict[str, Any]] = []
        architectural_alerts: list[dict[str, Any]] = []

        def _infer_layer(rel_path: str) -> str:
            lowered = rel_path.lower()
            if "controllers" in lowered or "api" in lowered:
                return "presentation"
            if "services" in lowered or "service" in lowered:
                return "domain"
            if "models" in lowered or "entities" in lowered:
                return "data"
            return "unknown"

        if firewall_enabled and import_graph:
            layer_rank = {"presentation": 3, "domain": 2, "data": 1, "unknown": 0}
            for src_file, targets in import_graph.items():
                src_layer = _infer_layer(src_file)
                for dst_file in targets:
                    dst_layer = _infer_layer(dst_file)
                    if src_layer == "presentation" and dst_layer == "data":
                        violation = {
                            "type": "layer_skip",
                            "source": src_file,
                            "target": dst_file,
                            "violation": "Presentation layer should not depend directly on data layer",
                            "recommendation": "Route dependencies through services to enforce layering",
                        }
                        boundary_violations.append(violation)
                        layer_violations.append(violation)
                    elif (
                        layer_rank.get(src_layer, 0) < layer_rank.get(dst_layer, 0)
                        and dst_layer != "unknown"
                    ):
                        violation = {
                            "type": "upward_dependency",
                            "source": src_file,
                            "target": dst_file,
                            "violation": "Lower layers should not depend on higher layers",
                            "recommendation": "Refactor to invert dependency or introduce interface",
                        }
                        layer_violations.append(violation)

            if boundary_violations or layer_violations:
                architectural_alerts.append(
                    {
                        "severity": "high",
                        "message": "Architectural boundary violations detected",
                        "count": len(boundary_violations) + len(layer_violations),
                    }
                )

        # Detect circular imports using get_circular_imports()
        circular_import_objs = resolver.get_circular_imports()
        circular_import_lists = [
            ci.cycle for ci in circular_import_objs
        ]  # CircularImport uses 'cycle'

        # Make target file relative (used by diagram + returned fields)
        target_rel = (
            str(target_path.relative_to(root_path))
            if target_path.is_absolute()
            else target_file
        )

        # Generate Mermaid diagram
        mermaid = ""
        if include_diagram:
            from collections import deque

            # Generate a focused diagram to avoid project-wide graph explosion.
            # We bound the subgraph by max_depth and cap nodes/edges.
            max_mermaid_nodes = 60
            max_mermaid_edges = 200

            start_file = target_rel
            # BFS from target file using the computed import_graph (file -> imported files)
            queue = deque([(start_file, 0)])
            seen_nodes: set[str] = set()
            edges_out: list[tuple[str, str]] = []

            while (
                queue
                and len(seen_nodes) < max_mermaid_nodes
                and len(edges_out) < max_mermaid_edges
            ):
                cur, depth = queue.popleft()
                if cur in seen_nodes:
                    continue
                seen_nodes.add(cur)
                if depth >= effective_max_depth:
                    continue
                for dep in import_graph.get(cur, [])[:max_mermaid_edges]:
                    if len(edges_out) >= max_mermaid_edges:
                        break
                    edges_out.append((cur, dep))
                    if dep not in seen_nodes:
                        queue.append((dep, depth + 1))

            # Mermaid with stable short node ids
            lines = ["graph TD"]
            node_ids: dict[str, str] = {}
            # Always include the start node (even if it has no outgoing edges)
            seen_nodes.add(start_file)
            for i, n in enumerate(sorted(seen_nodes)):
                node_ids[n] = f"N{i}"
                safe_label = n.replace("/", "_").replace(".", "_")
                lines.append(f"    {node_ids[n]}[{safe_label}]")

            for a, b in edges_out:
                if a in node_ids and b in node_ids:
                    lines.append(f"    {node_ids[a]} --> {node_ids[b]}")

            # Truncation hint
            if (
                len(seen_nodes) >= max_mermaid_nodes
                or len(edges_out) >= max_mermaid_edges
            ):
                lines.append(
                    f"    %% Diagram truncated (nodes<={max_mermaid_nodes}, edges<={max_mermaid_edges})"
                )

            mermaid = "\n".join(lines)

        # Calculate token estimate (rough: 4 chars per token)
        token_estimate = len(combined_code) // 4 if combined_code else 0

        # [20251216_FEATURE] v2.5.0 - Build low confidence warning if needed
        low_confidence_warning = None
        if extraction_result.low_confidence_count > 0:
            low_conf_names = [
                s.name for s in extraction_result.get_low_confidence_symbols()[:5]
            ]
            low_confidence_warning = (
                f" {extraction_result.low_confidence_count} symbol(s) have low confidence "
                f"(below 0.5): {', '.join(low_conf_names)}"
                + ("..." if extraction_result.low_confidence_count > 5 else "")
            )

        # Pro/Enterprise derived insights
        alias_resolutions: list[AliasResolutionModel] = []
        chained_alias_resolutions: list[ChainedAliasResolutionModel] = []
        wildcard_expansions: list[WildcardExpansionModel] = []
        reexport_chains: list[ReexportChainModel] = []
        coupling_violations: list[CouplingViolationModel] = []
        architectural_violations: list[ArchitecturalViolationModel] = []
        boundary_alerts: list[BoundaryAlertModel] = []
        layer_mapping: dict[str, list[str]] = {}
        rules_applied: list[str] = []
        exempted_files: list[str] = []
        files_analyzed = len(files_of_interest)
        if max_files_limit is not None:
            files_analyzed = min(files_analyzed, max_files_limit)
        max_depth_reached = extraction_result.depth_reached

        # Alias / wildcard / re-export analysis (Pro+)
        if tier != "community":
            module_cache: dict[str, str] = {}
            for abs_file in files_of_interest:
                module_name = resolver.file_to_module.get(abs_file)
                if not module_name:
                    continue
                module_cache[abs_file] = module_name

                # Alias resolutions
                for imp in resolver.imports.get(module_name, []):
                    if imp.alias:
                        try:
                            rel_file = str(Path(abs_file).relative_to(root_path))
                        except Exception:
                            rel_file = Path(abs_file).name

                        alias_resolutions.append(
                            AliasResolutionModel(
                                alias=imp.alias,
                                original_module=imp.module or "",
                                original_name=imp.name,
                                file=rel_file,
                                line=imp.line,
                            )
                        )
                        try:
                            resolved_mod, resolved_name, chain = (
                                resolver.resolve_alias_chain(module_name, imp.alias)
                            )
                            chained_alias_resolutions.append(
                                ChainedAliasResolutionModel(
                                    symbol=imp.alias,
                                    chain=chain or [],
                                    resolved_module=resolved_mod,
                                    resolved_name=resolved_name,
                                )
                            )
                        except Exception:
                            chained_alias_resolutions.append(
                                ChainedAliasResolutionModel(
                                    symbol=imp.alias,
                                    chain=[],
                                    resolved_module=None,
                                    resolved_name=None,
                                )
                            )

                # Wildcard expansions per module
                wildcard_map = resolver.expand_all_wildcards(module_name)
                for src_module, symbols in wildcard_map.items():
                    if symbols:
                        wildcard_expansions.append(
                            WildcardExpansionModel(
                                from_module=src_module, expanded_symbols=symbols
                            )
                        )

            # Re-export chains (project-wide)
            reexports = resolver.get_all_reexports()
            for apparent, mapping in reexports.items():
                for symbol, actual in mapping.items():
                    reexport_chains.append(
                        ReexportChainModel(
                            symbol=symbol,
                            apparent_source=apparent,
                            actual_source=actual,
                        )
                    )

        # Enterprise architectural rule engine outputs
        if firewall_enabled and caps_set.intersection(
            {
                "architectural_firewall",
                "dependency_rule_engine",
                "layer_constraint_enforcement",
            }
        ):
            try:
                from code_scalpel.ast_tools.architectural_rules import (
                    ArchitecturalRuleEngine,
                )

                engine = ArchitecturalRuleEngine(root_path)
                engine.load_config()

                rules_applied = list(engine.get_all_rules().keys())
                rules_applied.extend([r.name for r in engine.config.custom_rules])

                # Map modules to layers and exemptions
                for abs_file in files_of_interest:
                    module_name = resolver.file_to_module.get(abs_file)
                    if not module_name:
                        continue

                    if engine.is_exempt(module_name):
                        try:
                            exempted_files.append(
                                str(Path(abs_file).relative_to(root_path))
                            )
                        except Exception:
                            exempted_files.append(Path(abs_file).name)

                    layer = engine.get_layer(module_name)
                    if layer:
                        try:
                            rel = str(Path(abs_file).relative_to(root_path))
                        except Exception:
                            rel = Path(abs_file).name
                        layer_mapping.setdefault(layer, []).append(rel)

                # Dependency violations
                module_for_rel: dict[str, str] = {}
                for rel_path in import_graph.keys():
                    abs_path = str((root_path / rel_path).resolve())
                    module = resolver.file_to_module.get(abs_path)
                    if module:
                        module_for_rel[rel_path] = module

                for rel_src, targets in import_graph.items():
                    src_module = module_for_rel.get(rel_src)
                    if not src_module:
                        continue
                    for rel_tgt in targets:
                        tgt_module = module_for_rel.get(rel_tgt)
                        if not tgt_module:
                            continue
                        violations = engine.check_dependency(src_module, tgt_module)
                        for v in violations:
                            architectural_violations.append(
                                ArchitecturalViolationModel(
                                    type=v.rule_name,
                                    severity=v.severity.value,
                                    source=rel_src,
                                    target=rel_tgt,
                                    from_layer=v.from_layer,
                                    to_layer=v.to_layer,
                                    description=v.description,
                                    recommendation=None,
                                )
                            )
                            if v.from_layer and v.to_layer:
                                boundary_alerts.append(
                                    BoundaryAlertModel(
                                        rule=v.rule_name,
                                        from_layer=v.from_layer,
                                        to_layer=v.to_layer,
                                        source=rel_src,
                                        target=rel_tgt,
                                    )
                                )

                # Coupling violations
                for abs_file in files_of_interest:
                    module_name = resolver.file_to_module.get(abs_file)
                    if not module_name:
                        continue
                    fan_in = len(resolver.reverse_edges.get(module_name, set()))
                    fan_out = len(resolver.edges.get(module_name, set()))
                    coupling_viols = engine.check_coupling(
                        module_name, fan_in, fan_out, max_depth_reached
                    )
                    for v in coupling_viols:
                        metric = "fan_in"
                        limit = engine.config.coupling_limits.max_fan_in
                        value = fan_in
                        if v.rule_name == "high_fan_out":
                            metric = "fan_out"
                            limit = engine.config.coupling_limits.max_fan_out
                            value = fan_out
                        elif v.rule_name == "deep_dependency_chain":
                            metric = "dependency_depth"
                            limit = engine.config.coupling_limits.max_dependency_depth
                            value = max_depth_reached

                        coupling_violations.append(
                            CouplingViolationModel(
                                metric=metric,
                                value=value,
                                limit=limit,
                                module=module_name,
                                severity=v.severity.value,
                                description=v.description,
                            )
                        )

            except Exception:
                # Keep Enterprise extras optional; do not fail core analysis
                pass

        return CrossFileDependenciesResult(
            success=True,
            target_name=target_symbol,
            target_file=target_rel,
            extracted_symbols=extracted_symbols,
            total_dependencies=len(extracted_symbols) - 1,  # Exclude target itself
            unresolved_imports=unresolved_imports,  # Use local variable set from module_imports
            import_graph=import_graph,
            circular_imports=circular_import_lists,
            combined_code=combined_code,
            token_estimate=token_estimate,
            mermaid=mermaid,
            # [20251216_FEATURE] v2.5.0 - Confidence decay fields
            confidence_decay_factor=confidence_decay_factor,
            low_confidence_count=extraction_result.low_confidence_count,
            low_confidence_warning=low_confidence_warning,
            # [20251226_FEATURE] Tier-aware outputs
            transitive_depth=transitive_depth,
            alias_resolutions=alias_resolutions,
            wildcard_expansions=wildcard_expansions,
            reexport_chains=reexport_chains,
            chained_alias_resolutions=chained_alias_resolutions,
            coupling_score=coupling_score,
            coupling_violations=coupling_violations,
            dependency_chains=dependency_chains,
            boundary_violations=boundary_violations,
            layer_violations=layer_violations,
            architectural_alerts=architectural_alerts,
            architectural_violations=architectural_violations,
            boundary_alerts=boundary_alerts,
            layer_mapping=layer_mapping,
            rules_applied=rules_applied,
            architectural_rules_applied=rules_applied,
            exempted_files=exempted_files,
            files_scanned=files_scanned,
            files_truncated=files_truncated,
            truncation_warning=truncation_warning,
            truncated=files_truncated > 0,
            files_analyzed=files_analyzed,
            max_depth_reached=max_depth_reached,
        )

    except Exception as e:
        return CrossFileDependenciesResult(
            success=False,
            error=f"Cross-file dependency analysis failed: {str(e)}",
        )


@mcp.tool()
async def get_cross_file_dependencies(
    target_file: str,
    target_symbol: str,
    project_root: str | None = None,
    max_depth: int = 3,
    include_code: bool = True,
    include_diagram: bool = True,
    confidence_decay_factor: float = 0.9,
    max_files: int | None = None,
    timeout_seconds: float | None = None,
) -> CrossFileDependenciesResult:
    """
    Analyze and extract cross-file dependencies for a symbol.

    [v2.5.0] Use this tool to understand all dependencies a function/class needs
    from other files in the project. It recursively resolves imports and extracts
    the complete dependency chain with source code.

    **Confidence Decay (v2.5.0):**
    Deep dependency chains get exponentially decaying confidence scores.
    Formula: C_effective = 1.0  confidence_decay_factor^depth

    | Depth | Confidence (factor=0.9) |
    |-------|------------------------|
    | 0     | 1.000 (target)         |
    | 1     | 0.900                  |
    | 2     | 0.810                  |
    | 5     | 0.590                  |
    | 10    | 0.349                  |

    Symbols with confidence < 0.5 are flagged as "low confidence".

    Key capabilities:
    - Resolve imports to their source files
    - Extract code for all dependent symbols
    - Detect circular import cycles
    - Generate import relationship diagrams
    - Provide combined code block ready for AI analysis
    - **Confidence scoring** for each symbol based on depth

    Why AI agents need this:
    - Complete Context: Get all code needed to understand a function
    - Safe Refactoring: Know what depends on what before making changes
    - Debugging: Trace data flow across file boundaries
    - Code Review: Understand the full impact of changes
    - **Honest Uncertainty**: Know when deep dependencies may be unreliable

    Example:
        # Analyze 'process_order' function in 'services/order.py'
        result = get_cross_file_dependencies(
            target_file="services/order.py",
            target_symbol="process_order",
            max_depth=5,
            confidence_decay_factor=0.9
        )
        # Check for low-confidence symbols
        if result.low_confidence_count > 0:
            print(f"Warning: {result.low_confidence_warning}")

    Args:
        target_file: Path to file containing the target symbol (relative to project root)
        target_symbol: Name of the function or class to analyze
        project_root: Project root directory (default: server's project root)
        max_depth: Maximum depth of dependency resolution (default: 3)
        include_code: Include full source code in result (default: True)
        include_diagram: Include Mermaid diagram of imports (default: True)
        confidence_decay_factor: Decay factor per depth level (default: 0.9).
                                 Lower values = faster decay. Range: 0.0-1.0

    Returns:
        CrossFileDependenciesResult with extracted symbols, dependency graph, combined code,
        and confidence scores for each symbol
    """
    # [20251226_FEATURE] Tier-aware limits and capabilities
    tier = _get_current_tier()
    caps = get_tool_capabilities("get_cross_file_dependencies", tier) or {}
    limits = caps.get("limits", {}) or {}

    max_depth_limit = limits.get("max_depth")
    effective_max_depth = max_depth
    if max_depth_limit is not None and max_depth > max_depth_limit:
        effective_max_depth = int(max_depth_limit)

    max_files_limit = limits.get("max_files")
    if max_files is not None:
        max_files_limit = max_files

    return await asyncio.to_thread(
        _get_cross_file_dependencies_sync,
        target_file,
        target_symbol,
        project_root,
        effective_max_depth,
        include_code,
        include_diagram,
        confidence_decay_factor,
        tier,
        caps,
        max_files_limit,
        timeout_seconds,
    )


# ============================================================================
# [20251213_FEATURE] v1.5.1 - cross_file_security_scan MCP Tool
# ============================================================================


class TaintFlowModel(BaseModel):
    """Model for a taint flow across files."""

    source_function: str = Field(description="Function where taint originates")
    source_file: str = Field(description="File containing taint source")
    source_line: int = Field(default=0, description="Line number of taint source")
    sink_function: str = Field(description="Function where taint reaches sink")
    sink_file: str = Field(description="File containing sink")
    sink_line: int = Field(default=0, description="Line number of sink")
    flow_path: list[str] = Field(
        default_factory=list, description="Path: file:function -> file:function"
    )
    taint_type: str = Field(description="Type of taint source (e.g., 'request_input')")


class CrossFileVulnerabilityModel(BaseModel):
    """Model for a cross-file vulnerability."""

    type: str = Field(description="Vulnerability type (e.g., 'SQL Injection')")
    cwe: str = Field(description="CWE identifier")
    severity: str = Field(description="Severity: low, medium, high, critical")
    source_file: str = Field(description="File where taint originates")
    sink_file: str = Field(description="File where vulnerability manifests")
    description: str = Field(description="Human-readable description")
    flow: TaintFlowModel = Field(
        description="The taint flow that causes this vulnerability"
    )


class CrossFileSecurityResult(BaseModel):
    """Result of cross-file security analysis."""

    success: bool = Field(description="Whether analysis succeeded")
    server_version: str = Field(default=__version__, description="Code Scalpel version")

    # [20260111_FEATURE] v1.0 - Output metadata fields for transparency
    tier_applied: str = Field(
        default="community",
        description="The tier used for analysis (community, pro, enterprise)",
    )
    max_depth_applied: int | None = Field(
        default=None,
        description="The max depth limit applied (None = unlimited for Enterprise)",
    )
    max_modules_applied: int | None = Field(
        default=None,
        description="The max modules limit applied (None = unlimited for Enterprise)",
    )
    framework_aware_enabled: bool = Field(
        default=False,
        description="Whether framework-aware taint tracking was enabled (Pro+)",
    )
    enterprise_features_enabled: bool = Field(
        default=False,
        description="Whether enterprise features were enabled (global flows, microservice boundaries)",
    )

    # Summary
    files_analyzed: int = Field(default=0, description="Number of files analyzed")
    has_vulnerabilities: bool = Field(
        default=False, description="Whether vulnerabilities were found"
    )
    vulnerability_count: int = Field(
        default=0, description="Total vulnerabilities found"
    )
    risk_level: str = Field(default="low", description="Overall risk level")

    # Detailed findings
    vulnerabilities: list[CrossFileVulnerabilityModel] = Field(
        default_factory=list, description="Cross-file vulnerabilities found"
    )
    taint_flows: list[TaintFlowModel] = Field(
        default_factory=list, description="All taint flows detected"
    )

    # Entry points and sinks
    taint_sources: list[str] = Field(
        default_factory=list, description="Functions containing taint sources"
    )
    dangerous_sinks: list[str] = Field(
        default_factory=list, description="Functions containing dangerous sinks"
    )

    # [20251226_FEATURE] Tier-aware optional outputs for Pro/Enterprise
    framework_contexts: list[dict[str, Any]] | None = Field(
        default=None,
        description="Framework-aware context detection (Spring Beans, React Context)",
    )
    dependency_chains: list[dict[str, Any]] | None = Field(
        default=None,
        description="Inter-file dependency chains contributing to taint flows",
    )
    confidence_scores: dict[str, float] | None = Field(
        default=None, description="Heuristic confidence scores per flow"
    )
    global_flows: list[dict[str, Any]] | None = Field(
        default=None,
        description="Global taint flows across service boundaries (Enterprise)",
    )
    microservice_boundaries: list[dict[str, Any]] | None = Field(
        default=None,
        description="Detected service/domain boundaries (Enterprise)",
    )
    distributed_trace: dict[str, Any] | None = Field(
        default=None, description="Distributed trace representation for flows"
    )

    # Visualization
    mermaid: str = Field(default="", description="Mermaid diagram of taint flows")

    error: str | None = Field(default=None, description="Error message if failed")


def _cross_file_security_scan_sync(
    project_root: str | None,
    entry_points: list[str] | None,
    max_depth: int,
    include_diagram: bool,
    timeout_seconds: float | None = 120.0,  # [20251220_PERF] Default 2 minute timeout
    max_modules: (
        int | None
    ) = 500,  # [20251220_PERF] Default module limit for large projects
    tier: str | None = None,
    capabilities: dict | None = None,
) -> CrossFileSecurityResult:
    """Synchronous implementation of cross_file_security_scan."""
    from code_scalpel.licensing.features import get_tool_capabilities
    from code_scalpel.security.analyzers import CrossFileTaintTracker

    tier = tier or _get_current_tier()
    caps = capabilities or get_tool_capabilities("cross_file_security_scan", tier)
    caps_set = set(caps.get("capabilities", set()) or [])
    limits = caps.get("limits", {}) or {}

    # Enforce limits (support both max_* keys from limits.toml and features.py)
    max_modules_limit = (
        limits.get("max_modules")
        if "max_modules" in limits
        else limits.get("max_files")
    )
    if max_modules_limit is not None and max_modules is not None:
        max_modules = min(max_modules, max_modules_limit)
    elif max_modules_limit is not None:
        max_modules = max_modules_limit

    max_depth_limit = (
        limits.get("max_depth")
        if "max_depth" in limits
        else limits.get("max_taint_depth")
    )
    if max_depth_limit is not None and max_depth is not None:
        max_depth = min(max_depth, max_depth_limit)
    elif max_depth_limit is not None:
        max_depth = max_depth_limit

    root_path = Path(project_root) if project_root else PROJECT_ROOT

    if not root_path.exists():
        return CrossFileSecurityResult(
            success=False,
            error=f"Project root not found: {root_path}.",
        )

    try:
        tracker = CrossFileTaintTracker(root_path)
        # [20251220_PERF] Pass timeout and module limit to prevent hanging
        result = tracker.analyze(
            entry_points=entry_points,
            max_depth=max_depth,
            timeout_seconds=timeout_seconds,
            max_modules=max_modules,
        )

        # Helper to get file path from module name
        def get_file_for_module(module: str) -> str:
            """Get file path for a module, falling back to module name if not found."""
            file_path = tracker.resolver.module_to_file.get(module, module)
            if isinstance(file_path, Path):
                file_path = str(file_path)
            # Make relative if absolute
            try:
                p = Path(file_path)
                if p.is_absolute():
                    return str(p.relative_to(root_path))
            except (ValueError, TypeError):
                pass
            return file_path

        # Convert vulnerabilities to models
        vulnerabilities = []
        for vuln in result.vulnerabilities:
            # [20251215_BUGFIX] v2.0.1 - Use source_module not source_file
            source_file = get_file_for_module(vuln.flow.source_module)
            sink_file = get_file_for_module(vuln.flow.sink_module)

            flow_model = TaintFlowModel(
                source_function=vuln.flow.source_function,
                source_file=source_file,
                source_line=vuln.flow.source_line,
                sink_function=vuln.flow.sink_function,
                sink_file=sink_file,
                sink_line=vuln.flow.sink_line,
                flow_path=[
                    f"{get_file_for_module(m)}:{f}" for m, f, _ in vuln.flow.flow_path
                ],
                taint_type=str(
                    vuln.flow.sink_type.name
                    if hasattr(vuln.flow.sink_type, "name")
                    else vuln.flow.sink_type
                ),
            )
            vulnerabilities.append(
                CrossFileVulnerabilityModel(
                    type=vuln.vulnerability_type,
                    cwe=vuln.cwe_id,
                    severity=vuln.severity,
                    source_file=source_file,
                    sink_file=sink_file,
                    description=vuln.description,
                    flow=flow_model,
                )
            )

        # Convert taint flows to models
        taint_flows = []
        for flow in result.taint_flows:
            # [20251215_BUGFIX] v2.0.1 - Use source_module not source_file
            source_file = get_file_for_module(flow.source_module)
            sink_file = get_file_for_module(flow.sink_module)

            taint_flows.append(
                TaintFlowModel(
                    source_function=flow.source_function,
                    source_file=source_file,
                    source_line=flow.source_line,
                    sink_function=flow.sink_function,
                    sink_file=sink_file,
                    sink_line=flow.sink_line,
                    flow_path=[
                        f"{get_file_for_module(m)}:{f}" for m, f, _ in flow.flow_path
                    ],
                    taint_type=str(
                        flow.sink_type.name
                        if hasattr(flow.sink_type, "name")
                        else flow.sink_type
                    ),
                )
            )

        # Determine risk level
        vuln_count = len(vulnerabilities)
        if vuln_count == 0:
            risk_level = "low"
        elif vuln_count <= 2:
            risk_level = "medium"
        elif vuln_count <= 5:
            risk_level = "high"
        else:
            risk_level = "critical"

        # Generate Mermaid diagram
        mermaid = ""
        if include_diagram:
            mermaid = tracker.get_taint_graph_mermaid()

        # Extract taint sources from tracker's internal state
        taint_sources = []
        dangerous_sinks = []

        # Get taint sources if available
        if hasattr(tracker, "module_taint_sources"):
            for module, sources in tracker.module_taint_sources.items():
                for src in sources:
                    taint_sources.append(f"{module}:{src.function}")

        # Get sinks from taint flows
        for flow in result.taint_flows:
            sink_key = f"{flow.sink_function}"
            if sink_key not in dangerous_sinks:
                dangerous_sinks.append(sink_key)

        # Tier-aware enrichments (heuristic, lightweight to satisfy tier expectations)
        framework_contexts: list[dict[str, Any]] | None = None
        dependency_chains: list[dict[str, Any]] | None = None
        confidence_scores: dict[str, float] | None = None
        global_flows: list[dict[str, Any]] | None = None
        microservice_boundaries: list[dict[str, Any]] | None = None
        distributed_trace: dict[str, Any] | None = None

        def _detect_framework_contexts(
            base_path: Path, limit: int | None
        ) -> list[dict[str, Any]]:
            contexts: list[dict[str, Any]] = []
            max_files = limit or 50
            scanned = 0
            for file_path in base_path.rglob("*"):
                if not file_path.is_file():
                    continue
                if file_path.suffix.lower() not in {
                    ".py",
                    ".js",
                    ".ts",
                    ".tsx",
                    ".jsx",
                }:
                    continue
                try:
                    content = file_path.read_text(errors="ignore")
                except Exception:
                    continue
                scanned += 1
                if scanned > max_files:
                    break
                if "@Autowired" in content or "@Component" in content:
                    contexts.append(
                        {
                            "framework": "spring",
                            "file": str(file_path.relative_to(base_path)),
                        }
                    )
                if "useContext(" in content:
                    contexts.append(
                        {
                            "framework": "react",
                            "file": str(file_path.relative_to(base_path)),
                        }
                    )
            return contexts

        def _build_dependency_chains(
            flows: list[TaintFlowModel],
        ) -> list[dict[str, Any]]:
            chains: list[dict[str, Any]] = []
            for flow in flows[:50]:
                chains.append(
                    {
                        "source": f"{flow.source_file}:{flow.source_function}",
                        "sink": f"{flow.sink_file}:{flow.sink_function}",
                        "path": flow.flow_path,
                    }
                )
            return chains

        def _build_confidence_scores(
            vulns_list: list[CrossFileVulnerabilityModel],
        ) -> dict[str, float]:
            scores: dict[str, float] = {}
            for vuln in vulns_list:
                key = f"{vuln.type}:{vuln.source_file}->{vuln.sink_file}"
                sev = vuln.severity.lower()
                scores[key] = 0.95 if sev in {"critical", "high"} else 0.8
            return scores

        def _detect_global_flows(flows: list[TaintFlowModel]) -> list[dict[str, Any]]:
            gflows: list[dict[str, Any]] = []
            for flow in flows[:100]:
                if "frontend" in flow.source_file or "frontend" in "".join(
                    flow.flow_path
                ):
                    gflows.append(
                        {
                            "source": flow.source_file,
                            "sink": flow.sink_file,
                            "flow_path": flow.flow_path,
                            "taint_type": flow.taint_type,
                        }
                    )
            return gflows

        def _detect_microservice_boundaries(base_path: Path) -> list[dict[str, Any]]:
            boundaries: list[dict[str, Any]] = []
            for child in list(base_path.iterdir())[:10]:
                if child.is_dir():
                    boundaries.append(
                        {
                            "service": child.name,
                            "path": str(child.relative_to(base_path)),
                        }
                    )
            return boundaries

        def _build_distributed_trace(gflows: list[dict[str, Any]]) -> dict[str, Any]:
            nodes: set[str] = set()
            edges: list[tuple[str, str]] = []
            for gf in gflows:
                nodes.add(gf["source"])
                nodes.add(gf["sink"])
                edges.append((gf["source"], gf["sink"]))
            return {"nodes": sorted(nodes), "edges": edges}

        if {
            "framework_aware_taint",
            "spring_bean_tracking",
            "react_context_tracking",
            "dependency_injection_resolution",
        } & caps_set:
            framework_contexts = _detect_framework_contexts(
                root_path, max_modules_limit
            )
            dependency_chains = _build_dependency_chains(taint_flows)
            confidence_scores = _build_confidence_scores(vulnerabilities)

        if {
            "global_taint_flow",
            "frontend_to_backend_tracing",
            "api_to_database_tracing",
            "microservice_boundary_crossing",
        } & caps_set:
            global_flows = _detect_global_flows(taint_flows)
            microservice_boundaries = _detect_microservice_boundaries(root_path)
            if global_flows:
                distributed_trace = _build_distributed_trace(global_flows)

        # [20260111_FEATURE] v1.0 - Compute metadata flags for transparency
        framework_aware_enabled = bool(
            {
                "framework_aware_taint",
                "spring_bean_tracking",
                "react_context_tracking",
                "dependency_injection_resolution",
            }
            & caps_set
        )
        enterprise_features_enabled = bool(
            {
                "global_taint_flow",
                "frontend_to_backend_tracing",
                "api_to_database_tracing",
                "microservice_boundary_crossing",
            }
            & caps_set
        )

        return CrossFileSecurityResult(
            success=True,
            tier_applied=tier,
            max_depth_applied=max_depth_limit,
            max_modules_applied=max_modules_limit,
            framework_aware_enabled=framework_aware_enabled,
            enterprise_features_enabled=enterprise_features_enabled,
            files_analyzed=result.modules_analyzed,  # Use modules_analyzed
            has_vulnerabilities=vuln_count > 0,
            vulnerability_count=vuln_count,
            risk_level=risk_level,
            vulnerabilities=vulnerabilities,
            taint_flows=taint_flows,
            taint_sources=taint_sources,
            dangerous_sinks=dangerous_sinks,
            mermaid=mermaid,
            framework_contexts=framework_contexts,
            dependency_chains=dependency_chains,
            confidence_scores=confidence_scores,
            global_flows=global_flows,
            microservice_boundaries=microservice_boundaries,
            distributed_trace=distributed_trace,
        )

    except Exception as e:
        return CrossFileSecurityResult(
            success=False,
            error=f"Cross-file security analysis failed: {str(e)}",
        )


@mcp.tool()
async def cross_file_security_scan(
    project_root: str | None = None,
    entry_points: list[str] | None = None,
    max_depth: int = 5,
    include_diagram: bool = True,
    timeout_seconds: float | None = 120.0,
    max_modules: int | None = 500,
    ctx: Context | None = None,
) -> CrossFileSecurityResult:
    """
    Perform cross-file security analysis tracking taint flow across module boundaries.

    [v1.5.1] Use this tool to detect vulnerabilities where tainted data crosses
    file boundaries before reaching a dangerous sink. This catches security
    issues that single-file analysis would miss.

    [20251215_FEATURE] v2.0.0 - Progress reporting for long-running operations.
    Reports progress during file discovery and taint analysis phases.

    [20251220_PERF] v3.0.4 - Added timeout and module limits to prevent hanging
    on large codebases with circular imports.

    Key capabilities:
    - Track taint flow through function calls across files
    - Detect vulnerabilities where source and sink are in different files
    - Identify all taint entry points (web inputs, file reads, etc.)
    - Map dangerous sinks (SQL execution, command execution, etc.)
    - Generate taint flow diagrams

    Detects cross-file patterns like:
    - User input in routes.py -> SQL execution in db.py (SQL Injection)
    - Request data in views.py -> os.system() in utils.py (Command Injection)
    - Form input in handlers.py -> open() in storage.py (Path Traversal)

    Why AI agents need this:
    - Defense in depth: Find vulnerabilities that span multiple files
    - Architecture review: Understand how untrusted data flows through the app
    - Code audit: Generate security reports for compliance
    - Risk assessment: Identify highest-risk code paths

    Args:
        project_root: Project root directory (default: server's project root)
        entry_points: Optional list of entry point functions to start from
                     (e.g., ["app.py:main", "routes.py:index"])
                     If None, analyzes all detected entry points
        max_depth: Maximum call depth to trace (default: 5)
        include_diagram: Include Mermaid diagram of taint flows (default: True)
        timeout_seconds: Maximum time in seconds for analysis (default: 120)
                        Set to None for no timeout (not recommended for large projects)
        max_modules: Maximum number of modules to analyze (default: 500)
                    Set to None for no limit (not recommended for large projects)

    Returns:
        CrossFileSecurityResult with vulnerabilities, taint flows, and risk assessment
    """
    # [20251215_FEATURE] v2.0.0 - Progress token support
    # [20251220_FEATURE] v3.0.5 - Enhanced progress messages
    if ctx:
        await ctx.report_progress(
            progress=0, total=100, message="Starting cross-file security scan..."
        )

    tier = _get_current_tier()
    caps = get_tool_capabilities("cross_file_security_scan", tier)

    result = await asyncio.to_thread(
        _cross_file_security_scan_sync,
        project_root,
        entry_points,
        max_depth,
        include_diagram,
        timeout_seconds,
        max_modules,
        tier,
        caps,
    )

    # Report completion with summary
    if ctx:
        vuln_count = result.vulnerability_count
        await ctx.report_progress(
            progress=100,
            total=100,
            message=f"Scan complete: {vuln_count} cross-file vulnerabilities found",
        )

    return result


# ============================================================================
# PATH VALIDATION (v1.5.3)
# ============================================================================


class PathValidationResult(BaseModel):
    """Result of path validation."""

    success: bool = Field(description="Whether all paths were accessible")
    server_version: str = Field(default=__version__, description="Code Scalpel version")
    error: str | None = Field(default=None, description="Error message if failed")
    error_code: str | None = Field(
        default=None, description="Machine-readable error code (when failed)"
    )

    # [20260110_FEATURE] Limits-aware metadata (present when max_paths truncation occurs)
    truncated: bool | None = Field(
        default=None,
        description="Whether input paths were truncated due to tier limits",
    )
    paths_received: int | None = Field(
        default=None,
        description="Number of input paths received before truncation",
    )
    paths_checked: int | None = Field(
        default=None,
        description="Number of paths actually validated",
    )
    max_paths_applied: int | None = Field(
        default=None,
        description="Applied max_paths limit when truncation occurred",
    )
    accessible: list[str] = Field(
        default_factory=list, description="Paths that were successfully resolved"
    )
    inaccessible: list[str] = Field(
        default_factory=list, description="Paths that could not be resolved"
    )
    suggestions: list[str] = Field(
        default_factory=list, description="Suggestions for resolving inaccessible paths"
    )
    workspace_roots: list[str] = Field(
        default_factory=list, description="Detected workspace root directories"
    )
    is_docker: bool = Field(
        default=False, description="Whether running in Docker container"
    )

    # [20251225_FEATURE] v3.3.0 - Tier-specific outputs
    # Pro tier additions
    alias_resolutions: list[dict] = Field(
        default_factory=list, description="Resolved path aliases (tsconfig/webpack)"
    )
    dynamic_imports: list[dict] = Field(
        default_factory=list,
        description="Detected dynamic import patterns in source files",
    )
    # Enterprise tier additions
    traversal_vulnerabilities: list[dict] = Field(
        default_factory=list,
        description="Directory traversal attempts detected with severity",
    )
    boundary_violations: list[dict] = Field(
        default_factory=list, description="Workspace boundary escape violations"
    )
    security_score: float | None = Field(
        default=None, description="Enterprise: Overall security score (0.0-10.0)"
    )


def _validate_paths_sync(
    paths: list[str],
    project_root: str | None,
    tier: str = "community",
    capabilities: dict | None = None,
) -> PathValidationResult:
    """Synchronous implementation of validate_paths.

    [20251225_FEATURE] v3.3.0 - Tier-based feature gating:
      - Community: basic validation, enforces `max_paths`
      - Pro: alias resolution (tsconfig/webpack), dynamic import detection
      - Enterprise: traversal simulation, boundary testing, security score

    [20251231_FEATURE] v3.3.1 - Response filtering via response_config.json
    """
    import json
    import re
    from pathlib import Path as PathLib

    from code_scalpel.licensing.config_loader import (
        filter_response,
        get_cached_response_config,
    )
    from code_scalpel.licensing.features import get_tool_capabilities
    from code_scalpel.mcp.path_resolver import PathResolver

    # Resolve capabilities if not provided
    if capabilities is None:
        capabilities = get_tool_capabilities("validate_paths", tier) or {}

    caps_set: set[str] = set((capabilities.get("capabilities", []) or []))
    limits = capabilities.get("limits", {}) or {}

    # [20260110_FEATURE] Make tier limits observable for safe batching.
    paths_received = len(paths)
    truncated = False
    max_paths_applied: int | None = None

    try:
        max_paths = limits.get("max_paths")
        if max_paths is not None and len(paths) > max_paths:
            truncated = True
            max_paths_applied = int(max_paths)
            paths = paths[:max_paths_applied]

        resolver = PathResolver()
        accessible, inaccessible = resolver.validate_paths(paths, project_root)

        # [20260110_BUGFIX] Deterministic ordering for stable outputs.
        workspace_roots_sorted = sorted(resolver.workspace_roots)

        suggestions: list[str] = []
        if truncated and max_paths_applied is not None:
            suggestions.append(
                f"Input exceeded max_paths={max_paths_applied}; validated first {len(paths)} of {paths_received}."
            )
            suggestions.append("Consider batching into smaller chunks and retrying.")

        if inaccessible:
            if resolver.is_docker:
                suggestions.append(
                    "Running in Docker: Mount your project with -v /path/to/project:/workspace"
                )
                suggestions.append(
                    "Example: docker run -v $(pwd):/workspace code-scalpel:latest"
                )
            else:
                suggestions.append(
                    "Ensure files exist and use absolute paths or place in workspace roots:"
                )
                for root in workspace_roots_sorted[:3]:
                    suggestions.append(f"  - {root}")
            suggestions.append("Set WORKSPACE_ROOT env variable to specify custom root")

        # Initialize tier-specific outputs
        alias_resolutions: list[dict] = []
        dynamic_imports: list[dict] = []
        traversal_vulnerabilities: list[dict] = []
        boundary_violations: list[dict] = []
        security_score: float | None = None

        # Pro: Path alias resolution via tsconfig.json
        if "path_alias_resolution" in caps_set and project_root:
            if "tsconfig_paths_support" in caps_set:
                tsconfig_path = PathLib(project_root) / "tsconfig.json"
                if tsconfig_path.exists():
                    try:
                        tsconfig = json.loads(tsconfig_path.read_text())
                        compiler_options = tsconfig.get("compilerOptions", {})
                        paths_config = compiler_options.get("paths", {})
                        base_url = compiler_options.get("baseUrl", ".")
                        for alias, target_patterns in paths_config.items():
                            for target in target_patterns:
                                alias_key = alias.replace("/*", "")
                                target_path = target.replace("/*", "")
                                resolved = str(
                                    PathLib(project_root) / base_url / target_path
                                )
                                alias_resolutions.append(
                                    {
                                        "alias": alias_key,
                                        "original_path": target,
                                        "resolved_path": resolved,
                                        "source": "tsconfig.json",
                                    }
                                )
                    except Exception as e:  # noqa: BLE001
                        suggestions.append(f"Could not parse tsconfig.json: {e}")

            # Pro: webpack alias support (simple detection)
            if "webpack_alias_support" in caps_set:
                webpack_path = PathLib(project_root) / "webpack.config.js"
                if webpack_path.exists():
                    try:
                        content = webpack_path.read_text()
                        alias_pattern = r"[\"'](@[^\"']+)[\"']\s*:\s*"
                        matches = re.findall(alias_pattern, content)
                        for alias in matches:
                            alias_resolutions.append(
                                {
                                    "alias": alias,
                                    "original_path": f"src/{alias.replace('@', '')}",
                                    "resolved_path": str(
                                        PathLib(project_root)
                                        / "src"
                                        / alias.replace("@", "")
                                    ),
                                    "source": "webpack.config.js",
                                }
                            )
                    except Exception as e:  # noqa: BLE001
                        suggestions.append(f"Could not parse webpack.config.js: {e}")

        # Pro: Dynamic import detection (simplified)
        if "dynamic_import_resolution" in caps_set:
            for apath in accessible:
                if apath.endswith((".js", ".ts", ".jsx", ".tsx")):
                    try:
                        content = PathLib(apath).read_text()
                        if "import(" in content:
                            dynamic_imports.append(
                                {
                                    "source_file": apath,
                                    "import_pattern": "dynamic_import_detected",
                                    "resolved_paths": [],
                                }
                            )
                    except Exception:  # noqa: BLE001
                        pass

        # Enterprise: Path traversal simulation
        if "path_traversal_simulation" in caps_set:
            for p in paths:
                # Traversal simulation should only flag explicit parent directory
                # traversal sequences (".."). Absolute paths outside the workspace
                # are handled by boundary testing, not traversal.
                if ".." in p:
                    severity = "high"
                    if p.count("..") > 2:
                        severity = "critical"
                    traversal_vulnerabilities.append(
                        {
                            "path": p,
                            "escape_attempt": f"Contains {p.count('..')} parent directory references",
                            "severity": severity,
                            "recommendation": "Remove traversal sequences or validate against whitelist",
                        }
                    )

        # Enterprise: Security boundary testing
        if "security_boundary_testing" in caps_set:
            for p in paths:
                try:
                    abs_path = PathLib(p).resolve()
                    is_within = False
                    for root in resolver.workspace_roots:
                        try:
                            abs_path.relative_to(PathLib(root).resolve())
                            is_within = True
                            break
                        except ValueError:
                            continue
                    if not is_within and not p.startswith(("/usr/", "/lib/", "/etc/")):
                        boundary_violations.append(
                            {
                                "path": str(abs_path),
                                "boundary": (
                                    resolver.workspace_roots[0]
                                    if resolver.workspace_roots
                                    else "unknown"
                                ),
                                "violation_type": "workspace_escape",
                                "risk": "high",
                            }
                        )
                except Exception:  # noqa: BLE001
                    pass

        # Enterprise: Security score calculation
        if "security_boundary_testing" in caps_set:
            score = 10.0
            critical_count = sum(
                1 for v in traversal_vulnerabilities if v.get("severity") == "critical"
            )
            high_count = sum(
                1 for v in traversal_vulnerabilities if v.get("severity") == "high"
            )
            score -= critical_count * 3.0 + high_count * 1.5
            score -= len(boundary_violations) * 2.0
            score -= len(inaccessible) * 0.5
            security_score = max(0.0, min(10.0, score))

        # Build result
        result = PathValidationResult(
            success=len(inaccessible) == 0,
            accessible=accessible,
            inaccessible=inaccessible,
            suggestions=suggestions,
            workspace_roots=workspace_roots_sorted,
            is_docker=resolver.is_docker,
            alias_resolutions=alias_resolutions,
            dynamic_imports=dynamic_imports,
            traversal_vulnerabilities=traversal_vulnerabilities,
            boundary_violations=boundary_violations,
            security_score=security_score,
            truncated=True if truncated else None,
            paths_received=paths_received if truncated else None,
            paths_checked=len(paths) if truncated else None,
            max_paths_applied=max_paths_applied if truncated else None,
        )

        # [20251231_FEATURE] Apply response_config.json filtering
        # Note: Response filtering is applied to the dict representation, then we
        # reconstruct the model. Fields excluded by config will use defaults.
        response_config = get_cached_response_config()
        try:
            filtered_dict = filter_response(
                result.model_dump(), "validate_paths", response_config
            )

            return PathValidationResult(**filtered_dict)
        except Exception as e:  # noqa: BLE001
            # [20260110_BUGFIX] Fail open on filtering errors; prefer returning data over crashing.
            _ = e
            return result

    except Exception as e:  # noqa: BLE001
        # [20260110_BUGFIX] Return machine-readable failure instead of raising.
        return PathValidationResult(
            success=False,
            error=str(e),
            error_code="VALIDATE_PATHS_INTERNAL_ERROR",
            accessible=[],
            inaccessible=[],
            suggestions=[
                "Internal error while validating paths.",
                "Retry, and if it persists, report the error_code and error message.",
            ],
            workspace_roots=[],
            is_docker=False,
            truncated=True if truncated else None,
            paths_received=paths_received if truncated else None,
            paths_checked=len(paths) if truncated else None,
            max_paths_applied=max_paths_applied if truncated else None,
        )


@mcp.tool()
async def validate_paths(
    paths: list[str], project_root: str | None = None
) -> PathValidationResult:
    """
    Validate that paths are accessible before running file-based operations.

    [v1.0] Use this tool to check path accessibility before attempting
    file-based operations. Essential for Docker deployments where volume
    mounts must be configured correctly.

    Key capabilities:
    - Check if files are accessible from the MCP server
    - Detect Docker environment automatically
    - Provide actionable suggestions for fixing path issues
    - Report detected workspace roots
    - Generate Docker volume mount commands

    Why AI agents need this:
    - Prevent failures: Check paths before expensive operations
    - Debug deployment: Understand why paths aren't accessible
    - Guide users: Provide specific Docker mount commands
    - Environment awareness: Know if running in Docker vs local

    Common scenarios:
    - Before extract_code: Validate file exists and is accessible
    - Before crawl_project: Check project root is mounted
    - Troubleshooting: Help users configure Docker volumes

    Example::

        result = await validate_paths([
            "/home/user/project/main.py",
            "/nonexistent/file.py",
            "utils/helpers.py"
        ])

        # Returns PathValidationResult:
        # - success: False (not all paths accessible)
        # - accessible: ["/home/user/project/main.py", "utils/helpers.py"]
        # - inaccessible: ["/nonexistent/file.py"]
        # - suggestions: [
        #     "File not found: /nonexistent/file.py",
        #     "Check if the path exists and is spelled correctly"
        # ]
        # - workspace_roots: ["/home/user/project"]
        # - is_docker: False

        # In Docker environment:
        docker_result = await validate_paths(["/app/src/main.py"])
        # - is_docker: True
        # - suggestions: [
        #     "Running in Docker: Mount your project with -v /path/to/project:/app",
        #     "Example: docker run -v $(pwd):/app code-scalpel:latest"
        # ]

    Args:
        paths: List of file paths to validate
        project_root: Optional explicit project root directory

    Returns:
        PathValidationResult with accessible, inaccessible, suggestions, workspace_roots, is_docker
    """
    # [20251225_FEATURE] Tier-aware invocation
    tier = _get_current_tier()
    capabilities = get_tool_capabilities("validate_paths", tier) or {}
    return await asyncio.to_thread(
        _validate_paths_sync, paths, project_root, tier, capabilities
    )


# ============================================================================
# POLICY VERIFICATION TOOL
# ============================================================================


# [20250108_FEATURE] v2.5.0 Guardian - Policy verification models
class PolicyVerificationResult(BaseModel):
    """Result of cryptographic policy verification."""

    success: bool = Field(description="Whether all policy files verified successfully")
    manifest_valid: bool = Field(
        default=False, description="Whether manifest signature is valid"
    )
    files_verified: int = Field(
        default=0, description="Number of files successfully verified"
    )
    files_failed: list[str] = Field(
        default_factory=list, description="List of files that failed verification"
    )
    error: str | None = Field(
        default=None, description="Error message if verification failed"
    )
    error_code: str | None = Field(
        default=None,
        description=(
            "Stable, machine-readable error code for failures (non-breaking additive field)"
        ),
    )
    manifest_source: str | None = Field(
        default=None, description="Source of the policy manifest"
    )
    policy_dir: str | None = Field(
        default=None, description="Policy directory that was verified"
    )
    # [20251226_FEATURE] Tier-specific outputs
    tier: str = Field(default="community", description="Tier used for verification")
    signature_validated: bool = Field(
        default=False, description="Pro+: Whether HMAC signature was validated"
    )
    tamper_detection_enabled: bool = Field(
        default=False, description="Pro+: Whether tamper detection is active"
    )
    audit_log_entry: dict | None = Field(
        default=None, description="Enterprise: Audit log entry for this verification"
    )


def _verify_policy_integrity_sync(
    policy_dir: str | None = None,
    manifest_source: str = "file",
    tier: str | None = None,
    capabilities: dict | None = None,
) -> PolicyVerificationResult:
    """
    Synchronous implementation of policy integrity verification.

    [20250108_FEATURE] v2.5.0 Guardian - Cryptographic verification
    [20251220_BUGFIX] v3.0.5 - Consolidated imports inside try block
    [20251226_FEATURE] v3.3.0 - Tier-based feature gating:
      - Community: Basic policy file existence and format checking
      - Pro: HMAC-SHA256 signature validation, tamper detection
      - Enterprise: Full cryptographic verification with audit logging
    """
    from datetime import datetime

    from code_scalpel.licensing.features import get_tool_capabilities

    dir_path = policy_dir or ".code-scalpel"

    # Resolve tier
    if tier is None:
        tier = _get_current_tier()

    # Resolve capabilities
    if capabilities is None:
        capabilities = get_tool_capabilities("verify_policy_integrity", tier) or {}

    caps_set: set[str] = set((capabilities.get("capabilities", []) or []))
    limits = capabilities.get("limits", {}) or {}

    # Feature flags based on tier
    signature_validation_enabled = limits.get("signature_validation", False)
    tamper_detection_enabled = limits.get("tamper_detection", False)
    audit_logging_enabled = "audit_logging" in caps_set

    # Initialize result
    result = PolicyVerificationResult(
        success=False,
        manifest_source=manifest_source,
        policy_dir=dir_path,
        tier=tier,
        signature_validated=False,
        tamper_detection_enabled=tamper_detection_enabled,
        audit_log_entry=None,
        error_code=None,
    )

    try:
        # Community tier: Basic policy file existence and format checking
        if "basic_verification" in caps_set:
            import json
            from pathlib import Path

            import yaml

            policy_path = Path(dir_path)
            if not policy_path.exists():
                result.error = f"Policy directory not found: {dir_path}"
                result.error_code = "POLICY_DIR_NOT_FOUND"
                return result

            # Check for policy files
            policy_files = []
            for ext in ["*.yaml", "*.yml", "*.json"]:
                policy_files.extend(policy_path.glob(ext))

            # [20260103_BUGFIX] Exclude manifest file from policy file count
            policy_files = [
                pf for pf in policy_files if pf.name != "policy.manifest.json"
            ]

            # [20260110_BUGFIX] Deterministic ordering to prevent flaky outputs/tests
            policy_files = sorted(policy_files, key=lambda p: p.name)

            if not policy_files:
                result.error = f"No policy files found in {dir_path}"
                result.error_code = "NO_POLICY_FILES"
                return result

            # [20260103_FEATURE] Check tier limits for max_policy_files
            from code_scalpel.licensing.config_loader import get_tool_limits

            tier_limits = get_tool_limits("verify_policy_integrity", tier)
            max_files = tier_limits.get("max_policy_files")

            if max_files is not None and len(policy_files) > max_files:
                result.error = (
                    f"Policy file limit exceeded: {len(policy_files)} files found, "
                    f"{max_files} allowed for {tier} tier."
                )
                result.success = False
                result.error_code = "POLICY_FILE_LIMIT_EXCEEDED"
                return result

            # Validate format of each file
            # [20251230_FIX][policy] Sanitize error strings to avoid emitting raw control characters
            # that can break JSON/MCP transport (e.g., fixtures containing \x13).
            def _safe_error_text(text: str) -> str:
                out: list[str] = []
                for ch in text:
                    o = ord(ch)
                    if o < 32 and ch not in ("\n", "\r", "\t"):
                        out.append(f"\\x{o:02x}")
                    else:
                        out.append(ch)
                return "".join(out)

            files_verified = 0
            files_failed = []
            for pf in policy_files:
                try:
                    content = pf.read_text()
                    if pf.suffix == ".json":
                        json.loads(content)
                    else:
                        yaml.safe_load(content)
                    files_verified += 1
                except Exception as e:
                    files_failed.append(f"{pf.name}: {_safe_error_text(str(e))}")

            result.files_verified = files_verified
            result.files_failed = files_failed

            # If no signature validation, basic check passes if files are valid
            if not signature_validation_enabled:
                result.success = len(files_failed) == 0
                if files_failed:
                    result.error = f"Invalid policy files: {', '.join(files_failed)}"
                    result.error_code = "POLICY_PARSE_ERROR"
                return result

        # Pro/Enterprise tier: Full cryptographic verification
        if "signature_validation" in caps_set and signature_validation_enabled:
            from code_scalpel.policy_engine.crypto_verify import (
                CryptographicPolicyVerifier,
                SecurityError,
            )

            try:
                verifier = CryptographicPolicyVerifier(
                    manifest_source=manifest_source,
                    policy_dir=dir_path,
                )

                crypto_result = verifier.verify_all_policies()

                result.success = crypto_result.success
                result.manifest_valid = crypto_result.manifest_valid
                result.files_verified = crypto_result.files_verified
                result.files_failed = crypto_result.files_failed
                result.error = crypto_result.error
                result.signature_validated = crypto_result.manifest_valid

            except SecurityError as e:
                result.success = False
                result.error = str(e)
                result.error_code = "SECURITY_ERROR"
                result.signature_validated = False

        # Enterprise tier: Audit logging
        if audit_logging_enabled and "full_integrity_check" in caps_set:
            result.audit_log_entry = {
                "timestamp": datetime.now().isoformat(),
                "action": "policy_verification",
                "policy_dir": dir_path,
                "manifest_source": manifest_source,
                "success": result.success,
                "files_verified": result.files_verified,
                "files_failed": result.files_failed,
                "signature_validated": result.signature_validated,
                "tier": tier,
            }

        return result

    except ImportError as e:
        result.error = f"Policy engine not available: {str(e)}."
        result.error_code = "POLICY_ENGINE_UNAVAILABLE"
        return result
    except Exception as e:
        # Handle SecurityError and other exceptions
        result.error = f"Verification failed: {str(e)}."
        result.error_code = "INTERNAL_ERROR"
        return result


@mcp.tool()
async def verify_policy_integrity(
    policy_dir: str | None = None,
    manifest_source: str = "file",
) -> PolicyVerificationResult:
    """
    Verify policy file integrity using cryptographic signatures.

    [v2.5.0] Use this tool to verify that policy files have not been tampered
    with since they were signed. This is essential for tamper-resistant
    governance in enterprise deployments.

    **Security Model: FAIL CLOSED**
    - Missing manifest  DENY ALL
    - Invalid signature  DENY ALL
    - Hash mismatch  DENY ALL

    **How it works:**
    1. Load policy manifest from configured source (git, env, file)
    2. Verify HMAC-SHA256 signature using secret key
    3. Verify SHA-256 hash of each policy file matches manifest
    4. Any failure results in security error

    **Bypass Prevention:**
    This addresses the 3rd party review feedback that file permissions
    (chmod 0444) can be bypassed. Even if an agent runs `chmod +w` and
    modifies a policy file, the hash verification will detect the change.

    Key capabilities:
    - Verify manifest signature integrity
    - Detect tampered policy files
    - Detect missing policy files
    - Report detailed verification status
    - Fail closed on any error

    Why AI agents need this:
    - **Trust Verification:** Confirm policies haven't been modified
    - **Audit Trail:** Verify policy integrity before operations
    - **Security Compliance:** Meet enterprise security requirements

    Example:
        # Verify policy integrity before operations
        result = verify_policy_integrity(policy_dir=".code-scalpel")

        if not result.success:
            print(f"SECURITY: {result.error}")
            # Fail closed - do not proceed
        else:
            print(f"Verified {result.files_verified} policy files")

    Args:
        policy_dir: Directory containing policy files (default: .code-scalpel)
        manifest_source: Where to load manifest from - "git", "env", or "file"
            - "git": Load from committed version in git history (most secure)
            - "env": Load from SCALPEL_POLICY_MANIFEST environment variable
            - "file": Load from local policy.manifest.json file

    Returns:
        PolicyVerificationResult with verification status and details

    Note:
        Requires SCALPEL_MANIFEST_SECRET environment variable to be set.
        This secret should be managed by administrators, not agents.

    **Tier-Based Features:**
    - Community: Basic policy file existence and format checking
    - Pro: HMAC-SHA256 signature validation, tamper detection
    - Enterprise: Full cryptographic verification with audit logging
    """
    tier = _get_current_tier()
    capabilities = get_tool_capabilities("verify_policy_integrity", tier) or {}
    return await asyncio.to_thread(
        _verify_policy_integrity_sync, policy_dir, manifest_source, tier, capabilities
    )


# ============================================================================
# CODE POLICY CHECK MCP TOOL
# ============================================================================

# [20251226_FEATURE] v3.4.0 - Code policy check MCP tool


class CodePolicyCheckResult(BaseModel):
    """Result model for code_policy_check tool."""

    success: bool = Field(description="Whether check passed (no critical violations)")
    files_checked: int = Field(description="Number of files analyzed")
    rules_applied: int = Field(description="Number of rules applied")
    summary: str = Field(description="Human-readable summary")
    tier: str = Field(default="community", description="Current tier level")

    # [20260111_FEATURE] Output metadata for transparency
    tier_applied: str = Field(
        default="community",
        description="Tier used for this analysis (community/pro/enterprise)",
    )
    files_limit_applied: int | None = Field(
        default=None,
        description="Max files limit applied (None=unlimited for Enterprise)",
    )
    rules_limit_applied: int | None = Field(
        default=None,
        description="Max rules limit applied (None=unlimited for Enterprise)",
    )

    # Core violations (all tiers)
    violations: list[dict[str, Any]] = Field(
        default_factory=list, description="List of policy violations found"
    )

    # Pro tier fields
    best_practices_violations: list[dict[str, Any]] = Field(
        default_factory=list, description="Best practice violations (Pro tier)"
    )
    security_warnings: list[dict[str, Any]] = Field(
        default_factory=list, description="Security warnings (Pro tier)"
    )
    custom_rule_results: dict[str, list[dict[str, Any]]] = Field(
        default_factory=dict, description="Custom rule matches (Pro tier)"
    )

    # Enterprise tier fields
    compliance_reports: dict[str, dict[str, Any]] = Field(
        default_factory=dict, description="Compliance audit reports (Enterprise tier)"
    )
    compliance_score: float = Field(
        default=0.0, description="Overall compliance score 0-100 (Enterprise tier)"
    )
    certifications: list[dict[str, Any]] = Field(
        default_factory=list, description="Generated certifications (Enterprise tier)"
    )
    audit_trail: list[dict[str, Any]] = Field(
        default_factory=list, description="Audit log entries (Enterprise tier)"
    )
    pdf_report: str | None = Field(
        default=None, description="Base64-encoded PDF report (Enterprise tier)"
    )

    error: str | None = Field(default=None, description="Error message if failed")


def _code_policy_check_sync(
    paths: list[str],
    rules: list[str] | None,
    compliance_standards: list[str] | None,
    generate_report: bool,
    tier: str,
    capabilities: dict[str, Any],
) -> CodePolicyCheckResult:
    """
    Synchronous implementation of code_policy_check.

    [20251226_FEATURE] Core implementation with tier-based feature gating.
    """
    from code_scalpel.policy_engine.code_policy_check import CodePolicyChecker

    # Create checker with tier
    checker = CodePolicyChecker(
        tier=tier,
        compliance_standards=compliance_standards,
    )

    # Run the check
    result = checker.check_files(
        paths=paths,
        rules=rules,
        generate_report=generate_report and tier == "enterprise",
    )

    # [20260111_FEATURE] Get tier limits for metadata
    limits = capabilities.get("limits", {})
    files_limit = limits.get("max_files")
    rules_limit = limits.get("max_rules")

    # Convert to MCP result model
    mcp_result = CodePolicyCheckResult(
        success=result.success,
        files_checked=result.files_checked,
        rules_applied=result.rules_applied,
        summary=result.summary,
        tier=tier,
        # [20260111_FEATURE] Output metadata for transparency
        tier_applied=tier,
        files_limit_applied=files_limit,
        rules_limit_applied=rules_limit,
        violations=[
            cast(dict[str, Any], v.to_dict() if hasattr(v, "to_dict") else v)
            for v in result.violations
        ],
        error=result.error,
    )

    # Add Pro tier fields if available
    set(capabilities.get("capabilities", []))

    if tier in ("pro", "enterprise"):
        mcp_result.best_practices_violations = [
            cast(dict[str, Any], v.to_dict() if hasattr(v, "to_dict") else v)
            for v in result.best_practices_violations
        ]
        mcp_result.security_warnings = [
            cast(dict[str, Any], w.to_dict() if hasattr(w, "to_dict") else w)
            for w in result.security_warnings
        ]
        mcp_result.custom_rule_results = result.custom_rule_results

    # Add Enterprise tier fields if available
    if tier == "enterprise":
        mcp_result.compliance_reports = {
            k: v.to_dict() for k, v in result.compliance_reports.items()
        }
        mcp_result.compliance_score = result.compliance_score
        mcp_result.certifications = [c.to_dict() for c in result.certifications]
        mcp_result.audit_trail = [e.to_dict() for e in result.audit_trail]
        mcp_result.pdf_report = result.pdf_report

    return mcp_result


@mcp.tool()
async def code_policy_check(
    paths: list[str],
    rules: list[str] | None = None,
    compliance_standards: list[str] | None = None,
    generate_report: bool = False,
) -> CodePolicyCheckResult:
    """
    Check code against style guides, best practices, and compliance standards.

    [20251226_FEATURE] v3.4.0 - Code policy enforcement engine.

    This tool provides tier-based code policy checking:

    **Community Tier:**
    - Style guide checking (PEP8 validation)
    - Basic anti-pattern detection (bare except, mutable defaults, star imports)
    - Limited to 100 files, 50 rules

    **Pro Tier:**
    - All Community features
    - Best practice analysis (type hints, docstrings, function length)
    - Async/await pattern detection (blocking calls, missing await)
    - Security pattern detection (hardcoded secrets, SQL injection patterns)
    - Custom rule support
    - Unlimited files and rules

    **Enterprise Tier:**
    - All Pro features
    - Compliance auditing (HIPAA, SOC2, GDPR, PCI-DSS)
    - PDF compliance certification generation
    - Audit trail for compliance tracking
    - Compliance scoring (0-100%)

    Example usage:

        # Basic style check (Community)
        result = code_policy_check(paths=["src/"])

        # With specific rules
        result = code_policy_check(
            paths=["src/"],
            rules=["PY001", "PY002", "SEC001"]
        )

        # Compliance audit (Enterprise)
        result = code_policy_check(
            paths=["src/"],
            compliance_standards=["hipaa", "soc2"],
            generate_report=True
        )

    Args:
        paths: List of file paths or directories to check
        rules: Optional list of rule IDs to apply (None = all rules for tier)
        compliance_standards: Compliance standards to audit (Enterprise tier)
            Options: "hipaa", "soc2", "gdpr", "pci_dss"
        generate_report: Generate PDF compliance report (Enterprise tier)

    Returns:
        CodePolicyCheckResult with violations, warnings, and compliance data

    **Detected Patterns:**

    Community (PY001-PY010):
    - PY001: Bare except clause
    - PY002: Mutable default argument
    - PY003: Global statement
    - PY004: Star import
    - PY005: Assert statement
    - PY006: exec() usage
    - PY007: eval() usage
    - PY008: type() comparison
    - PY009: Empty except block
    - PY010: input() for passwords

    Pro (SEC001-SEC010, ASYNC001-ASYNC005, BP001-BP007):
    - SEC001: Hardcoded password
    - SEC002: SQL string concatenation
    - SEC003: os.system() usage
    - SEC004: subprocess shell=True
    - SEC005: pickle usage
    - SEC006: yaml.load without Loader
    - SEC007: Hardcoded IP address
    - SEC008: Insecure SSL
    - SEC009: Debug mode enabled
    - SEC010: Weak hash (MD5/SHA1)
    - ASYNC001: Missing await
    - ASYNC002: Blocking call in async
    - ASYNC003: Nested asyncio.run
    - ASYNC004: Unhandled task
    - ASYNC005: Async generator cleanup
    - BP001: Missing type hints
    - BP002: Missing docstring
    - BP003: Too many arguments
    - BP004: Function too long
    - BP005: Nested too deep
    - BP006: File without context manager
    - BP007: Magic number

    Enterprise (HIPAA, SOC2, GDPR, PCI-DSS):
    - HIPAA001-003: PHI handling violations
    - SOC2001-003: Security control violations
    - GDPR001-003: Data protection violations
    - PCI001-003: Payment data violations
    """
    tier = _get_current_tier()
    capabilities = get_tool_capabilities("code_policy_check", tier)

    # Apply tier limits
    limits = capabilities.get("limits", {})
    limits.get("max_files")

    # If compliance_standards requested but not enterprise tier, filter out
    if compliance_standards and tier != "enterprise":
        compliance_standards = None

    # If generate_report requested but not enterprise tier, disable
    if generate_report and tier != "enterprise":
        generate_report = False

    return await asyncio.to_thread(
        _code_policy_check_sync,
        paths,
        rules,
        compliance_standards,
        generate_report,
        tier,
        capabilities,
    )


# ============================================================================
# ENTRYPOINT
# ============================================================================


def run_server(
    transport: str = "stdio",
    host: str = "127.0.0.1",
    port: int = 8080,
    allow_lan: bool = False,
    root_path: str | None = None,
    tier: str | None = None,
    ssl_certfile: str | None = None,
    ssl_keyfile: str | None = None,
    auto_init: bool = False,
    auto_init_mode: str | None = None,
    auto_init_target: str | None = None,
):
    """
    Run the Code Scalpel MCP server.

    Args:
        transport: Transport type - "stdio" or "streamable-http"
        host: Host to bind to (HTTP only)
        port: Port to bind to (HTTP only)
        allow_lan: Allow connections from LAN (disables host validation)
        root_path: Project root directory (default: current directory)
        tier: Tool tier (community, pro, enterprise). Defaults to env vars or enterprise.
        ssl_certfile: Path to SSL certificate file for HTTPS (optional)
        ssl_keyfile: Path to SSL private key file for HTTPS (optional)

    Security Note:
        By default, the HTTP transport only allows connections from localhost.
        Use --allow-lan to enable LAN access. This disables DNS rebinding
        protection and allows connections from any host. Only use on trusted
        networks.

    HTTPS Note:
        [20251215_FEATURE] For production deployments (especially with Claude API),
        provide ssl_certfile and ssl_keyfile to enable HTTPS. Both must be specified
        for HTTPS to be enabled.
    """
    # [20251215_BUGFIX] Configure logging to stderr before anything else
    _configure_logging(transport)

    # [20251228_FEATURE] Optional CRL fetch/cache/refresh.
    # This is opt-in to preserve offline-first posture.
    from code_scalpel.licensing.crl_fetcher import (
        ensure_crl_available,
        start_crl_refresh_thread,
    )
    from code_scalpel.licensing.runtime_revalidator import (
        start_license_revalidation_thread,
    )

    ensure_crl_available()
    start_crl_refresh_thread(interval_seconds=3600)
    # [20251228_FEATURE] Revalidate the license at most once per 24h.
    start_license_revalidation_thread(interval_seconds=24 * 60 * 60)

    # [20251227_SECURITY] Tier selection is authorization-based.
    # - The license determines the maximum allowed tier.
    # - CLI/env can only request a tier <= licensed tier.
    # - If Pro/Enterprise is requested without a valid license, fail closed.
    from code_scalpel.licensing.authorization import compute_effective_tier_for_startup

    requested_tier = (
        tier or os.environ.get("CODE_SCALPEL_TIER") or os.environ.get("SCALPEL_TIER")
    )
    validator = JWTLicenseValidator()
    effective_tier, startup_warning = compute_effective_tier_for_startup(
        requested_tier=requested_tier,
        validator=validator,
    )

    tier = effective_tier
    if tier == "free":
        tier = "community"
    if tier == "all":
        tier = "enterprise"
    if tier not in {"community", "pro", "enterprise"}:
        raise ValueError(
            "Invalid tier. Expected one of: community, pro, enterprise "
            "(or set CODE_SCALPEL_TIER/SCALPEL_TIER)"
        )

    # Expose tier to the response envelope wrapper.
    global CURRENT_TIER
    CURRENT_TIER = tier

    # [20251227_BUGFIX] All 22 tools available at all tiers - no filtering
    # Feature restrictions (file size, language support, etc.) enforced within tools via limits.toml
    _apply_tier_tool_filter(tier)

    global PROJECT_ROOT
    if root_path:
        PROJECT_ROOT = Path(root_path).resolve()
        if not PROJECT_ROOT.exists():
            # Use stderr for warnings to avoid corrupting stdio transport
            print(
                f"Warning: Root path {PROJECT_ROOT} does not exist. Using current directory.",
                file=sys.stderr,
            )
            PROJECT_ROOT = Path.cwd()

    # [20251215_BUGFIX] Print to stderr for stdio transport
    output = sys.stderr if transport == "stdio" else sys.stdout
    print(f"Code Scalpel MCP Server v{__version__}", file=output)
    print(f"Project Root: {PROJECT_ROOT}", file=output)
    print(f"Tier: {tier.title()}", file=output)

    # [20251230_FEATURE] Optional auto-init of `.code-scalpel/` at startup.
    # This is opt-in because it writes files.
    init_result = _maybe_auto_init_config_dir(
        project_root=PROJECT_ROOT,
        tier=tier,
        enabled=auto_init or _env_truthy(os.environ.get("SCALPEL_AUTO_INIT")),
        mode=auto_init_mode,
        target=auto_init_target,
    )
    if init_result is not None and init_result.get("created"):
        print(
            "Auto-init: created .code-scalpel "
            f"(target={init_result.get('target')}, mode={init_result.get('mode')})",
            file=output,
        )

    if startup_warning:
        print(startup_warning, file=output)

    # [20251227_FEATURE] Log license source for troubleshooting (never log token).
    license_file = validator.find_license_file()
    if license_file:
        print(f"License File: {license_file}", file=output)

    # [20251215_FEATURE] SSL/HTTPS support for production deployments
    use_https = ssl_certfile and ssl_keyfile
    if use_https:
        print(f"SSL/TLS: ENABLED (cert: {ssl_certfile})", file=output)
    else:
        if transport in ("streamable-http", "sse"):
            print(
                "SSL/TLS: DISABLED (use --ssl-cert and --ssl-key for HTTPS)",
                file=output,
            )

    if transport == "streamable-http" or transport == "sse":
        from mcp.server.transport_security import TransportSecuritySettings

        mcp.settings.host = host
        mcp.settings.port = port

        # [20251215_FEATURE] Configure SSL if certificates provided
        if use_https:
            # Use setattr for optional SSL settings that may not be in all FastMCP versions
            setattr(mcp.settings, "ssl_certfile", ssl_certfile)
            setattr(mcp.settings, "ssl_keyfile", ssl_keyfile)
            protocol = "https"
        else:
            protocol = "http"

        if allow_lan or host == "0.0.0.0":  # nosec B104
            # [20251218_SECURITY] Intentional LAN binding for server functionality (B104)
            # Disable host validation for LAN access
            # WARNING: Only use on trusted networks!
            mcp.settings.transport_security = TransportSecuritySettings(
                enable_dns_rebinding_protection=False,
                allowed_hosts=["*"],
                allowed_origins=["*"],
            )
            print("WARNING: LAN access enabled. Host validation disabled.", file=output)
            print("Only use on trusted networks!", file=output)

        # FastMCP defaults: streamable-http mounts at /mcp and SSE mounts at /sse
        # (see mcp.server.fastmcp.server.FastMCP settings)
        endpoint_path = "/mcp" if transport == "streamable-http" else "/sse"
        print(f"MCP endpoint: {protocol}://{host}:{port}{endpoint_path}", file=output)

        # [20251215_FEATURE] Register HTTP health endpoint for Docker health checks
        _register_http_health_endpoint(mcp, host, port, ssl_certfile, ssl_keyfile)

        mcp.run(transport=transport)
    else:
        mcp.run()


def _apply_tier_tool_filter(tier: str) -> None:
    """[20251226_BUGFIX] ALL tools available at ALL tiers.

    Tier system provides ALL 22 tools at every tier (Community, Pro, Enterprise).
    Feature restrictions (file size limits, language support, depth limits, etc.)
    are enforced within each tool's implementation via the limits.toml configuration.

    Previous implementation incorrectly removed tools entirely from lower tiers.
    Correct behavior: All tools always available, feature limits vary by tier.
    """
    # [20251226_BUGFIX] No tool filtering - all tools available at all tiers
    # Feature restrictions are enforced via limits.toml within each tool
    pass


def _register_http_health_endpoint(
    mcp_instance,
    host: str,
    port: int,
    ssl_certfile: str | None = None,
    ssl_keyfile: str | None = None,
):
    """
    Register a simple HTTP/HTTPS /health endpoint for Docker health checks.

    [20251215_FEATURE] v2.0.0 - HTTP health endpoint that returns immediately.
    [20251215_FEATURE] HTTPS support for production deployments.

    This endpoint is separate from the MCP protocol and provides a simple
    200 OK response for container orchestration health checks.
    """
    import json
    import threading
    from http.server import BaseHTTPRequestHandler, HTTPServer

    use_https = ssl_certfile and ssl_keyfile

    class HealthHandler(BaseHTTPRequestHandler):
        def do_GET(self):
            if self.path == "/health":
                self.send_response(200)
                self.send_header("Content-Type", "application/json")
                self.end_headers()
                response = json.dumps(
                    {
                        "status": "healthy",
                        "version": __version__,
                        "transport": "https" if use_https else "http",
                    }
                )
                self.wfile.write(response.encode())
            else:
                # Let other paths fall through to MCP handler
                self.send_response(404)
                self.end_headers()

        def log_message(self, format, *args):
            # Suppress HTTP access logs to stderr
            pass

    def run_health_server():
        # Run on a different port (health_port = mcp_port + 1)
        health_port = port + 1
        try:
            server = HTTPServer((host, health_port), HealthHandler)

            # [20251215_FEATURE] Wrap with SSL if certificates provided
            if use_https:
                import ssl

                ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)
                # ssl_certfile and ssl_keyfile are guaranteed non-None when use_https is True
                assert ssl_certfile is not None and ssl_keyfile is not None
                ssl_context.load_cert_chain(ssl_certfile, ssl_keyfile)
                server.socket = ssl_context.wrap_socket(server.socket, server_side=True)
                protocol = "https"
            else:
                protocol = "http"

            logger.info(
                f"Health endpoint available at {protocol}://{host}:{health_port}/health"
            )
            server.serve_forever()
        except Exception as e:
            logger.warning(f"Could not start health server: {e}")

    # Start health server in background thread
    health_thread = threading.Thread(target=run_health_server, daemon=True)
    health_thread.start()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Code Scalpel MCP Server")
    parser.add_argument(
        "--transport",
        choices=["stdio", "streamable-http", "sse"],
        default="stdio",
        help="Transport type (default: stdio)",
    )
    parser.add_argument(
        "--host",
        default="127.0.0.1",
        help="Host to bind to (HTTP only, default: 127.0.0.1)",
    )
    parser.add_argument(
        "--port",
        type=int,
        default=8080,
        help="Port to bind to (HTTP only, default: 8080)",
    )
    parser.add_argument(
        "--allow-lan",
        action="store_true",
        help="Allow LAN connections (disables host validation, use on trusted networks only)",
    )
    parser.add_argument(
        "--root",
        help="Project root directory for resources (default: current directory)",
    )
    parser.add_argument(
        "--tier",
        choices=["community", "pro", "enterprise"],
        default=None,
        help="Tool tier (default: enterprise or CODE_SCALPEL_TIER/SCALPEL_TIER)",
    )
    # [20251215_FEATURE] SSL/TLS support for HTTPS
    parser.add_argument(
        "--ssl-cert",
        help="Path to SSL certificate file for HTTPS (required for production/Claude)",
    )
    parser.add_argument(
        "--ssl-key",
        help="Path to SSL private key file for HTTPS (required for production/Claude)",
    )

    # [20251230_FEATURE] Optional auto-init of `.code-scalpel/` at startup.
    parser.add_argument(
        "--auto-init",
        action="store_true",
        help="Create .code-scalpel directory on startup (opt-in; writes files)",
    )
    parser.add_argument(
        "--auto-init-mode",
        choices=["full", "templates_only"],
        default=None,
        help="Auto-init mode: full (writes .env + manifest) or templates_only",
    )

    parser.add_argument(
        "--auto-init-target",
        choices=["project", "user"],
        default=None,
        help=(
            "Where to create `.code-scalpel/` when auto-init is enabled: "
            "project (PROJECT_ROOT/cwd) or user (XDG config home)."
        ),
    )

    args = parser.parse_args()
    run_server(
        transport=args.transport,
        host=args.host,
        port=args.port,
        allow_lan=args.allow_lan,
        root_path=args.root,
        tier=args.tier,
        ssl_certfile=args.ssl_cert,
        ssl_keyfile=args.ssl_key,
        auto_init=args.auto_init,
        auto_init_mode=args.auto_init_mode,
        auto_init_target=args.auto_init_target,
    )


def main_renamed():
    pass


def main():
    pass
