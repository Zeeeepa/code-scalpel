# Large Output Handling Strategy for MCP Tools

**Created**: January 4, 2026  
**Context**: Analysis of get_project_map Enterprise output (6.8 MB)  
**Status**: DESIGN COMPLETE - READY FOR IMPLEMENTATION

---

## Problem Statement

MCP tools can produce massive outputs (> 1 MB) that are:
- **Too large for chat display** (token/character limits)
- **Difficult to work with** (cannot be copy-pasted)
- **Not efficiently queryable** (monolithic JSON)
- **Problematic for streaming** (long transfer times)

**Example**: `get_project_map` on 2,028-file project â†’ 6.8 MB output

---

## Decision Tree for Output Size

```
Tool execution complete
    â”‚
    â”œâ”€ Output size < 100 KB
    â”‚  â””â”€ Return in chat (normal response)
    â”‚
    â”œâ”€ Output size 100 KB - 1 MB
    â”‚  â””â”€ Return in chat + mention file saved
    â”‚
    â”œâ”€ Output size 1 MB - 10 MB
    â”‚  â”œâ”€ Output to file automatically
    â”‚  â”œâ”€ Return summary + file path
    â”‚  â””â”€ (CURRENT: get_project_map Enterprise)
    â”‚
    â””â”€ Output size > 10 MB
       â”œâ”€ Chunk into multiple files
       â”œâ”€ Provide index file
       â””â”€ Return navigation guide
```

---

## Implementation Options

### Option 1: Automatic File Output (RECOMMENDED for v1)

**When to use**: Project with 500+ files

**Behavior**:
```python
def get_project_map(project_root, ...):
    result = analyze_project(project_root)
    
    # Check output size
    output_json = json.dumps(result)
    size_mb = len(output_json) / (1024 * 1024)
    
    if size_mb > 1:  # Automatic file output
        output_file = f"project_map_{timestamp}.json"
        with open(output_file, 'w') as f:
            f.write(output_json)
        
        return {
            "status": "success",
            "file": output_file,
            "size_mb": size_mb,
            "summary": {
                "total_files": result["total_files"],
                "packages": len(result["packages"]),
                "entry_points": len(result["entry_points"])
            }
        }
    else:
        return result  # Return in chat as normal
```

**Advantages**:
- âœ… Simple to implement
- âœ… No breaking changes
- âœ… Works immediately for large projects
- âœ… Users still see summary in chat

**Disadvantages**:
- âŒ Still monolithic files (not chunked)
- âŒ No performance improvement (16+ min still slow)
- âŒ Limited queryability

---

### Option 2: Chunked Output with Index (RECOMMENDED for v2)

**When to use**: Projects > 1,000 files

**Structure**:
```
project_map_20260104_1200/
â”œâ”€â”€ index.json                    (navigation + metadata)
â”œâ”€â”€ summary.json                  (statistics only)
â”œâ”€â”€ packages_0.json              (packages 0-99)
â”œâ”€â”€ packages_1.json              (packages 100-199)
â”œâ”€â”€ modules_0.json               (modules 0-999)
â”œâ”€â”€ modules_1.json               (modules 1000-1999)
â”œâ”€â”€ modules_2.json               (modules 2000-2028)
â”œâ”€â”€ dependencies.json            (all import relationships)
â”œâ”€â”€ git_history.json             (commit activity)
â””â”€â”€ diagram.mmd                  (architecture diagram)
```

**Index file example**:
```json
{
  "version": "1.0",
  "generated": "2026-01-04T12:00:00Z",
  "total_size_mb": 6.8,
  "execution_time_seconds": 1016,
  "chunks": {
    "packages": {
      "total": 127,
      "files": ["packages_0.json", "packages_1.json"],
      "items_per_file": 100
    },
    "modules": {
      "total": 2028,
      "files": ["modules_0.json", "modules_1.json", "modules_2.json"],
      "items_per_file": 1000
    }
  },
  "quick_stats": {
    "total_files": 2028,
    "languages": {"python": 2028, "json": 518, ...},
    "complexity_avg": 7.2,
    "stability_score": 0.57
  },
  "entry_points": [
    "src/code_scalpel/__main__.py:main",
    "tests/run_tests.py:main"
  ]
}
```

**Advantages**:
- âœ… Modular and organized
- âœ… Can load specific chunks only
- âœ… Easy to version control (diffs per chunk)
- âœ… Supports incremental updates
- âœ… Better for pagination

**Disadvantages**:
- âŒ More complex implementation
- âŒ Requires index parsing first
- âŒ Still doesn't solve performance issue

---

### Option 3: Database Backend (RECOMMENDED for v3+)

**When to use**: Interactive queries needed

**Architecture**:
```
get_project_map()
  â”œâ”€ Create/update SQLite database
  â”œâ”€ Load packages table
  â”œâ”€ Load modules table
  â”œâ”€ Load dependencies table (indexed)
  â”œâ”€ Load git_history table
  â””â”€ Return database path + quick stats

User can then:
  â””â”€ Query for specific packages
  â””â”€ Find all dependents of a module
  â””â”€ Analyze import patterns
  â””â”€ Track file changes
```

**API Example**:
```python
# Get all modules in a package
SELECT * FROM modules WHERE package = 'code_scalpel' LIMIT 100;

# Find circular dependencies
SELECT a, b FROM dependencies WHERE b IN (
  SELECT source FROM dependencies WHERE target = a
);

# Most changed files
SELECT file, change_count FROM git_history 
ORDER BY change_count DESC LIMIT 10;
```

**Advantages**:
- âœ… Interactive queries
- âœ… Efficient filtering
- âœ… Scalable to very large projects
- âœ… Supports incremental updates
- âœ… Natural pagination

**Disadvantages**:
- âŒ Complex implementation
- âŒ Requires schema design
- âŒ More dependencies (sqlite3, etc.)
- âŒ Not human-readable (need query tool)

---

## Phased Implementation Plan

### Phase 1: Immediate (Next Release)
**Goal**: Stop outputting 6.8 MB to chat

**Implementation**:
1. Add size check in all large-output tools
2. Auto-write to file if > 1 MB
3. Return summary in chat instead
4. Document file location for users

**Tools affected**: 
- `get_project_map` (MAIN CASE)
- `crawl_project` (potentially)
- `cross_file_security_scan` (on large projects)

**Effort**: 2-3 hours

---

### Phase 2: Short-term (v1.1)
**Goal**: Improve queryability and structure

**Implementation**:
1. Implement chunked output for > 1 MB
2. Create index files
3. Add `--format` flag (json/chunked/summary)
4. Add progress indicators

**Tools affected**: 
- `get_project_map`
- `crawl_project`

**Effort**: 4-6 hours

---

### Phase 3: Medium-term (v1.2)
**Goal**: Solve performance and interactivity

**Implementation**:
1. Parallel analysis for get_project_map
2. Incremental analysis support
3. Basic query helpers
4. Caching layer

**Tools affected**: 
- `get_project_map` (main optimization target)

**Effort**: 8-12 hours

---

### Phase 4: Long-term (v2.0)
**Goal**: Production-grade solution

**Implementation**:
1. Full database backend
2. Query API
3. Web UI for exploration
4. Export formats (JSON, CSV, etc.)

**Effort**: 20+ hours

---

## Recommendation for code-scalpel Project

### Immediate Action (Next 24 hours)

âœ… **DOCUMENT & DECIDE**: This analysis document is ready to guide decisions

**Key decision**: Should get_project_map auto-output to file?
- **YES**: Implement Phase 1 immediately
- **MAYBE**: Add flag `--auto-file` for testing
- **NO**: Require manual `--output-file` flag

---

## Configuration Approach

### Environment Variable
```bash
export SCALPEL_AUTO_FILE_SIZE_MB=1
# Tool will automatically write to file if output > 1 MB
```

### Command-line Flag
```bash
python -m code_scalpel.tools.get_project_map \
  --project /path/to/code \
  --auto-file              # Enable auto-file output
  --chunk-size 100         # If chunking, chunk at 100 items
  --output-dir ./results   # Where to save files
```

### Configuration File
```toml
# .code-scalpel/limits.toml
[tools.get_project_map]
auto_file_size_mb = 1
chunk_size = 1000
output_format = "chunked"  # json, chunked, database
```

---

## Testing Strategy for Large Output Handling

### Test Cases

1. **Small project** (< 10 files)
   - Verify output goes to chat
   - No file created

2. **Medium project** (100-500 files)
   - Verify output goes to chat + file
   - File can be read and parsed

3. **Large project** (500+ files)
   - Verify auto-file output works
   - Verify summary is accurate
   - Verify file is valid JSON/chunks

4. **Chunked output**
   - Verify all chunks can be loaded
   - Verify index is accurate
   - Verify no data loss

5. **Performance**
   - Measure analysis time vs project size
   - Identify bottlenecks
   - Profile memory usage

---

## Deliverables

âœ… **Completed**:
- [x] Output analysis (ENTERPRISE_OUTPUT_ANALYSIS.md)
- [x] Implementation strategy (this document)
- [x] Decision framework
- [x] Phased implementation plan

ðŸ“‹ **Next Steps**:
- [ ] Implement Phase 1 (auto-file output)
- [ ] Add size warnings
- [ ] Update tool documentation
- [ ] Add test cases
- [ ] Monitor actual usage

---

## Summary Table

| Aspect | Current | Phase 1 | Phase 2 | Phase 3+ |
|--------|---------|---------|---------|----------|
| Output Method | Chat | File | Chunked | Database |
| Max Size | 6.8 MB | 1 MB per file | 100 KB per file | Unlimited |
| Query Support | None | grep only | Index queries | SQL |
| Performance | 16+ min | 16+ min | < 1 min | < 1 min |
| Scalability | Limited | 5K files | 50K files | 1M+ files |
| Implementation Effort | - | 2-3 hrs | 4-6 hrs | 20+ hrs |

---

**Status**: ANALYSIS COMPLETE & ACTIONABLE  
**Ready for**: Team discussion and Phase 1 implementation planning
