# MCP Tools (Current)

This document is generated from the current MCP tool registry shipped by this repo.

- Generated by: `scripts/generate_mcp_tools_reference.py`
- Tool count: **20**

## Index

- [analyze_code](#analyze-code)
- [crawl_project](#crawl-project)
- [cross_file_security_scan](#cross-file-security-scan)
- [extract_code](#extract-code)
- [generate_unit_tests](#generate-unit-tests)
- [get_call_graph](#get-call-graph)
- [get_cross_file_dependencies](#get-cross-file-dependencies)
- [get_file_context](#get-file-context)
- [get_graph_neighborhood](#get-graph-neighborhood)
- [get_project_map](#get-project-map)
- [get_symbol_references](#get-symbol-references)
- [scan_dependencies](#scan-dependencies)
- [security_scan](#security-scan)
- [simulate_refactor](#simulate-refactor)
- [symbolic_execute](#symbolic-execute)
- [type_evaporation_scan](#type-evaporation-scan)
- [unified_sink_detect](#unified-sink-detect)
- [update_symbol](#update-symbol)
- [validate_paths](#validate-paths)
- [verify_policy_integrity](#verify-policy-integrity)

## analyze_code

Analyze source code structure.

    Use this tool to understand the high-level architecture (classes, functions, imports)
    of a file before attempting to edit it. This helps prevent hallucinating non-existent
    methods or classes.

    [20251219_BUGFIX] v3.0.4 - Now auto-detects language from code content.

    Example::

        result = await analyze_code('''
        import math

        class Calculator:
            def add(self, a: int, b: int) -> int:
                return a + b

        def helper(x):
            if x > 10:
                return x * 2
            return x
        ''')

        # Returns AnalysisResult:
        # - functions: ["helper"]
        # - classes: ["Calculator"]
        # - imports: ["math"]
        # - complexity_score: 2 (one branch in helper)
        # - has_main: False
        # - lines_of_code: 10

    Args:
        code: Source code to analyze
        language: Language of the code ("auto", "python", "javascript", "typescript", "java")
                  Default "auto" detects from code content.

    Returns:
        AnalysisResult with functions, classes, imports, complexity_score, and metrics

### Input schema

```json
{
  "properties": {
    "code": {
      "title": "Code",
      "type": "string"
    },
    "language": {
      "default": "auto",
      "title": "Language",
      "type": "string"
    }
  },
  "required": [
    "code"
  ],
  "title": "analyze_codeArguments",
  "type": "object"
}
```

## crawl_project

Crawl an entire project directory and analyze all Python files.

    Use this tool to get a comprehensive overview of a project's structure,
    complexity hotspots, and code metrics before diving into specific files.

    [20251215_FEATURE] v2.0.0 - Progress reporting for long-running operations.
    Reports progress as files are discovered and analyzed.

    Example::

        result = await crawl_project(
            root_path="/home/user/myproject",
            complexity_threshold=8,
            include_report=True
        )

        # Returns ProjectCrawlResult:
        # - summary: ProjectSummary(
        #     total_files=42,
        #     total_lines=5680,
        #     total_functions=187,
        #     total_classes=23,
        #     average_complexity=4.2
        # )
        # - files: [CrawlFileResult(path="src/main.py", ...), ...]
        # - complexity_hotspots: [
        #     CrawlFunctionInfo(name="parse_config", complexity=15, lineno=42),
        #     CrawlFunctionInfo(name="process_batch", complexity=12, lineno=156)
        # ]
        # - markdown_report: "# Project Analysis Report

## Summary
..."

        # Find files exceeding complexity threshold
        for hotspot in result.complexity_hotspots:
            print(f"{hotspot.name}: complexity {hotspot.complexity}")

    Args:
        root_path: Path to project root (defaults to current working directory)
        exclude_dirs: Additional directories to exclude (common ones already excluded)
        complexity_threshold: Complexity score that triggers a warning (default: 10)
        include_report: Include a markdown report in the response (default: True)

    Returns:
        ProjectCrawlResult with files, summary stats, complexity_hotspots, and markdown_report

### Input schema

```json
{
  "properties": {
    "complexity_threshold": {
      "default": 10,
      "title": "Complexity Threshold",
      "type": "integer"
    },
    "exclude_dirs": {
      "anyOf": [
        {
          "items": {
            "type": "string"
          },
          "type": "array"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "Exclude Dirs"
    },
    "include_report": {
      "default": true,
      "title": "Include Report",
      "type": "boolean"
    },
    "root_path": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "Root Path"
    }
  },
  "title": "crawl_projectArguments",
  "type": "object"
}
```

## cross_file_security_scan

Perform cross-file security analysis tracking taint flow across module boundaries.

    [v1.5.1] Use this tool to detect vulnerabilities where tainted data crosses
    file boundaries before reaching a dangerous sink. This catches security
    issues that single-file analysis would miss.

    [20251215_FEATURE] v2.0.0 - Progress reporting for long-running operations.
    Reports progress during file discovery and taint analysis phases.

    [20251220_PERF] v3.0.4 - Added timeout and module limits to prevent hanging
    on large codebases with circular imports.

    Key capabilities:
    - Track taint flow through function calls across files
    - Detect vulnerabilities where source and sink are in different files
    - Identify all taint entry points (web inputs, file reads, etc.)
    - Map dangerous sinks (SQL execution, command execution, etc.)
    - Generate taint flow diagrams

    Detects cross-file patterns like:
    - User input in routes.py -> SQL execution in db.py (SQL Injection)
    - Request data in views.py -> os.system() in utils.py (Command Injection)
    - Form input in handlers.py -> open() in storage.py (Path Traversal)

    Why AI agents need this:
    - Defense in depth: Find vulnerabilities that span multiple files
    - Architecture review: Understand how untrusted data flows through the app
    - Code audit: Generate security reports for compliance
    - Risk assessment: Identify highest-risk code paths

    Args:
        project_root: Project root directory (default: server's project root)
        entry_points: Optional list of entry point functions to start from
                     (e.g., ["app.py:main", "routes.py:index"])
                     If None, analyzes all detected entry points
        max_depth: Maximum call depth to trace (default: 5)
        include_diagram: Include Mermaid diagram of taint flows (default: True)
        timeout_seconds: Maximum time in seconds for analysis (default: 120)
                        Set to None for no timeout (not recommended for large projects)
        max_modules: Maximum number of modules to analyze (default: 500)
                    Set to None for no limit (not recommended for large projects)

    Returns:
        CrossFileSecurityResult with vulnerabilities, taint flows, and risk assessment

### Input schema

```json
{
  "properties": {
    "entry_points": {
      "anyOf": [
        {
          "items": {
            "type": "string"
          },
          "type": "array"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "Entry Points"
    },
    "include_diagram": {
      "default": true,
      "title": "Include Diagram",
      "type": "boolean"
    },
    "max_depth": {
      "default": 5,
      "title": "Max Depth",
      "type": "integer"
    },
    "max_modules": {
      "anyOf": [
        {
          "type": "integer"
        },
        {
          "type": "null"
        }
      ],
      "default": 500,
      "title": "Max Modules"
    },
    "project_root": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "Project Root"
    },
    "timeout_seconds": {
      "anyOf": [
        {
          "type": "number"
        },
        {
          "type": "null"
        }
      ],
      "default": 120.0,
      "title": "Timeout Seconds"
    }
  },
  "title": "cross_file_security_scanArguments",
  "type": "object"
}
```

## extract_code

Surgically extract specific code elements (functions, classes, methods).

    **TOKEN-EFFICIENT MODE (RECOMMENDED):**
    Provide `file_path` - the server reads the file directly. The Agent
    never sees the full file content, saving potentially thousands of tokens.

    **MULTI-LANGUAGE SUPPORT (v2.0.0):**
    Supports Python, JavaScript, TypeScript, and Java. Language is auto-detected
    from file extension, or specify explicitly with `language` parameter.

    **CROSS-FILE DEPENDENCIES:**
    Set `include_cross_file_deps=True` to automatically resolve imports.
    If your function uses `TaxRate` from `models.py`, this will extract
    `TaxRate` from `models.py` and include it in the response.

    **LEGACY MODE:**
    Provide `code` as a string - for when you already have code in context.

    Args:
        target_type: Type of element - "function", "class", or "method".
        target_name: Name of the element. For methods, use "ClassName.method_name".
        file_path: Path to the source file (TOKEN SAVER - server reads file).
        code: Source code string (fallback if file_path not provided).
        language: Language override: "python", "javascript", "typescript", "java".
                  If None, auto-detects from file extension.
        include_context: If True, also extract intra-file dependencies.
        context_depth: How deep to traverse dependencies (1=direct, 2=transitive).
        include_cross_file_deps: If True, resolve imports from external files.
        include_token_estimate: If True, include estimated token count.

    Returns:
        ContextualExtractionResult with extracted code and metadata.

    Example (Efficient - Agent sends ~50 tokens, receives ~200):
        extract_code(
            file_path="/project/src/utils.py",
            target_type="function",
            target_name="calculate_tax"
        )

    Example (JavaScript extraction):
        extract_code(
            file_path="/project/src/utils.js",
            target_type="function",
            target_name="calculateTax"
        )

    Example (Java method extraction):
        extract_code(
            file_path="/project/src/Calculator.java",
            target_type="method",
            target_name="Calculator.add"
        )

    Example (With cross-file dependencies):
        extract_code(
            file_path="/project/src/services/order.py",
            target_type="function",
            target_name="process_order",
            include_cross_file_deps=True
        )

### Input schema

```json
{
  "properties": {
    "code": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "Code"
    },
    "context_depth": {
      "default": 1,
      "title": "Context Depth",
      "type": "integer"
    },
    "file_path": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "File Path"
    },
    "include_context": {
      "default": false,
      "title": "Include Context",
      "type": "boolean"
    },
    "include_cross_file_deps": {
      "default": false,
      "title": "Include Cross File Deps",
      "type": "boolean"
    },
    "include_token_estimate": {
      "default": true,
      "title": "Include Token Estimate",
      "type": "boolean"
    },
    "language": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "Language"
    },
    "target_name": {
      "title": "Target Name",
      "type": "string"
    },
    "target_type": {
      "title": "Target Type",
      "type": "string"
    }
  },
  "required": [
    "target_type",
    "target_name"
  ],
  "title": "extract_codeArguments",
  "type": "object"
}
```

## generate_unit_tests

Generate unit tests from code using symbolic execution.

    Use this tool to automatically create test cases that cover all execution paths
    in a function. Each test case includes concrete input values that trigger a
    specific path through the code.

    [20251220_FIX] v3.0.5 - Added file_path parameter for consistency with other tools.

    Example::

        result = await generate_unit_tests(
            code='''
            def calculate_discount(price, is_member):
                if price > 100:
                    if is_member:
                        return price * 0.8  # 20% off for members
                    return price * 0.9  # 10% off
                return price  # No discount
            ''',
            framework="pytest"
        )

        # Returns TestGenerationResult:
        # - pytest_code: '''
        #     import pytest
        #
        #     def test_calculate_discount_member_high_price():
        #         assert calculate_discount(150, True) == 120.0
        #
        #     def test_calculate_discount_non_member_high_price():
        #         assert calculate_discount(150, False) == 135.0
        #
        #     def test_calculate_discount_low_price():
        #         assert calculate_discount(50, True) == 50
        #   '''
        # - test_cases: [
        #     GeneratedTestCase(inputs={"price": 150, "is_member": True}, ...),
        #     GeneratedTestCase(inputs={"price": 150, "is_member": False}, ...),
        #     GeneratedTestCase(inputs={"price": 50, "is_member": True}, ...)
        # ]
        # - coverage_paths: 3

    Args:
        code: Source code containing the function to test (provide code or file_path)
        file_path: Path to file containing the function to test (provide code or file_path)
        function_name: Name of function to generate tests for (auto-detected if None)
        framework: Test framework ("pytest" or "unittest")

    Returns:
        TestGenerationResult with pytest_code/unittest_code and generated test_cases

### Input schema

```json
{
  "properties": {
    "code": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "Code"
    },
    "file_path": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "File Path"
    },
    "framework": {
      "default": "pytest",
      "title": "Framework",
      "type": "string"
    },
    "function_name": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "Function Name"
    }
  },
  "title": "generate_unit_testsArguments",
  "type": "object"
}
```

## get_call_graph

Build a call graph showing function relationships in the project.

    [v1.5.0] Use this tool to understand code flow and function dependencies.
    Analyzes Python source files to build a static call graph with:
    - Line number tracking for each function
    - Entry point detection (main, CLI commands, routes)
    - Depth-limited traversal from any starting function
    - Mermaid diagram generation for visualization
    - Circular import detection

    [v3.0.5] Now reports progress during graph construction.

    Why AI agents need this:
    - Navigation: Quickly understand how functions connect
    - Impact analysis: See what breaks if you change a function
    - Refactoring: Identify tightly coupled code
    - Documentation: Generate visual diagrams of code flow

    Args:
        project_root: Project root directory (default: server's project root)
        entry_point: Starting function name (e.g., "main" or "app.py:main")
                    If None, includes all functions
        depth: Maximum depth to traverse from entry point (default: 10)
        include_circular_import_check: Check for circular imports (default: True)

    Returns:
        CallGraphResultModel with nodes, edges, Mermaid diagram, and any circular imports

### Input schema

```json
{
  "properties": {
    "depth": {
      "default": 10,
      "title": "Depth",
      "type": "integer"
    },
    "entry_point": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "Entry Point"
    },
    "include_circular_import_check": {
      "default": true,
      "title": "Include Circular Import Check",
      "type": "boolean"
    },
    "project_root": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "Project Root"
    }
  },
  "title": "get_call_graphArguments",
  "type": "object"
}
```

## get_cross_file_dependencies

Analyze and extract cross-file dependencies for a symbol.

    [v2.5.0] Use this tool to understand all dependencies a function/class needs
    from other files in the project. It recursively resolves imports and extracts
    the complete dependency chain with source code.

    **Confidence Decay (v2.5.0):**
    Deep dependency chains get exponentially decaying confidence scores.
    Formula: C_effective = 1.0 × confidence_decay_factor^depth

    | Depth | Confidence (factor=0.9) |
    |-------|------------------------|
    | 0     | 1.000 (target)         |
    | 1     | 0.900                  |
    | 2     | 0.810                  |
    | 5     | 0.590                  |
    | 10    | 0.349                  |

    Symbols with confidence < 0.5 are flagged as "low confidence".

    Key capabilities:
    - Resolve imports to their source files
    - Extract code for all dependent symbols
    - Detect circular import cycles
    - Generate import relationship diagrams
    - Provide combined code block ready for AI analysis
    - **Confidence scoring** for each symbol based on depth

    Why AI agents need this:
    - Complete Context: Get all code needed to understand a function
    - Safe Refactoring: Know what depends on what before making changes
    - Debugging: Trace data flow across file boundaries
    - Code Review: Understand the full impact of changes
    - **Honest Uncertainty**: Know when deep dependencies may be unreliable

    Example:
        # Analyze 'process_order' function in 'services/order.py'
        result = get_cross_file_dependencies(
            target_file="services/order.py",
            target_symbol="process_order",
            max_depth=5,
            confidence_decay_factor=0.9
        )
        # Check for low-confidence symbols
        if result.low_confidence_count > 0:
            print(f"Warning: {result.low_confidence_warning}")

    Args:
        target_file: Path to file containing the target symbol (relative to project root)
        target_symbol: Name of the function or class to analyze
        project_root: Project root directory (default: server's project root)
        max_depth: Maximum depth of dependency resolution (default: 3)
        include_code: Include full source code in result (default: True)
        include_diagram: Include Mermaid diagram of imports (default: True)
        confidence_decay_factor: Decay factor per depth level (default: 0.9).
                                 Lower values = faster decay. Range: 0.0-1.0

    Returns:
        CrossFileDependenciesResult with extracted symbols, dependency graph, combined code,
        and confidence scores for each symbol

### Input schema

```json
{
  "properties": {
    "confidence_decay_factor": {
      "default": 0.9,
      "title": "Confidence Decay Factor",
      "type": "number"
    },
    "include_code": {
      "default": true,
      "title": "Include Code",
      "type": "boolean"
    },
    "include_diagram": {
      "default": true,
      "title": "Include Diagram",
      "type": "boolean"
    },
    "max_depth": {
      "default": 3,
      "title": "Max Depth",
      "type": "integer"
    },
    "project_root": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "Project Root"
    },
    "target_file": {
      "title": "Target File",
      "type": "string"
    },
    "target_symbol": {
      "title": "Target Symbol",
      "type": "string"
    }
  },
  "required": [
    "target_file",
    "target_symbol"
  ],
  "title": "get_cross_file_dependenciesArguments",
  "type": "object"
}
```

## get_file_context

Get a file overview without reading full content.

    [v1.4.0] Use this tool to quickly assess if a file is relevant to your task
    without consuming tokens on full content. Returns functions, classes, imports,
    complexity score, and security warnings.

    Why AI agents need this:
    - Quickly assess file relevance before extracting code
    - Understand file structure without token overhead
    - Make informed decisions about which functions to modify

    Example::

        result = await get_file_context("src/services/payment.py")

        # Returns FileContextResult:
        # - file_path: "src/services/payment.py"
        # - functions: ["process_payment", "validate_card", "refund_transaction"]
        # - classes: ["PaymentProcessor", "PaymentError"]
        # - imports: ["stripe", "decimal.Decimal", "datetime"]
        # - complexity_score: 8
        # - line_count: 245
        # - has_security_issues: True
        # - security_warnings: ["Potential SQL injection at line 87"]
        # - docstring: "Payment processing service for Stripe integration."

        # Use to decide if file is relevant
        if "payment" in result.functions or result.has_security_issues:
            # Now extract specific functions
            code = await extract_code(file_path, symbol_name="process_payment")

    Args:
        file_path: Path to the file (absolute or relative to project root)
                   Supports: .py, .js, .ts, .java, .go, .rs, .rb, .php

    Returns:
        FileContextResult with functions, classes, imports, complexity, and security warnings

### Input schema

```json
{
  "properties": {
    "file_path": {
      "title": "File Path",
      "type": "string"
    }
  },
  "required": [
    "file_path"
  ],
  "title": "get_file_contextArguments",
  "type": "object"
}
```

## get_graph_neighborhood

Extract k-hop neighborhood subgraph around a center node.

    [v2.5.0] Use this tool to prevent graph explosion when analyzing large
    codebases. Instead of loading the entire graph, extract only the nodes
    within k hops of a specific node.

    **Graph Pruning Formula:** N(v, k) = {u ∈ V : d(v, u) ≤ k}

    This extracts all nodes u where the shortest path from center v to u
    is at most k hops.

    **Truncation Protection:**
    If the neighborhood exceeds max_nodes, the graph is truncated and
    a warning is returned. This prevents memory exhaustion on dense graphs.

    Key capabilities:
    - Extract focused subgraph around any node
    - Control traversal depth with k parameter
    - Limit graph size with max_nodes
    - Filter by edge direction (incoming, outgoing, both)
    - Filter by minimum confidence score
    - Generate Mermaid visualization

    Why AI agents need this:
    - **Focused Analysis:** Analyze only relevant code, not entire codebase
    - **Memory Safety:** Prevent OOM on large graphs
    - **Honest Uncertainty:** Know when graph is incomplete

    Example:
        # Get 2-hop neighborhood around a function
        result = get_graph_neighborhood(
            center_node_id="python::services::function::process_order",
            k=2,
            max_nodes=50
        )
        if result.truncated:
            print(f"Warning: {result.truncation_warning}")

    Args:
        center_node_id: ID of the center node (format: language::module::type::name)
        k: Maximum hops from center (default: 2)
        max_nodes: Maximum nodes to include (default: 100)
        direction: "outgoing", "incoming", or "both" (default: "both")
        min_confidence: Minimum edge confidence to follow (default: 0.0)
        project_root: Project root directory (default: server's project root)

    Returns:
        GraphNeighborhoodResult with subgraph, truncation info, and Mermaid diagram

### Input schema

```json
{
  "properties": {
    "center_node_id": {
      "title": "Center Node Id",
      "type": "string"
    },
    "direction": {
      "default": "both",
      "title": "Direction",
      "type": "string"
    },
    "k": {
      "default": 2,
      "title": "K",
      "type": "integer"
    },
    "max_nodes": {
      "default": 100,
      "title": "Max Nodes",
      "type": "integer"
    },
    "min_confidence": {
      "default": 0.0,
      "title": "Min Confidence",
      "type": "number"
    },
    "project_root": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "Project Root"
    }
  },
  "required": [
    "center_node_id"
  ],
  "title": "get_graph_neighborhoodArguments",
  "type": "object"
}
```

## get_project_map

Generate a comprehensive map of the project structure.

    [v1.5.0] Use this tool to get a high-level overview of a codebase before diving in.
    Analyzes all Python files to provide:
    - Package and module structure
    - Function and class inventory per file
    - Entry point detection (main, CLI commands, routes)
    - Complexity hotspots (files that need attention)
    - Circular import detection
    - Mermaid diagram of project structure

    [v3.0.5] Now reports progress during analysis.

    Why AI agents need this:
    - Orientation: Understand project structure before making changes
    - Navigation: Know where to find specific functionality
    - Risk assessment: Identify complex areas that need careful handling
    - Architecture: See how packages and modules are organized

    Args:
        project_root: Project root directory (default: server's project root)
        include_complexity: Calculate cyclomatic complexity (default: True)
        complexity_threshold: Threshold for flagging hotspots (default: 10)
        include_circular_check: Check for circular imports (default: True)

    Returns:
        ProjectMapResult with comprehensive project overview

### Input schema

```json
{
  "properties": {
    "complexity_threshold": {
      "default": 10,
      "title": "Complexity Threshold",
      "type": "integer"
    },
    "include_circular_check": {
      "default": true,
      "title": "Include Circular Check",
      "type": "boolean"
    },
    "include_complexity": {
      "default": true,
      "title": "Include Complexity",
      "type": "boolean"
    },
    "project_root": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "Project Root"
    }
  },
  "title": "get_project_mapArguments",
  "type": "object"
}
```

## get_symbol_references

Find all references to a symbol across the project.

    [v1.4.0] Use this tool before modifying a function, class, or variable to
    understand its usage across the codebase. Essential for safe refactoring.

    [v3.0.5] Now reports progress as files are scanned.

    Why AI agents need this:
    - Safe refactoring: know all call sites before changing signatures
    - Impact analysis: understand blast radius of changes
    - No hallucination: real references, not guessed ones

    Example::

        result = await get_symbol_references("process_order")

        # Returns SymbolReferencesResult:
        # - symbol_name: "process_order"
        # - definition_file: "services/order.py"
        # - definition_line: 42
        # - total_references: 7
        # - references: [
        #     SymbolReference(
        #         file="handlers/api.py",
        #         line=156,
        #         column=8,
        #         context="        result = process_order(order_data)",
        #         is_definition=False
        #     ),
        #     SymbolReference(
        #         file="tests/test_order.py",
        #         line=23,
        #         column=4,
        #         context="    process_order(mock_order)",
        #         is_definition=False
        #     ),
        #     ...
        # ]

        # Before changing function signature, check all call sites
        if result.total_references > 0:
            print(f"Warning: {result.total_references} call sites to update")

    Args:
        symbol_name: Name of the function, class, or variable to search for
        project_root: Project root directory (default: server's project root)

    Returns:
        SymbolReferencesResult with definition_file, definition_line, and all references

### Input schema

```json
{
  "properties": {
    "project_root": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "Project Root"
    },
    "symbol_name": {
      "title": "Symbol Name",
      "type": "string"
    }
  },
  "required": [
    "symbol_name"
  ],
  "title": "get_symbol_referencesArguments",
  "type": "object"
}
```

## scan_dependencies

Scan project dependencies for known vulnerabilities (A06:2021 - Vulnerable Components).

    [20251219_FEATURE] v3.0.4 - A06 Vulnerable and Outdated Components
    [20251220_FIX] v3.0.5 - Added timeout parameter for OSV API calls
    [20251220_FEATURE] v3.0.5 - Progress reporting during vulnerability scan
    [20251220_FEATURE] v3.0.5 - Returns DependencyScanResult with per-dependency tracking

    This tool scans dependency files and checks them against the Google OSV
    (Open Source Vulnerabilities) database for known CVEs and security advisories.

    Supported dependency files:
    - npm: package.json
    - Maven: pom.xml, build.gradle
    - Python: requirements.txt, pyproject.toml

    Example usage:
    - Scan a single file: scan_dependencies(path="package.json")
    - Scan a project directory: scan_dependencies(project_root="/path/to/project")
    - Without vulnerability check: scan_dependencies(path="requirements.txt", scan_vulnerabilities=False)

    The scan will recursively find all dependency files in a directory,
    skipping node_modules and .venv directories.

    Args:
        path: Path to a dependency file or project directory
        project_root: Alias for 'path' (for backward compatibility)
        scan_vulnerabilities: Whether to check OSV for vulnerabilities (default True)
        include_dev: Whether to include dev dependencies (default True)
        timeout: Timeout in seconds for OSV API calls (default: 30.0)

    Returns:
        DependencyScanResult with dependency-level vulnerability tracking.

### Input schema

```json
{
  "properties": {
    "include_dev": {
      "default": true,
      "title": "Include Dev",
      "type": "boolean"
    },
    "path": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "Path"
    },
    "project_root": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "Project Root"
    },
    "scan_vulnerabilities": {
      "default": true,
      "title": "Scan Vulnerabilities",
      "type": "boolean"
    },
    "timeout": {
      "default": 30.0,
      "title": "Timeout",
      "type": "number"
    }
  },
  "title": "scan_dependenciesArguments",
  "type": "object"
}
```

## security_scan

Scan Python code for security vulnerabilities using taint analysis.

    Use this tool to audit code for security vulnerabilities before deploying
    or committing changes. It tracks data flow from sources to sinks.

    [20251214_FEATURE] v2.0.0 - Now accepts file_path parameter to scan files directly.

    Detects:
    - SQL Injection (CWE-89)
    - NoSQL Injection (CWE-943) - MongoDB
    - LDAP Injection (CWE-90)
    - Cross-Site Scripting (CWE-79)
    - Command Injection (CWE-78)
    - Path Traversal (CWE-22)
    - XXE - XML External Entity (CWE-611) [v1.4.0]
    - SSTI - Server-Side Template Injection (CWE-1336) [v1.4.0]
    - Hardcoded Secrets (CWE-798) - 30+ patterns
    - Weak Cryptography (CWE-327) - MD5, SHA-1 [v2.0.0]
    - Dangerous Patterns - shell=True, eval(), pickle [v2.0.0]

    Example::

        # Scan code directly
        result = await security_scan(code='''
        def get_user(user_id):
            query = f"SELECT * FROM users WHERE id = {user_id}"
            cursor.execute(query)  # SQL Injection!
        ''')

        # Returns SecurityResult:
        # - has_vulnerabilities: True
        # - vulnerability_count: 1
        # - risk_level: "high"
        # - vulnerabilities: [
        #     VulnerabilityInfo(
        #         type="SQL Injection",
        #         cwe="CWE-89",
        #         line=3,
        #         description="Tainted data flows to SQL execution"
        #     )
        # ]
        # - taint_flows: [{source: "user_id", sink: "execute", path: [...]}]

        # Or scan a file
        result = await security_scan(file_path="/app/handlers/user.py")

    Args:
        code: Python source code to scan (provide either code or file_path)
        file_path: Path to Python file to scan (provide either code or file_path)

    Returns:
        SecurityResult with vulnerabilities, risk_level, taint_flows, and remediation hints

### Input schema

```json
{
  "properties": {
    "code": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "Code"
    },
    "file_path": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "File Path"
    }
  },
  "title": "security_scanArguments",
  "type": "object"
}
```

## simulate_refactor

Simulate applying a code change and check for safety issues.

    Use this tool before applying AI-generated code changes to verify they don't
    introduce security vulnerabilities or break existing functionality.

    Provide either the new_code directly OR a unified diff patch.

    Example::

        # Check if a refactor introduces vulnerabilities
        result = await simulate_refactor(
            original_code='''
            def process_data(data):
                return sanitize(data)
            ''',
            new_code='''
            def process_data(data):
                return eval(data)  # Dangerous change!
            '''
        )

        # Returns RefactorSimulationResult:
        # - is_safe: False
        # - status: "unsafe"
        # - security_issues: [
        #     {"type": "Code Injection", "cwe": "CWE-94",
        #      "description": "eval() introduced in refactor"}
        # ]
        # - structural_changes: [
        #     {"type": "function_body_changed", "name": "process_data"}
        # ]

        # Safe refactor example
        safe_result = await simulate_refactor(
            original_code="def add(a, b): return a + b",
            new_code="def add(a: int, b: int) -> int: return a + b"
        )
        # is_safe: True, status: "safe"

    Args:
        original_code: The original source code
        new_code: The modified code to compare against (optional)
        patch: A unified diff patch to apply (optional)
        strict_mode: If True, treat warnings as unsafe

    Returns:
        RefactorSimulationResult with is_safe verdict, security_issues, and structural_changes

### Input schema

```json
{
  "properties": {
    "new_code": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "New Code"
    },
    "original_code": {
      "title": "Original Code",
      "type": "string"
    },
    "patch": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "Patch"
    },
    "strict_mode": {
      "default": false,
      "title": "Strict Mode",
      "type": "boolean"
    }
  },
  "required": [
    "original_code"
  ],
  "title": "simulate_refactorArguments",
  "type": "object"
}
```

## symbolic_execute

Perform symbolic execution on Python code.

    Use this tool to explore execution paths and find bugs that static analysis misses.
    It treats variables as symbolic values and uses a Z3 solver to find inputs that
    trigger specific paths.

    Example::

        result = await symbolic_execute('''
        def check_bounds(x, y):
            if x > 100:
                if y < 0:
                    return "danger"  # Path we want to find
                return "warning"
            return "safe"
        ''')

        # Returns SymbolicResult:
        # - paths_explored: 3
        # - paths: [
        #     ExecutionPath(
        #         path_id=0,
        #         conditions=["x > 100", "y < 0"],
        #         reproduction_input={"x": 101, "y": -1},  # Triggers "danger" path
        #         is_reachable=True
        #     ),
        #     ExecutionPath(
        #         path_id=1,
        #         conditions=["x > 100", "y >= 0"],
        #         reproduction_input={"x": 101, "y": 0},  # Triggers "warning" path
        #         is_reachable=True
        #     ),
        #     ExecutionPath(
        #         path_id=2,
        #         conditions=["x <= 100"],
        #         reproduction_input={"x": 50},  # Triggers "safe" path
        #         is_reachable=True
        #     )
        # ]
        # - symbolic_variables: ["x", "y"]
        # - constraints: ["x > 100", "y < 0", "y >= 0", "x <= 100"]

    Args:
        code: Python source code to analyze
        max_paths: Maximum number of paths to explore (default: 10)

    Returns:
        SymbolicResult with paths, reproduction_input for each path, and symbolic constraints

### Input schema

```json
{
  "properties": {
    "code": {
      "title": "Code",
      "type": "string"
    },
    "max_paths": {
      "default": 10,
      "title": "Max Paths",
      "type": "integer"
    }
  },
  "required": [
    "code"
  ],
  "title": "symbolic_executeArguments",
  "type": "object"
}
```

## type_evaporation_scan

Detect Type System Evaporation vulnerabilities across TypeScript frontend and Python backend.

    [20251229_FEATURE] v3.0.4 - Ninja Warrior Stage 3.1

    Type System Evaporation occurs when TypeScript compile-time types (like union types)
    are trusted but evaporate at serialization boundaries (JSON.stringify).

    This tool analyzes:
    - TypeScript frontend: unsafe type assertions, DOM input, fetch boundaries
    - Python backend: unvalidated external input in HTTP responses
    - Cross-file: correlates TS fetch() endpoints with Python @app.route() decorators

    Example vulnerability:
        Frontend: type Role = 'admin' | 'user'; const role = input.value as Role;
        Backend: role = request.get_json()['role']  # No validation!

    The TypeScript type provides NO runtime enforcement - attacker can send any value.

    Args:
        frontend_code: TypeScript/JavaScript frontend code
        backend_code: Python backend code
        frontend_file: Frontend filename for error messages
        backend_file: Backend filename for error messages

    Returns:
        TypeEvaporationResultModel with frontend, backend, and cross-file vulnerabilities.

### Input schema

```json
{
  "properties": {
    "backend_code": {
      "title": "Backend Code",
      "type": "string"
    },
    "backend_file": {
      "default": "backend.py",
      "title": "Backend File",
      "type": "string"
    },
    "frontend_code": {
      "title": "Frontend Code",
      "type": "string"
    },
    "frontend_file": {
      "default": "frontend.ts",
      "title": "Frontend File",
      "type": "string"
    }
  },
  "required": [
    "frontend_code",
    "backend_code"
  ],
  "title": "type_evaporation_scanArguments",
  "type": "object"
}
```

## unified_sink_detect

Unified polyglot sink detection with confidence thresholds.

    Sinks are dangerous functions where untrusted data should never reach directly
    (e.g., eval(), execute(), os.system()). This tool detects sinks across multiple
    languages and maps them to CWE identifiers.

    [20251216_FEATURE] v2.5.0 "Guardian" - Expose unified sink detector via MCP.
    [20251220_BUGFIX] v3.0.5 - Use DEFAULT_MIN_CONFIDENCE for consistency.

    Example::

        result = await unified_sink_detect(
            code="eval(user_input)",
            language="python"
        )

        # Returns UnifiedSinkResult:
        # - sinks: [SinkInfo(name="eval", line=1, cwe="CWE-94", confidence=0.95)]
        # - sink_count: 1
        # - coverage: {"code_injection": True, "sql_injection": False, ...}
        # - language: "python"

        # Multi-language support:
        js_result = await unified_sink_detect(
            code="document.innerHTML = userInput;",
            language="javascript"
        )
        # Detects XSS sink with CWE-79

    Args:
        code: Source code to analyze
        language: Programming language (python, java, typescript, javascript)
        min_confidence: Minimum confidence threshold (0.0-1.0, default: 0.7)

    Returns:
        UnifiedSinkResult with detected sinks, CWE mappings, and coverage summary

### Input schema

```json
{
  "properties": {
    "code": {
      "title": "Code",
      "type": "string"
    },
    "language": {
      "title": "Language",
      "type": "string"
    },
    "min_confidence": {
      "default": 0.7,
      "title": "Min Confidence",
      "type": "number"
    }
  },
  "required": [
    "code",
    "language"
  ],
  "title": "unified_sink_detectArguments",
  "type": "object"
}
```

## update_symbol

Surgically replace a function, class, or method in a file with new code.

        This is the SAFE way to modify code - you provide only the new symbol,
        and the server handles:
        - Locating the exact symbol boundaries (including decorators)
        - Validating the replacement code syntax
        - Preserving all surrounding code exactly
        - Creating a backup before modification
        - Atomic write (prevents partial writes)

        Args:
            file_path: Path to the Python source file to modify.
            target_type: Type of element - "function", "class", or "method".
            target_name: Name of the element. For methods, use "ClassName.method_name".
            new_code: The complete new definition (including def/class line and body).
            create_backup: If True (default), create a .bak file before modifying.

        Returns:
            PatchResultModel with success status, line changes, and backup path.

        Example (Fix a function):
            update_symbol(
                file_path="/project/src/utils.py",
                target_type="function",
                target_name="calculate_tax",
                new_code='''def calculate_tax(amount, rate=0.1):
        """Calculate tax with proper rounding."""
        return round(amount * rate, 2)
    '''
            )

        Example (Update a method):
            update_symbol(
                file_path="/project/src/models.py",
                target_type="method",
                target_name="User.validate_email",
                new_code='''def validate_email(self, email):
        import re
        pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        return bool(re.match(pattern, email))
    '''
            )

        Safety Features:
            - Backup created at {file_path}.bak (unless create_backup=False)
            - Syntax validation before any file modification
            - Atomic write prevents corruption on crash
            - Original indentation preserved

### Input schema

```json
{
  "properties": {
    "create_backup": {
      "default": true,
      "title": "Create Backup",
      "type": "boolean"
    },
    "file_path": {
      "title": "File Path",
      "type": "string"
    },
    "new_code": {
      "title": "New Code",
      "type": "string"
    },
    "target_name": {
      "title": "Target Name",
      "type": "string"
    },
    "target_type": {
      "title": "Target Type",
      "type": "string"
    }
  },
  "required": [
    "file_path",
    "target_type",
    "target_name",
    "new_code"
  ],
  "title": "update_symbolArguments",
  "type": "object"
}
```

## validate_paths

Validate that paths are accessible before running file-based operations.

    [v1.5.3] Use this tool to check path accessibility before attempting
    file-based operations. Essential for Docker deployments where volume
    mounts must be configured correctly.

    Key capabilities:
    - Check if files are accessible from the MCP server
    - Detect Docker environment automatically
    - Provide actionable suggestions for fixing path issues
    - Report detected workspace roots
    - Generate Docker volume mount commands

    Why AI agents need this:
    - Prevent failures: Check paths before expensive operations
    - Debug deployment: Understand why paths aren't accessible
    - Guide users: Provide specific Docker mount commands
    - Environment awareness: Know if running in Docker vs local

    Common scenarios:
    - Before extract_code: Validate file exists and is accessible
    - Before crawl_project: Check project root is mounted
    - Troubleshooting: Help users configure Docker volumes

    Example::

        result = await validate_paths([
            "/home/user/project/main.py",
            "/nonexistent/file.py",
            "utils/helpers.py"
        ])

        # Returns PathValidationResult:
        # - success: False (not all paths accessible)
        # - accessible: ["/home/user/project/main.py", "utils/helpers.py"]
        # - inaccessible: ["/nonexistent/file.py"]
        # - suggestions: [
        #     "File not found: /nonexistent/file.py",
        #     "Check if the path exists and is spelled correctly"
        # ]
        # - workspace_roots: ["/home/user/project"]
        # - is_docker: False

        # In Docker environment:
        docker_result = await validate_paths(["/app/src/main.py"])
        # - is_docker: True
        # - suggestions: [
        #     "Running in Docker: Mount your project with -v /path/to/project:/app",
        #     "Example: docker run -v $(pwd):/app code-scalpel:latest"
        # ]

    Args:
        paths: List of file paths to validate
        project_root: Optional explicit project root directory

    Returns:
        PathValidationResult with accessible, inaccessible, suggestions, workspace_roots, is_docker

### Input schema

```json
{
  "properties": {
    "paths": {
      "items": {
        "type": "string"
      },
      "title": "Paths",
      "type": "array"
    },
    "project_root": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "Project Root"
    }
  },
  "required": [
    "paths"
  ],
  "title": "validate_pathsArguments",
  "type": "object"
}
```

## verify_policy_integrity

Verify policy file integrity using cryptographic signatures.

    [v2.5.0] Use this tool to verify that policy files have not been tampered
    with since they were signed. This is essential for tamper-resistant
    governance in enterprise deployments.

    **Security Model: FAIL CLOSED**
    - Missing manifest → DENY ALL
    - Invalid signature → DENY ALL
    - Hash mismatch → DENY ALL

    **How it works:**
    1. Load policy manifest from configured source (git, env, file)
    2. Verify HMAC-SHA256 signature using secret key
    3. Verify SHA-256 hash of each policy file matches manifest
    4. Any failure results in security error

    **Bypass Prevention:**
    This addresses the 3rd party review feedback that file permissions
    (chmod 0444) can be bypassed. Even if an agent runs `chmod +w` and
    modifies a policy file, the hash verification will detect the change.

    Key capabilities:
    - Verify manifest signature integrity
    - Detect tampered policy files
    - Detect missing policy files
    - Report detailed verification status
    - Fail closed on any error

    Why AI agents need this:
    - **Trust Verification:** Confirm policies haven't been modified
    - **Audit Trail:** Verify policy integrity before operations
    - **Security Compliance:** Meet enterprise security requirements

    Example:
        # Verify policy integrity before operations
        result = verify_policy_integrity(policy_dir=".code-scalpel")

        if not result.success:
            print(f"SECURITY: {result.error}")
            # Fail closed - do not proceed
        else:
            print(f"Verified {result.files_verified} policy files")

    Args:
        policy_dir: Directory containing policy files (default: .code-scalpel)
        manifest_source: Where to load manifest from - "git", "env", or "file"
            - "git": Load from committed version in git history (most secure)
            - "env": Load from SCALPEL_POLICY_MANIFEST environment variable
            - "file": Load from local policy.manifest.json file

    Returns:
        PolicyVerificationResult with verification status and details

    Note:
        Requires SCALPEL_MANIFEST_SECRET environment variable to be set.
        This secret should be managed by administrators, not agents.

### Input schema

```json
{
  "properties": {
    "manifest_source": {
      "default": "file",
      "title": "Manifest Source",
      "type": "string"
    },
    "policy_dir": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "Policy Dir"
    }
  },
  "title": "verify_policy_integrityArguments",
  "type": "object"
}
```
