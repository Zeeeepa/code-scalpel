[20251214_DOCS] ImportResolver parallel parse benchmark (local dev box, Windows, Python 3.13):
	command: python -c "import time; from pathlib import Path; from code_scalpel.ast_tools.import_resolver import ImportResolver; root=Path('src'); t=time.perf_counter(); r=ImportResolver(root).build(); elapsed=time.perf_counter()-t; print(f'build_success={r.success} modules={r.modules} imports={r.imports} elapsed_s={elapsed:.3f}'); print(f'warnings={len(r.warnings)} errors={len(r.errors)}')"
	result: build_success=True modules=124 imports=1043 elapsed_s=2.457 warnings=0 errors=0
	notes: Measured after wiring ParallelParser + AnalysisCache. Larger 1000+ file fixture pending; capture cache hit rates in cache_effectiveness_evidence.json.

[20251214_DOCS] Synthetic 1200-file fixture (Windows, Python 3.13):
	command: python -c "import tempfile,time,os;from pathlib import Path;from code_scalpel.ast_tools.import_resolver import ImportResolver;root=Path(tempfile.mkdtemp(prefix='scalpel_fixture_'));sub=root/'pkg';sub.mkdir(parents=True,exist_ok=True);[(sub/f'mod_{i}.py').write_text('\nVALUE={i}\n',encoding='utf-8') for i in range(1200)];(sub/'__init__.py').write_text('from .mod_0 import VALUE\n',encoding='utf-8');resolver=ImportResolver(root);import_res=time.perf_counter();r1=resolver.build();t1=time.perf_counter()-import_res;import_res=time.perf_counter();r2=resolver.build();t2=time.perf_counter()-import_res;print(f'fixture_root={root}');print(f'run1_success={r1.success} modules={r1.modules} imports={r1.imports} elapsed_s={t1:.3f}');print(f'run2_success={r2.success} modules={r2.modules} imports={r2.imports} elapsed_s={t2:.3f}');print(f'warnings_run1={len(r1.warnings)} errors_run1={len(r1.errors)}');print(f'warnings_run2={len(r2.warnings)} errors_run2={len(r2.errors)}')"
	result: run1_success=True modules=1201 imports=1 elapsed_s=10.419; run2_success=True modules=1201 imports=1 elapsed_s=1.142; warnings/errors: 0/0 both runs
	notes: Same resolver reused to demonstrate in-memory cache hits. Imports count low because synthetic files have minimal content; real fixtures should include import edges. Target <10s met on run1 (just above) and <2s on warm run.
[20251214_DOCS] Realistic synthetic 1200-file fixture with random import graph (tests/fixtures/data/synthetic):
  command: python tests/fixtures/generate_fixture.py --synthetic 1200
           python -c "import time; from pathlib import Path; from code_scalpel.ast_tools.import_resolver import ImportResolver; root=Path('tests/fixtures/data/synthetic'); t1=time.perf_counter(); r1=ImportResolver(root).build(); e1=time.perf_counter()-t1; print(f'run1: modules={r1.modules} imports={r1.imports} elapsed_s={e1:.3f} warnings={len(r1.warnings)} errors={len(r1.errors)}')"
  result: modules=1201 imports=4190 elapsed_s=12.093 warnings=0 errors=0
  notes: Each file imports 2-5 random siblings; 4190 total import edges. Cold run 12s slightly above 10s target; optimization opportunities remain (worker count tuning, reduce pickle overhead).

[20251214_PERF] Batched parallel parsing optimization (batch_size=100):
  before: 12.093s cold (per-file task submission)
  after:  3.077s cold (batched, 100 files per worker)
  improvement: 74.6% reduction in cold parse time
  notes: Batching amortizes ProcessPoolExecutor pickle overhead. Target <10s now easily met.

[20251214_DOCS] Disk cache impact analysis:
  cold (no disk cache): 3.077s
  warm (disk cache primed, new process): 4.762s
  notes: Warm run is SLOWER because get_cached() checks each file's disk cache serially before parallelizing. For bulk cold starts, disk cache adds overhead. Disk cache is optimized for incremental single-file updates, not bulk re-analysis. Consider lazy cache population or parallel cache reads for future optimization.